Preface
This book is intended to serve as a textbook for advanced juniors and seniors
and first-year graduate students in computer science and engineering . The
reader is not expected to have taken a course in artificial intelligence ( AI )  , 
although the book includes pointers to additional readings and advanced
exercises for more advanced students . The reader should have had at least
one course in object-oriented programming in order to follow the discussions
on how to implement and program robots using the structures described in
this book . These programming structures lend themselves well to laboratory
exercises on commercially available robots , such as the Khepera , Nomad 200
series , and Pioneers . Lego Mindstorms and Rug Warrior robots can be used
for the first six chapters , but their current programming interface and sensor
limitations interfere with using those robots for the more advanced material . 
A background in digital circuitry is not required , although many instructors
may want to introduce laboratory exercises for building reactive robots from
kits such as the Rug Warrior or the Handy Board . 
Introduction to AI Robotics attempts to cover all the topics needed to program an artificially intelligent robot for applications involving sensing , navigation , path planning , and navigating with uncertainty . Although machine
perception is a separate field of endeavor , the book covers enough computer
vision and sensing to enable students to embark on a serious robot project
or competition . The book is divided into two parts . Part I defines what are
intelligent robots and introduces why artificial intelligence is needed . It covers the ‚Äú theory ‚Äù of AI robotics , taking the reader through a historical journey
from the Hierarchical to the Hybrid Deliberative / Reactive Paradigm for organizing intelligence . The bulk of the seven chapters is concerned with the
Reactive Paradigm and behaviors . A chapter on sensing and programming
techniques for reactive behaviors is included in order to permit a class to get
xviii Preface
a head start on a programming project . Also , Part I covers the coordination
and control of teams of multi-agents . Since the fundamental mission of a
mobile robot involves moving about in the world , Part II devotes three chapters to qualitative and metric navigation and path planning techniques , plus
work in uncertainty management . The book concludes with an overview of
how advances in computer vision are now being integrated into robots , and
how successes in robots are driving the web-bot and know-bot craze . 
Since Introduction to AI Robotics is an introductory text , it is impossible to
cover all the fine work that has been in the field . The guiding principle has
been to include only material that clearly illuminates a specific topic . References to other approaches and systems are usually included as an advanced
reading question at the end of the chapter or as an end note . Behavior-based
Robotics 10 provides a thorough survey of the field and should be an instructor ‚Äô s companion . 
Acknowledgments
It would be impossible to thank all of the people involved in making this
book possible , but I would like to try to list the ones who made the most
obvious contributions . I ‚Äô d like to thank my parents ( I think this is the equivalent of scoring a goal and saying ‚Äú Hi Mom !  ‚Äù on national TV ) and my family 
( Kevin , Kate , and Allan )  . I had the honor of being in the first AI robotics
course taught by my PhD advisor Ron Arkin at Georgia Tech ( where I was
also his first PhD student )  , and much of the material and organization of this
book can be traced back to his course . I have tried to maintain the intellectual rigor of his course and excellent book while trying to distill the material
for a novice audience . Any errors in this book are strictly mine . David Kortenkamp suggested that I write this book after using my course notes for a
class he taught , which served as a very real catalyst . Certainly the students
at both the Colorado School of Mines ( CSM )  , where I first developed my
robotics courses , and at the University of South Florida ( USF ) merit special
thanks for being guinea pigs . I would like to specifically thank Leslie Baski , 
John Blitch , Glenn Blauvelt , Ann Brigante , Greg Chavez , Aaron Gage , Dale
Hawkins , Floyd Henning , Jim Hoffman , Dave Hershberger , Kevin Gifford , 
Matt Long , Charlie Ozinga , Tonya Reed Frazier , Michael Rosenblatt , Jake
Sprouse , Brent Taylor , and Paul Wiebe from my CSM days and Jenn Casper , 
Aaron Gage , Jeff Hyams , Liam Irish , Mark Micire , Brian Minten , and Mark
Powell from USF . 
Preface xix
Special thanks go to the numerous reviewers , especially Karen Sutherland
and Ken Hughes . Karen Sutherland and her robotics class at the University
of Wisconsin-LaCrosse ( Kristoff Hans Ausderau , Teddy Bauer , Scott David
Becker , Corrie L . Brague , Shane Brownell , Edwin J . Colby III , Mark Erickson , Chris Falch , Jim Fick , Jennifer Fleischman , Scott Galbari , Mike Halda , 
Brian Kehoe , Jay D . Paska , Stephen Pauls , Scott Sandau , Amy Stanislowski , 
Jaromy Ward , Steve Westcott , Peter White , Louis Woyak , and Julie A . Zander ) painstakingly reviewed an early draft of the book and made extensive
suggestions and added review questions . Ken Hughes also deserves special
thanks ; he also provided a chapter by chapter critique as well as witty emails . 
Ken always comes to my rescue . 
Likewise , the book would not be possible without my ongoing involvement in robotics research ; my efforts have been supported by NSF , DARPA , 
and ONR . Most of the case studies came from work or through equipment
sponsored by NSF . Howard Moraff , Rita Rodriguez , and Harry Hedges were
always very encouraging , beyond the call of duty of even the most dedicated NSF program director . Michael Mason also provided encouragement , 
in many forms , to hang in there and focus on education . 
My editor , Bob Prior , and the others at the MIT Press ( Katherine Innis , 
Judy Feldmann , Margie Hardwick , and Maureen Kuper ) also have my deepest appreciation for providing unfailingly good-humored guidance , technical assistance , and general savvy . Katherine and especially Judy were very
patient and nice‚Äî despite knowing that I was calling with Yet Another Crisis . Mike Hamilton at AAAI was very helpful in making available the various ‚Äú action shots ‚Äù used throughout the book . Chris Manning provided the
LATEX 2 " style files , with adaptations by Paul Anagnostopoulos . Liam Irish
and Ken Hughes contributed helpful scripts . 
Besides the usual suspects , there are some very special people who indirectly helped me . Without the encouragement of three liberal arts professors , 
Carlyle Ramsey , Monroe Swilley , and Chris Trowell , at South Georgia College in my small hometown of Douglas , Georgia , I probably wouldn ‚Äô t have
seriously considered graduate school in engineering and computer science . 
They taught me that learning isn ‚Äô t a place like a big university but rather a
personal discipline . The efforts of my husband , Kevin Murphy , were , as always , essential . He worked hard to make sure I could spend the time on this
book without missing time with the kids or going crazy . He also did a serious amount of editing , typing , scanning , and proofreading . I dedicate the
book to these four men who have influenced my professional career as much
as any academic mentor . 

P ART I
Robotic Paradigms
2 Part I
Contents :  Overview  Chapter 1 : From Teleoperation to Autonomy  Chapter 2 : The Hierarchical Paradigm  Chapter 3 : Biological Foundations of the Reactive Paradigm  Chapter 4 : The Reactive Paradigm  Chapter 5 : Designing a Reactive Implementation  Chapter 6 : Common Sensing Technique for Reactive Robots  Chapter 7 : The Hybrid Deliberative / Reactive Paradigm  Chapter 8 : Multiple Mobile Robots
Overview
The eight chapters in this part are devoted to describing what is AI robotics
and the three major paradigms for achieving it . These paradigms characterize the ways in which intelligence is organized in robots . This part of the
book also covers architectures that provide exemplars of how to transfer the
principles of the paradigm into a coherent , reusable implementation on a
single robot or teams of robots . 
What Are Robots ? 
One of the first questions most people have about robotics is ‚Äú what is a robot ?  ‚Äù followed immediately by ‚Äú what can they do ?  ‚Äù 
In popular culture , the term ‚Äú robot ‚Äù generally connotes some anthropomorphic ( human-like ) appearance ; consider robot ‚Äú arms ‚Äù for welding . The
tendency to think about robots as having a human-like appearance may stem
from the origins of the term ‚Äú robot .  ‚Äù The word ‚Äú robot ‚Äù came into the popular consciousness on January 25 , 1921 , in Prague with the first performance
of Karel Capek ‚Äô s play , R . U . R .  ( Rossum ‚Äô s Universal Robots )  . 37 In R . U . R .  , an
unseen inventor , Rossum , has created a race of workers made from a vat of
biological parts , smart enough to replace a human in any job ( hence ‚Äú universal ‚Äù  )  . Capek described the workers as robots , a term derived from the Czech
Part I 3
word ‚Äú robota ‚Äù which is loosely translated as menial laborer . Robot workers
implied that the artificial creatures were strictly meant to be servants to free 
 ‚Äú real ‚Äù people from any type of labor , but were too lowly to merit respect . 
This attitude towards robots has disastrous consequences , and the moral of
the rather socialist story is that work defines a person . 
The shift from robots as human-like servants constructed from biological
parts to human-like servants made up of mechanical parts was probably due
to science fiction . Three classic films , Metropolis ( 1926 )  , The Day the Earth
Stood Still ( 1951 )  , and Forbidden Planet ( 1956 )  , cemented the connotation that
robots were mechanical in origin , ignoring the biological origins in Capek ‚Äô s
play . Meanwhile , computers were becoming commonplace in industry and
accounting , gaining a perception of being literal minded . Industrial automation confirmed this suspicion as robot arms were installed which would go
through the motions of assembling parts , even if there were no parts . Eventually , the term robot took on nuances of factory automation : mindlessness
and good only for well-defined repetitious types of work . The notion of
anthropomorphic , mechanical , and literal-minded robots complemented the
viewpoint taken in many of the short stories in Isaac Asimov ‚Äô s perennial favorite collection , I , Robot . 
15 Many ( but not all ) of these stories involve either
a ‚Äú robopsychologist ,  ‚Äù Dr . Susan Calvin , or two erstwhile trouble shooters , 
Powell and Donovan , diagnosing robots who behaved logically but did the
wrong thing . 
The shift from human-like mechanical creatures to whatever shape gets
the job done is due to reality . While robots are mechanical , they don ‚Äô t have to
be anthropomorphic or even animal-like . Consider robot vacuum cleaners ; 
they look like vacuum cleaners , not janitors . And the HelpMate Robotics , 
Inc .  , robot which delivers hospital meals to patients to permit nurses more
time with patients , looks like a cart , not a nurse . 
It should be clear from Fig . I . 1 that appearance does not form a useful definition of a robot . Therefore , the definition that will be used in this book
INTELLIGENT ROBOT is an intelligent robot is a mechanical creature which can function autonomously .  
 ‚Äú Intelligent ‚Äù implies that the robot does not do things in a mindless , repetitive way ; it is the opposite of the connotation from factory automation . The 
 ‚Äú mechanical creature ‚Äù portion of the definition is an acknowledgment of the
fact that our scientific technology uses mechanical building blocks , not biological components ( although with recent advances in cloning , this may
change )  . It also emphasizes that a robot is not the same as a computer . A robot may use a computer as a building block , equivalent to a nervous system
or brain , but the robot is able to interact with its world : move around , change
4 Part I
a . b . 
Figure I . 1 Two views of robots : a ) the humanoid robot from the 1926 movie
Metropolis ( image courtesty Fr . Doug Quinn and the Metropolis Home
Page )  , and b ) a HMMWV military vehicle capable of driving on roads and
open terrains .  ( Photograph courtesy of the National Institute for Standards
and Technology .  ) 
it , etc . A computer doesn ‚Äô t move around under its own power .  ‚Äú Function
autonomously ‚Äù indicates that the robot can operate , self-contained , under
all reasonable conditions without requiring recourse to a human operator . 
Autonomy means that a robot can adapt to changes in its environment ( the
lights get turned off ) or itself ( a part breaks ) and continue to reach its goal . 
Perhaps the best example of an intelligent mechanical creature which can
function autonomously is the Terminator from the 1984 movie of the same
name . Even after losing one camera ( eye ) and having all external coverings ( skin , flesh ) burned off , it continued to pursue its target ( Sarah Connor )  . 
Extreme adaptability and autonomy in an extremely scary robot ! A more
practical ( and real ) example is Marvin , the mail cart robot , for the Baltimore
FBI office , described in a Nov . 9 , 1996 , article in the Denver Post . Marvin is
able to accomplish its goal of stopping and delivering mail while adapting
to people getting in its way at unpredictable times and locations . 
Part I 5
What are Robotic Paradigms ? 
PARADIGM A paradigm is a philosophy or set of assumptions and / or techniques which characterize an approach to a class of problems . It is both a way of looking at the world
and an implied set of tools for solving problems . No one paradigm is right ; 
rather , some problems seem better suited for different approaches . For example , consider calculus problems . There are problems that could be solved
by differentiating in cartesian ( X ; Y ; Z ) coordinates , but are much easier to
solve if polar coordinates ( r ;  ) are used . In the domain of calculus problems , 
Cartesian and polar coordinates represent two different paradigms for viewing and manipulating a problem . Both produce the correct answer , but one
takes less work for certain problems . 
Applying the right paradigm makes problem solving easier . Therefore , 
knowing the paradigms of AI robotics is one key to being able to successfully
program a robot for a particular application . It is also interesting from a historical perspective to work through the different paradigms , and to examine
the issues that spawned the shift from one paradigm to another . 
ROBOTIC PARADIGMS There are currently three paradigms for organizing intelligence in robots : 
hierarchical , reactive , and hybrid deliberative / reactive . The paradigms are
described in two ways . 
1 . By the relationship between the three commonly accepted primitives
ROBOT PARADIGM of robotics : SENSE , PLAN , ACT . The functions of a robot can be divided
PRIMITIVES into three very general categories . If a function is taking in information
from the robot ‚Äô s sensors and producing an output useful by other functions , then that function falls in the SENSE category . If the function is
taking in information ( either from sensors or its own knowledge about
how the world works ) and producing one or more tasks for the robot to
perform ( go down the hall , turn left , proceed 3 meters and stop )  , that function is in the PLAN category . Functions which produce output commands
to motor actuators fall into ACT ( turn 98 
, clockwise , with a turning velocity of 0 . 2mps )  . Fig . I . 2 attempts to define these three primitives in terms
of inputs and outputs ; this figure will appear throughout the chapters in
Part I . 
2 . By the way sensory data is processed and distributed through the system . How much a person or robot or animal is influenced by what it
senses . So it is often difficult to adequately describe a paradigm with just
a box labeled SENSE . In some paradigms , sensor information is restricted
to being used in a specific , or dedicated , way for each function of a robot ; 
6 Part I
ROBOT PRIMITIVES INPUT OUTPUT
SENSE
PLAN
ACT
Sensor data Sensed information
Information ( sensed
and / or cognitive ) 
Sensed information
or directives
Directives
Actuator commands
Figure I . 2 Robot primitives defined in terms of inputs and outputs . 
SENSING in that case processing is local to each function . Other paradigms expect
ORGANIZATION IN
ROBOT PARADIGMS
all sensor information to be first processed into one global world model
and then subsets of the model distributed to other functions as needed . 
Overview of the Three Paradigms
In order to set the stage for learning details , it may be helpful to begin with
a general overview of the robot paradigms . Fig . I . 3 shows the differences
between the three paradigms in terms of the SENSE , PLAN , ACT primitives . 
HIERARCHICAL The Hierarchical Paradigm is the oldest paradigm , and was prevalent from
PARADIGM 1967‚Äì1990 . Under it , the robot operates in a top-down fashion , heavy on
planning ( see Fig . I . 3 )  . This was based on an introspective view of how people think .  ‚Äú I see a door , I decide to head toward it , and I plot a course around
the chairs .  ‚Äù  ( Unfortunately , as many cognitive psychologists now know , introspection is not always a good way of getting an accurate assessment of
a thought process . We now suspect no one actually plans how they get out
of a room ; they have default schemas or behaviors .  ) Under the Hierarchical
Paradigm , the robot senses the world , plans the next action , and then acts 
( SENSE , PLAN , ACT )  . Then it senses the world , plans , acts . At each step , 
the robot explicitly plans the next move . The other distinguishing feature of
the Hierarchical paradigm is that all the sensing data tends to be gathered
into one global world model , a single representation that the planner can use
and can be routed to the actions . Constructing generic global world models
Part I 7
SENSE PLAN ACT
a . 
PLAN
SENSE ACT
b . 
ACT
PLAN
SENSE
c . 
Figure I . 3 Three paradigms : a .  ) Hierarchical , b .  ) Reactive , and c .  ) Hybrid
deliberative / reactive . 
turns out to be very hard and brittle due to the frame problem and the need
for a closed world assumption . 
Fig . I . 4 shows how the Hierarchical Paradigm can be thought of as a transitive , or Z-like , flow of events through the primitives given in Fig . I . 4 . Unfortunately , the flow of events ignored biological evidence that sensed information can be directly coupled to an action , which is why the sensed information input is blacked out . 
8 Part I
ROBOT PRIMITIVES INPUT OUTPUT
SENSE
PLAN
ACT
Sensor data Sensed information
Information ( sensed
and / or cognitive ) 
Sensed information
or directives
Directives
Actuator commands
Figure I . 4 Another view of the Hierarchical Paradigm . 
REACTIVE PARADIGM The Reactive Paradigm was a reaction to the Hierarchical Paradigm , and
led to exciting advances in robotics . It was heavily used in robotics starting
in 1988 and continuing through 1992 . It is still used , but since 1992 there
has been a tendency toward hybrid architectures . The Reactive Paradigm
was made possible by two trends . One was a popular movement among AI
researchers to investigate biology and cognitive psychology in order to examine living exemplars of intelligence . Another was the rapidly decreasing
cost of computer hardware coupled with the increase in computing power . 
As a result , researchers could emulate frog and insect behavior with robots
costing less than $500 versus the $100 , 000s Shakey , the first mobile robot , 
cost . 
The Reactive Paradigm threw out planning all together ( see Figs . I . 3b and
I . 5 )  . It is a SENSE-ACT ( S-A ) type of organization . Whereas the Hierarchical
Paradigm assumes that the input to a ACT will always be the result of a
PLAN , the Reactive Paradigm assumes that the input to an ACT will always
be the direct output of a sensor , SENSE . 
If the sensor is directly connected to the action , why isn ‚Äô t a robot running
under the Reactive Paradigm limited to doing just one thing ? The robot has
multiple instances of SENSE-ACT couplings , discussed in Ch . 4 . These couplings are concurrent processes , called behaviors , which take local sensing
data and compute the best action to take independently of what the other
processes are doing . One behavior can direct the robot to ‚Äú move forward 5
meters ‚Äù  ( ACT on drive motors ) to reach a goal ( SENSE the goal )  , while another behavior can say ‚Äú turn 90 
 ‚Äù  ( ACT on steer motors ) to avoid a collision
Part I 9
ROBOT PRIMITIVES INPUT OUTPUT
SENSE
PLAN
ACT
Sensor data Sensed information
Information ( sensed
and / or cognitive ) 
Sensed information
or directives
Directives
Actuator commands
Figure I . 5 The reactive paradigm . 
with an object dead ahead ( SENSE obstacles )  . The robot will do a combination of both behaviors , swerving off course temporarily at a 45
angle to
avoid the collision . Note that neither behavior directed the robot to ACT with
a 45
turn ; the final ACT emerged from the combination of the two behaviors . 
While the Reactive Paradigm produced exciting results and clever robot
insect demonstrations , it quickly became clear that throwing away planning
was too extreme for general purpose robots . In some regards , the Reactive Paradigm reflected the work of Harvard psychologist B . F . Skinner in
stimulus-response training with animals . It explained how some animals
accomplished tasks , but was a dead end in explaining the entire range of
human intelligence . 
But the Reactive Paradigm has many desirable properties , especially the
fast execution time that came from eliminating any planning . As a result , 
HYBRID DELIBERA- the Reactive Paradigm serves as the basis for the Hybrid Deliberative / Reactive
TIVE / REACTIVE
PARADIGM
Paradigm , shown in Fig . I . 3c . The Hybrid Paradigm emerged in the 1990 ‚Äô s and
continues to be the current area of research . Under the Hybrid Paradigm , the
robot first plans ( deliberates ) how to best decompose a task into subtasks 
( also called ‚Äú mission planning ‚Äù  ) and then what are the suitable behaviors to
accomplish each subtask , etc . Then the behaviors start executing as per the
Reactive Paradigm . This type of organization is PLAN , SENSE-ACT ( P , S-A )  , 
where the comma indicates that planning is done at one step , then sensing
and acting are done together . Sensing organization in the Hybrid Paradigm
is also a mixture of Hierarchical and Reactive styles . Sensor data gets routed
to each behavior that needs that sensor , but is also available to the planner
10 Part I
ROBOT PRIMITIVES INPUT OUTPUT
PLAN
SENSE-ACT 
( behaviors ) 
Sensor data
Information ( sensed
and / or cognitive ) Directives
Actuator commands
Figure I . 6 The hybrid deliberative / reactive paradigm . 
for construction of a task-oriented global world model . The planner may
also ‚Äú eavesdrop ‚Äù on the sensing done by each behavior ( i . e .  , the behavior
identifies obstacles that could then be put into a map of the world by the
planner )  . Each function performs computations at its own rate ; deliberative
planning , which is generally computationally expensive may update every
5 seconds , while the reactive behaviors often execute at 1 / 60 second . Many
robots run at 80 centimeters per second . 
Architectures
Determining that a particular paradigm is well suited for an application is
certainly the first step in constructing the AI component of a robot . But that
step is quickly followed with the need to use the tools associated with that
paradigm . In order to visualize how to apply these paradigms to real-world
applications , it is helpful to examine representative architectures . These architectures provide templates for an implementation , as well as examples of
what each paradigm really means . 
What is an architecture ? Arkin offers several definitions in his book , Behavior-Based Robots . 
10 Two of the definitions he cites from other researchers
capture how the term will be used in this book . Following Mataric , 89 an
architecture provides a principled way of organizing a control system . However , in addition to providing structure , it imposes constraints on the way the
control problem can be solved . Following Dean and Wellman , 43 an architecARCHITECTURE ture describes a set of architectural components and how they interact . This
book is interested in the components common in robot architectures ; these
are the basic building blocks for programming a robot . It also is interested in
the principles and rules of thumb for connecting these components together . 
Part I 11
To see the importance of an architecture , consider building a house or a
car . There is no ‚Äú right ‚Äù design for a house , although most houses share the
same components ( kitchens , bathrooms , walls , floors , doors , etc .  )  . Likewise
with designing robots , there can be multiple ways of organizing the components , even if all the designs follow the same paradigm . This is similar to cars
designed by different manufacturers . All internal combustion engine types
of cars have the same basic components , but the cars look different ( BMWs
and Jaguars look quite different than Hondas and Fords )  . The internal combustion ( IC ) engine car is a paradigm ( as contrasted to the paradigm of an
electric car )  . Within the IC engine car community , the car manufacturers each
have their own architecture . The car manufacturers may make slight modifications to the architecture for sedans , convertibles , sport-utility vehicles , 
etc .  , to throw out unnecessary options , but each style of car is a particular
instance of the architecture . The point is : by studying representative robot
architectures and the instances where they were used for a robot application , 
we can learn the different ways that the components and tools associated
with a paradigm can be used to build an artificially intelligent robot . 
Since a major objective in robotics is to learn how to build them , an important skill to develop is evaluating whether or not a previously developed
architecture ( or large chunks of it ) will suit the current application . This skill
will save both time spent on re-inventing the wheel and avoid subtle problems that other people have encountered and solved . Evaluation requires a
set of criteria . The set that will be used in this book is adapted from BehaviorBased Robotics : 
10
MODULARITY 1 . Support for modularity : does it show good software engineering principles ? 
NICHE TARGETABILITY 2 . Niche targetability : how well does it work for the intended application ? 
PORTABILITY 3 . Ease of portability to other domains : how well would it work for other
applications or other robots ? 
ROBUSTNESS 4 . Robustness : where is the system vulnerable , and how does it try to reduce that vulnerability ? 
Note that niche targetability and ease of portability are often at odds with
each other . Most of the architectures described in this book were intended to
be generic , therefore emphasizing portability . The generic structures , however , often introduce undesirable computational and storage overhead , so in
practice the designer must make trade-offs . 
12 Part I
Layout of the Section
This section is divided into eight chapters , one to define robotics and the
other seven to intertwine both the theory and practice associated with each
paradigm . Ch . 2 describes the Hierarchical Paradigm and two representative
architectures . Ch . 3 sets the stage for understanding the Reactive Paradigm
by reviewing the key concepts from biology and ethology that served to motivate the shift from Hierarchical to Reactive systems . Ch . 4 describes the
Reactive Paradigm and the architectures that originally popularized this approach . It also offers definitions of primitive robot behaviors . Ch . 5 provides
guidelines and case studies on designing robot behaviors . It also introduces
issues in coordinating and controlling multiple behaviors and the common
techniques for resolving these issues . At this point , the reader should be
almost able to design and implement a reactive robot system , either in simulation or on a real robot . However , the success of a reactive system depends
on the sensing . Ch . 6 discusses simple sonar and computer vision processing
techniques that are commonly used in inexpensive robots . Ch . 7 describes
the Hybrid Deliberative-Reactive Paradigm , concentrating on architectural
trends . Up until this point , the emphasis is towards programming a single
robot . Ch . 8 concludes the section by discussing how the principles of the
three paradigms have been transferred to teams of robots . 
End Note
Robot paradigm primitives . 
While the SENSE , PLAN , ACT primitives are generally accepted , some researchers
are suggesting that a fourth primitive be added , LEARN . There are no formal architectures at this time which include this , so a true paradigm shift has not yet occurred . 
1 From Teleoperation To Autonomy
Chapter Objectives :  Define intelligent robot .  Be able to describe at least two differences between AI and Engineering
approaches to robotics .  Be able to describe the difference between telepresence and semi-autonomous
control .  Have some feel for the history and societal impact of robotics . 
1 . 1 Overview
This book concentrates on the role of artificial intelligence for robots . At
first , that may appear redundant ; aren ‚Äô t robots intelligent ? The short answer is ‚Äú no ,  ‚Äù most robots currently in industry are not intelligent by any
definition . This chapter attempts to distinguish an intelligent robot from a
non-intelligent robot . 
The chapter begins with an overview of artificial intelligence and the social
implications of robotics . This is followed with a brief historical perspective
on the evolution of robots towards intelligence , as shown in Fig . 1 . 1 . One
way of viewing robots is that early on in the 1960 ‚Äô s there was a fork in the
evolutionary path . Robots for manufacturing took a fork that has focused on
engineering robot arms for manufacturing applications . The key to success in
industry was precision and repeatability on the assembly line for mass production , in effect , industrial engineers wanted to automate the workplace . 
Once a robot arm was programmed , it should be able to operate for weeks
and months with only minor maintenance . As a result , the emphasis was
14 1 From Teleoperation To Autonomy
telemanipulators
planetary rovers
vision
manufacturing
1960 1970 1980 1990 2000
Industrial
Manipulators
AI Robotics
telesystems
Figure 1 . 1 A timeline showing forks in development of robots . 
placed on the mechanical aspects of the robot to ensure precision and repeatability and methods to make sure the robot could move precisely and
repeatable , quickly enough to make a profit . Because assembly lines were
engineered to mass produce a certain product , the robot didn ‚Äô t have to be
able to notice any problems . The standards for mass production would make
it more economical to devise mechanisms that would ensure parts would be
in the correct place . A robot for automation could essentially be blind and
senseless . 
Robotics for the space program took a different fork , concentrating instead
on highly specialized , one-of-a-kind planetary rovers . Unlike a highly automated manufacturing plant , a planetary rover operating on the dark side of
the moon ( no radio communication ) might run into unexpected situations . 
Consider that on Apollo 17 , astronaut and geologist Harrison Schmitt found
an orange rock on the moon ; an orange rock was totally unexpected . Ideally , 
a robot would be able to notice something unusual , stop what it was doing 
( as long as it didn ‚Äô t endanger itself ) and investigate . Since it couldn ‚Äô t be preprogrammed to handle all possible contingencies , it had to be able to notice
its environment and handle any problems that might occur . At a minimum , 
a planetary rover had to have some source of sensory inputs , some way of
interpreting those inputs , and a way of modifying its actions to respond to
a changing world . And the need to sense and adapt to a partially unknown
environment is the need for intelligence . 
The fork toward AI robots has not reached a termination point of truly autonomous , intelligent robots . In fact , as will be seen in Ch . 2 and 4 , it wasn ‚Äô t
until the late 1980 ‚Äô s that any visible progress toward that end was made . So
what happened when someone had an application for a robot which needed
1 . 2 How Can a Machine Be Intelligent ? 15
real-time adaptability before 1990 ? In general , the lack of machine intelligence was compensated by the development of mechanisms which allow a
human to control all , or parts , of the robot remotely . These mechanisms are
generally referred to under the umbrella term : teleoperation . Teleoperation
can be viewed as the ‚Äú stuff ‚Äù in the middle of the two forks . In practice , intelligent robots such as the Mars Sojourner are controlled with some form of
teleoperation . This chapter will cover the flavors of teleoperation , given their
importance as a stepping stone towards truly intelligent robots . 
The chapter concludes by visiting the issues in AI , and argues that AI is imperative for many robotic applications . Teleoperation is simply not sufficient
or desirable as a long term solution . However , it has served as a reasonable
patch . 
It is interesting to note that the two forks , manufacturing and AI , currently
appear to be merging . Manufacturing is now shifting to a ‚Äú mass customization ‚Äù phase , where companies which can economically make short runs of
special order goods are thriving . The pressure is on for industrial robots , 
more correctly referred to as industrial manipulators , to be rapidly reprogrammed and more forgiving if a part isn ‚Äô t placed exactly as expected in its
workspace . As a result , AI techniques are migrating to industrial manipulators . 
1 . 2 How Can a Machine Be Intelligent ? 
ARTIFICIAL The science of making machines act intelligently is usually referred to as artifiINTELLIGENCE cial intelligence , or AI for short . Artificial Intelligence has no commonly accepted definitions . One of the first textbooks on AI defined it as ‚Äú the study
of ideas that enable computers to be intelligent ,  ‚Äù 143 which seemed to beg the
question . A later textbook was more specific ,  ‚Äú AI is the attempt to get the
computer to do things that , for the moment , people are better at .  ‚Äù 120 This
definition is interesting because it implies that once a task is performed successfully by a computer , then the technique that made it possible is no longer
AI , but something mundane . That definition is fairly important to a person
researching AI methods for robots , because it explains why certain topics
suddenly seem to disappear from the AI literature : it was perceived as being
solved ! Perhaps the most amusing of all AI definitions was the slogan for
the now defunct computer company , Thinking Machines , Inc .  ,  ‚Äú  .  .  . making
machines that will be proud of us .  ‚Äù 
16 1 From Teleoperation To Autonomy
The term AI is controversial , and has sparked ongoing philosophical debates on whether a machine can ever be intelligent . As Roger Penrose notes
in his book , The Emperor ‚Äô s New Mind :  ‚Äú Nevertheless , it would be fair to
say that , although many clever things have indeed been done , the simulation of anything that could pass for genuine intelligence is yet a long way
off .  ‚Äú 115 Engineers often dismiss AI as wild speculation . As a result of such
vehement criticisms , many researchers often label their work as ‚Äú intelligent
systems ‚Äù or " knowledge-based systems ‚Äù in an attempt to avoid the controversy surrounding the term ‚Äú AI .  ‚Äù 
A single , precise definition of AI is not necessary to study AI robotics . AI
robotics is the application of AI techniques to robots . More specifically , AI
robotics is the consideration of issues traditional covered by AI for application to robotics : learning , planning , reasoning , problem solving , knowledge
representation , and computer vision . An article in the May 5 , 1997 issue
of Newsweek ,  ‚Äú Actually , Chess is Easy ,  ‚Äù discusses why robot applications
are more demanding for AI than playing chess . Indeed , the concepts of the
reactive paradigm , covered in Chapter 4 , influenced major advances in traditional , non-robotic areas of AI , especially planning . So by studying AI robotics , a reader interested in AI is getting exposure to the general issues in
AI . 
1 . 3 What Can Robots Be Used For ? 
Now that a working definition of a robot and artificial intelligence has been
established , an attempt can be made to answer the question : what can intelligent robots be used for ? The short answer is that robots can be used for just
about any application that can be thought of . The long answer is that robots
are well suited for applications where 1 ) a human is at significant risk ( nuclear , space , military )  , 2 ) the economics or menial nature of the application
result in inefficient use of human workers ( service industry , agriculture )  , and
3 ) for humanitarian uses where there is great risk ( demining an area of land
mines , urban search and rescue )  . Or as the well-worn joke among roboticists
THE 3 D ‚Äô S goes , robots are good for the 3 D ‚Äô s : jobs that are dirty , dull , or dangerous . 
Historically , the military and industry invested in robotics in order to build
nuclear weapons and power plants ; now , the emphasis is on using robots for
environmental remediation and restoration of irradiated and polluted sites . 
Many of the same technologies developed for the nuclear industry for processing radioactive ore is now being adapted for the pharmaceutical indus-
1 . 3 What Can Robots Be Used For ? 17
try ; processing immune suppressant drugs may expose workers to highly
toxic chemicals . 
Another example of a task that poses significant risk to a human is space
exploration . People can be protected in space from the hard vacuum , solar
radiation , etc .  , but only at great economic expense . Furthermore , space suits
are so bulky that they severely limit an astronaut ‚Äô s ability to perform simple
tasks , such as unscrewing and removing an electronics panel on a satellite . 
Worse yet , having people in space necessitates more people in space . Solar
radiation embrittlement of metals suggests that astronauts building a large
space station would have to spend as much time repairing previously built
portions as adding new components . Even more people would have to be
sent into space , requiring a larger structure . the problem escalates . A study
by Dr . Jon Erickson ‚Äô s research group at NASA Johnson Space Center argued
that a manned mission to Mars was not feasible without robot drones capable
of constantly working outside of the vehicle to repair problems introduced
by deadly solar radiation . 51 ( Interestingly enough , a team of three robots
which did just this were featured in the 1971 film , Silent Running , as well as
by a young R2D2 in The Phantom Menace .  ) 
Nuclear physics and space exploration are activities which are often far removed from everyday life , and applications where robots figure more prominently in the future than in current times . 
The most obvious use of robots is manufacturing , where repetitious activities in unpleasant surroundings make human workers inefficient or expensive to retain . For example , robot ‚Äú arms ‚Äù have been used for welding
cars on assembly lines . One reason that welding is now largely robotic is
that it is an unpleasant job for a human ( hot , sweaty , tedious work ) with
a low tolerance for inaccuracy . Other applications for robots share similar
motivation : to automate menial , unpleasant tasks‚Äîusually in the service industry . One such activity is janitorial work , especially maintaining public
rest rooms , which has a high turnover in personnel regardless of payscale . 
The janitorial problem is so severe in some areas of the US , that the Postal
Service offered contracts to companies to research and develop robots capable of autonomously cleaning a bathroom ( the bathroom could be designed
to accommodate a robot )  . 
Agriculture is another area where robots have been explored as an economical alternative to hard to get menial labor . Utah State University has
been working with automated harvesters , using GPS ( global positioning satellite system ) to traverse the field while adapting the speed of harvesting
to the rate of food being picked , much like a well-adapted insect . The De-
18 1 From Teleoperation To Autonomy
partment of Mechanical and Material Engineering at the University of Western Australia developed a robot called Shear Majic capable of shearing a live
sheep . People available for sheep shearing has declined , along with profit
margins , increasing the pressure on the sheep industry to develop economic
alternatives . Possibly the most creative use of robots for agriculture is a mobile automatic milker developed in the Netherlands and in Italy . 68 ; 32 Rather
than have a person attach the milker to a dairy cow , the roboticized milker
arm identifies the teats as the cow walks into her stall , targets them , moves
about to position itself , and finally reaches up and attaches itself . 
Finally , one of the most compelling uses of robots is for humanitarian purposes . Recently , robots have been proposed to help with detecting unexploded ordinance ( land mines ) and with urban search and rescue ( finding
survivors after a terrorist bombing of a building or an earthquake )  . Humanitarian land demining is a challenging task . It is relatively easy to demine an
area with bulldozer , but that destroys the fields and improvements made by
the civilians and hurts the economy . Various types of robots are being tested
in the field , including aerial and ground vehicles . 73
1 . 3 . 1 Social implications of robotics
While many applications for artificially intelligent robots will actively reduce
risk to a human life , many applications appear to compete with a human ‚Äô s
livelihood . Don ‚Äô t robots put people out of work ? One of the pervasive
themes in society has been the impact of science and technology on the dignity of people . Charlie Chaplin ‚Äô s silent movie , Modern Times , presented the
world with visual images of how manufacturing-oriented styles of management reduces humans to machines , just ‚Äú cogs in the wheel .  ‚Äù 
Robots appear to amplify the tension between productivity and the role of
the individual . Indeed , the scientist in Metropolis points out to the corporate
ruler of the city that now that they have robots , they don ‚Äô t need workers
anymore . People who object to robots , or technology in general , are ofLUDDITES ten called Luddites , after Ned Ludd , who is often credited with leading a
short-lived revolution of workers against mills in Britain . Prior to the industrial revolution in Britain , wool was woven by individuals in their homes
or collectives as a cottage industry . Mechanization of the weaving process
changed the jobs associated with weaving , the status of being a weaver ( it
was a skill )  , and required people to work in a centralized location ( like having your telecommuting job terminated )  . Weavers attempted to organize and
destroyed looms and mill owners ‚Äô properties in reaction . After escalating vi-
1 . 4 A Brief History of Robotics 19
olence in 1812 , legislation was passed to end worker violence and protect the
mills . The rebelling workers were persecuted . While the Luddite movement
may have been motivated by a quality-of-life debate , the term is often applied to anyone who objects to technology , or ‚Äú progress ,  ‚Äù for any reason . The
connotation is that Luddites have an irrational fear of technological progress . 
The impact of robots is unclear , both what is the real story and how people
interact with robots . The HelpMate Robotics , Inc . robots and janitorial robots
appear to be competing with humans , but are filling a niche where it is hard
to get human workers at any price . Cleaning office buildings is menial and
boring , plus the hours are bad . One janitorial company has now invested in
mobile robots through a Denver-based company , Continental Divide Robotics , citing a 90% yearly turnover in staff , even with profit sharing after two
years . The Robotics Industries Association , a trade group , produces annual
reports outlining the need for robotics , yet possibly the biggest robot money
makers are in the entertainment and toy industries . 
The cultural implications of robotics cannot be ignored . While the sheep
shearing robots in Australia were successful and were ready to be commercialized for significant economic gains , the sheep industry reportedly rejected the robots . One story goes that the sheep ranchers would not accept
a robot shearer unless it had a 0% fatality rate ( it ‚Äô s apparently fairly easy to
nick an artery on a squirming sheep )  . But human shearers accidently kill
several sheep , while the robots had a demonstrably better rate . The use of
machines raises an ethical question : is it acceptable for an animal to die at the
hands of a machine rather than a person ? What if a robot was performing a
piece of intricate surgery on a human ? 
1 . 4 A Brief History of Robotics
Robotics has its roots in a variety of sources , including the way machines are
controlled and the need to perform tasks that put human workers at risk . 
In 1942 , the United States embarked on a top secret project , called the Manhattan Project , to build a nuclear bomb . The theory for the nuclear bomb had
existed for a number of years in academic circles . Many military leaders of
both sides of World War II believed the winner would be the side who could
build the first nuclear device : the Allied Powers led by USA or the Axis , led
by Nazi Germany . 
One of the first problems that the scientists and engineers encountered
was handling and processing radioactive materials , including uranium and
20 1 From Teleoperation To Autonomy
Figure 1 . 2 A Model 8 Telemanipulator . The upper portion of the device is placed
in the ceiling , and the portion on the right extends into the hot cell .  ( Photograph
courtesy Central Research Laboratories .  ) 
plutonium , in large quantities . Although the immensity of the dangers of
working with nuclear materials was not well understood at the time , all the
personnel involved knew there were health risks . One of the first solutions
was the glove box . Nuclear material was placed in a glass box . A person
stood ( or sat ) behind a leaded glass shield and stuck their hands into thick
rubberized gloves . This allowed the worker to see what they were doing and
to perform almost any task that they could do without gloves . 
But this was not an acceptable solution for highly radioactive materials , 
and mechanisms to physically remove and completely isolate the nuclear
materials from humans had to be developed . One such mechanism was
TELEMANIPULATOR a force reflecting telemanipulator , a sophisticated mechanical linkage which
translated motions on one end of the mechanism to motions at the other end . 
A popular telemanipulator is shown in Fig . 1 . 2 . 
A nuclear worker would insert their hands into ( or around ) the telemanipulator , and move it around while watching a display of what the other
end of the arm was doing in a containment cell . Telemanipulators are similar in principle to the power gloves now used in computer games , but much
harder to use . The mechanical technology of the time did not allow a perfect
mapping of hand and arm movements to the robot arm . Often the opera-
1 . 4 A Brief History of Robotics 21
tor had to make non-intuitive and awkward motions with their arms to get
the robot arm to perform a critical manipulation‚Äîvery much like working in
front of a mirror . Likewise , the telemanipulators had challenges in providing
force feedback so the operator could feel how hard the gripper was holding
an object . The lack of naturalness in controlling the arm ( now referred to as
a poor Human-Machine Interface ) meant that even simple tasks for an unencumbered human could take much longer . Operators might take years of
practice to reach the point where they could do a task with a telemanipulator
as quickly as they could do it directly . 
After World War II , many other countries became interested in producing a
nuclear weapon and in exploiting nuclear energy as a replacement for fossil
fuels in power plants . The USA and Soviet Union also entered into a nuclear arms race . The need to mass-produce nuclear weapons and to support
peaceful uses of nuclear energy kept pressure on engineers to design robot
arms which would be easier to control than telemanipulators . Machines that
looked more like and acted like robots began to emerge , largely due to advances in control theory . After WWII , pioneering work by Norbert Wiener
allowed engineers to accurately control mechanical and electrical devices using cybernetics . 
1 . 4 . 1 Industrial manipulators
Successes with at least partially automating the nuclear industry also meant
the technology was available for other applications , especially general manufacturing . Robot arms began being introduced to industries in 1956 by
Unimation ( although it wouldn ‚Äô t be until 1972 before the company made a
profit )  . 37 The two most common types of robot technology that have evolved
for industrial use are robot arms , called industrial manipulators , and mobile
carts , called automated guided vehicles ( AGVs )  . 
INDUSTRIAL An industrial manipulator , to paraphrase the Robot Institute of America ‚Äô s
MANIPULATOR definition , is a reprogrammable and multi-functional mechanism that is designed to move materials , parts , tools , or specialized devices . The emphasis
in industrial manipulator design is being able to program them to be able
to perform a task repeatedly with a high degree of accuracy and speed . In
order to be multi-functional , many manipulators have multiple degrees of
freedom , as shown in Fig . 1 . 4 . The MOVEMASTER arm has five degrees
of freedom , because it has five joints , each of which is capable of a single
rotational degree of freedom . A human arm has three joints ( shoulder , el-
22 1 From Teleoperation To Autonomy
Figure 1 . 3 An RT3300 industrial manipulator .  ( Photograph courtesy of Seiko Instruments .  ) 
bow , and wrist )  , two of which are complex ( shoulder and wrist )  , yielding six
degrees of freedom . 
Control theory is extremely important in industrial manipulators . Rapidly
moving around a large tool like a welding gun introduces interesting problems , like when to start decelerating so the gun will stop in the correct location without overshooting and colliding with the part to be welded . Also , 
oscillatory motion , in general , is undesirable . Another interesting problem is
the joint configuration . If a robot arm has a wrist , elbow and shoulder joints
like a human , there are redundant degrees of freedom . Redundant degrees
of freedom means there are multiple ways of moving the joints that will accomplish the same motion . Which one is better , more efficient , less stressful
on the mechanisms ? 
It is interesting to note that most manipulator control was assumed to be
BALLISTIC CONTROL ballistic control , or open loop control . In ballistic control , the position trajectory
OPEN LOOP CONTROL and velocity profile is computed once , then the arm carries it out . There
are no ‚Äú in-flight ‚Äù corrections , just like a ballistic missile doesn ‚Äô t make any
course corrections . In order to accomplish a precise task with ballistic control , 
everything about the device and how it works has to be modeled and figured
CLOSED-LOOP into the computation . The opposite of ballistic control is closed-loop control , 
CONTROL where the error between the goal and current position is noted by a sensor ( s )  , 
1 . 4 A Brief History of Robotics 23
a . b . 
Figure 1 . 4 A MOVEMASTER robot : a .  ) the robot arm and b .  ) the associated joints . 
and a new trajectory and profile is computed and executed , then modified on
the next update , and so on . Closed-loop control requires external sensors to
FEEDBACK provide the error signal , or feedback . 
In general , if the structural properties of the robot and its cargo are known , 
these questions can be answered and a program can be developed . In practice , the control theory is complex . The dynamics ( how the mechanism moves
and deforms ) and kinematics ( how the components of the mechanism are
connected ) of the system have to be computed for each joint of the robot , then
those motions can be propagated to the next joint iteratively . This requires a
computationally consuming change of coordinate systems from one joint to
the next . To move the gripper in Fig 1 . 4 requires four changes of coordinates
to go from the base of the arm to the gripper . The coordinate transformations
often have singularities , causing the equations to perform divide by zeros . It
can take a programmer weeks to reprogram a manipulator . 
One simplifying solution is to make the robot rigid at the desired velocities , 
reducing the dynamics . This eliminates having to compute the terms for
overshooting and oscillating . However , a robot is made rigid by making it
24 1 From Teleoperation To Autonomy
heavier . The end result is that it is not uncommon for a 2 ton robot to be
able to handle only a 200 pound payload . Another simplifying solution is to
avoid the computations in the dynamics and kinematics and instead have the
TEACH PENDANT programmer use a teach pendant . Using a teach pendant ( which often looks
like a joystick or computer game console )  , the programmer guides the robot
through the desired set of motions . The robot remembers these motions and
creates a program from them . Teach pendants do not mitigate the danger
of working around a 2 ton piece of equipment . Many programmers have to
direct the robot to perform delicate tasks , and have to get physically close
to the robot in order to see what the robot should do next . This puts the
programmer at risk of being hit by the robot should it hit a singularity point
in its joint configuration or if the programmer makes a mistake in directing
a motion . You don ‚Äô t want to have your head next to a 2 ton robot arm if it
suddenly spins around ! 
AUTOMATIC GUIDED Automatic guided vehicles , or AGVs , are intended to be the most flexible conVEHICLES veyor system possible : a conveyor which doesn ‚Äô t need a continuous belt or
roller table . Ideally an AGV would be able to pick up a bin of parts or manufactured items and deliver them as needed . For example , an AGV might
receive a bin containing an assembled engine . It could then deliver it automatically across the shop floor to the car assembly area which needed an
engine . As it returned , it might be diverted by the central computer and instructed to pick up a defective part and take it to another area of the shop for
reworking . 
However , navigation ( as will be seen in Part II ) is complex . The AGV has
to know where it is , plan a path from its current location to its goal destination , and to avoid colliding with people , other AGVs , and maintenance
workers and tools cluttering the factory floor . This proved too difficult to do , 
especially for factories with uneven lighting ( which interferes with vision ) 
and lots of metal ( which interferes with radio controllers and on-board radar
and sonar )  . Various solutions converged on creating a trail for the AGV to
follow . One method is to bury a magnetic wire in the floor for the AGV to
sense . Unfortunately , changing the path of an AGV required ripping up the
concrete floor . This didn ‚Äô t help with the flexibility needs of modern manufacturing . Another method is to put down a strip of photochemical tape for
the vehicle to follow . The strip is unfortunately vulnerable , both to wear and
to vandalism by unhappy workers . Regardless of the guidance method , in
the end the simplest way to thwart an AGV was to something on its path . 
If the AGV did not have range sensors , then it would be unable to detect
an expensive piece of equipment or a person put deliberately in its path . A
1 . 4 A Brief History of Robotics 25
few costly collisions would usually led to the AGV ‚Äô s removal . If the AGV
did have range sensors , it would stop for anything . A well placed lunch box
could hold the AGV for hours until a manager happened to notice what was
going on . Even better from a disgruntled worker ‚Äô s perspective , many AGVs
would make a loud noise to indicate the path was blocked . Imagine having
to constantly remove lunch boxes from the path of a dumb machine making
unpleasant siren noises . 
From the first , robots in the workplace triggered a backlash . Many of the
human workers felt threatened by a potential loss of jobs , even though the
jobs being mechanized were often menial or dangerous . This was particularly true of manufacturing facilities which were unionized . One engineer
reported that on the first day it was used in a hospital , a HelpMate Robotics
cart was discovered pushed down the stairs . Future models were modified
to have some mechanisms to prevent malicious acts . 
Despite the emerging Luddite effect , industrial engineers in each of the
BLACK FACTORY economic powers began working for a black factory in the 1980 ‚Äô s . A black factory is a factory that has no lights turned on because there are no workers . 
Computers and robots were expected to allow complete automation of manufacturing processes , and courses in ‚Äú Computer-Integrated Manufacturing
Systems ‚Äù became popular in engineering schools . 
But two unanticipated trends undermined industrial robots in a way that
the Luddite movement could not . First , industrial engineers did not have
experience designing manufacturing plants with robots . Often industrial
manipulators were applied to the wrong application . One of the most embarrassing examples was the IBM Lexington printer plant . The plant was
built with a high degree of automation , and the designers wrote numerous
articles on the exotic robot technology they had cleverly designed . Unfortunately , IBM had grossly over-estimated the market for printers and the plant
sat mostly idle at a loss . While the plant ‚Äô s failure wasn ‚Äô t the fault of robotics , 
per se , it did cause many manufacturers to have a negative view of automation in general . The second trend was the changing world economy . Customers were demanding ‚Äú mass customization .  ‚Äù Manufacturers who could
make short runs of a product tailored to each customer on a large scale were
the ones making the money .  ( Mass customization is also referred to as ‚Äú agile
manufacturing .  ‚Äù  ) However , the lack of adaptability and difficulties in programming industrial robot arms and changing the paths of AGVs interfered
with rapid retooling . The lack of adaptability , combined with concerns over
worker safety and the Luddite effect , served to discourage companies from
investing in robots through most of the 1990 ‚Äô s . 
26 1 From Teleoperation To Autonomy
a . b . 
Figure 1 . 5 Motivation for intelligent planetary rovers : a .  ) Astronaut John Young
awkwardly collecting lunar samples on Apollo 16 , and b .  ) Astronaut Jim Irwin stopping the lunar rover as it slides down a hill on Apollo 15 .  ( Photographs courtesy of
the National Aeronautics and Space Administration .  ) 
1 . 4 . 2 Space robotics and the AI approach
While the rise of industrial manipulators and the engineering approach to
robotics can in some measure be traced to the nuclear arms race , the rise
of the AI approach can be said to start with the space race . On May 25 , 
1961 , spurred by the success of the Soviet Union ‚Äô s Sputnik space programs , 
President John F . Kennedy announced that United States would put a man
on the moon by 1970 . Walking on the moon was just one aspect of space
exploration . There were concerns about the Soviets setting up military bases
on the Moon and Mars and economic exploitation of planetary resources . 
Clearly there was going to be a time lag of almost a decade before humans
from the USA would go to the Moon . And even then , it would most likely be
with experimental spacecraft , posing a risk to the human astronauts . Even
without the risk to humans , the bulk of spacesuits would make even trivial tasks difficult for astronauts to perform . Fig . 1 . 5a shows astronaut John
Young on Apollo 16 collecting samples with a lunar rake . The photo shows
the awkward way the astronaut had to bend his body and arms to complete
the task . 
Planetary rovers were a possible solution , either to replace an astronaut or
assist him or her . Unfortunately , rover technology in the 1960 ‚Äô s was limited . 
Because of the time delays , a human would be unable to safely control a rover
over the notoriously poor radio links of the time , even if the rover went very
1 . 4 A Brief History of Robotics 27
slow . Therefore , it would be desirable to have a robot that was autonomous . 
One option would be to have mobile robots land on a planetary conduct preliminary explorations , conduct tests , etc .  , and radio back the results . These
automated planetary rovers would ideally have a high degree of autonomy , 
much like a trained dog . The robot would receive commands from Earth
to explore a particular region . It would navigate around boulders and not
fall into canyons , and traverse steep slopes without rolling over . The robot
might even be smart enough to regulate its own energy supply , for example , 
by making sure it was sheltered during the planetary nights and to stop what
it was doing and position itself for recharging its solar batteries . A human
might even be able to speak to it in a normal way to give it commands . 
Getting a mobile robot to the level of a trained dog immediately presented
new issues . Just by moving around , a mobile robot could change the worldfor instance , by causing a rock slide . Fig . 1 . 5b shows astronaut Jim Irwin rescuing the lunar rover during an extra-vehicular activity ( EVA ) on Apollo 15
as it begins to slide downhill . Consider that if an astronaut has difficulty finding a safe parking spot on the moon , how much more challenging it would
be for an autonomous rover . Furthermore , an autonomous rover would have
no one to rescue it , should it make a mistake . 
Consider the impact of uncertain or incomplete information on a rover
that didn ‚Äô t have intelligence . If the robot was moving based on a map taken
from a telescope or an overhead command module , the map could still contain errors or at the wrong resolution to see certain dangers . In order to
navigate successfully , the robot has to compute its path with the new data
or risk colliding with a rock or falling into a hole . What if the robot did
something broke totally unexpected or all the assumptions about the planet
were wrong ? In theory , the robot should be able to diagnose the problem
and attempt to continue to make progress on its task . What seemed at first
like an interim solution to putting humans in space quickly became more
complicated . 
Clearly , developing a planetary rover and other robots for space was going to require a concentrated , long-term effort . Agencies in the USA such
as NASA Jet Propulsion Laboratory ( JPL ) in Pasadena , California , were given
the task of developing the robotic technology that would be needed to prepare the way for astronauts in space . They were in a position to take advantage of the outcome of the Dartmouth Conference . The Dartmouth Conference
was a gathering hosted by the Defense Advanced Research Projects Agency 
( DARPA ) in 1955 of prominent scientists working with computers or on the
theory for computers . DARPA was interested in hearing what the potential
28 1 From Teleoperation To Autonomy
uses for computers were . One outcome of the conference was the term ‚Äú artificial intelligence ‚Äù  ; the attending scientists believed that computers might
become powerful enough to understand human speech and duplicate human reasoning . This in turn suggested that computers might mimic the capabilities of animals and humans sufficiently for a planetary rover to survive
for long periods with only simple instructions from Earth . 
As an indirect result of the need for robotics converging with the possibility of artificial intelligence , the space program became one of the earliest
proponents of developing AI for robotics . NASA also introduced the notion
that AI robots would of course be mobile , rather than strapped to a factory
floor , and would have to integrate all forms of AI ( understanding speech , 
planning , reasoning , representing the world , learning ) into one program‚Äîa
daunting task which has not yet been reached . 
1 . 5 Teleoperation
TELEOPERATION Teleoperation is when a human operator controls a robot from a distance ( tele
means ‚Äú remote ‚Äù  )  . The connotation of teleoperation is that the distance is too
great for the operator to see what the robot is doing , so radio controlled toy
cars are not considered teleoperation systems . The operator and robot have
some type of master-slave relationship . In most cases , the human operator
sits at a workstation and directs a robot through some sort of interface , as
seen in Fig . 1 . 6 . 
The control interface could be a joystick , virtual reality gear , or any number of innovative interfaces . The human operator , or teleoperator , is often
LOCAL referred to as the local ( due to being at the local workstation ) and the robot
REMOTE as the remote ( since it is operating at a remote location from the teleoperator )  . 
The local must have some type of display and control mechanisms , while the
remote must have sensors , effectors , power , and in the case of mobile robots , 
mobility . 141 The teleoperator cannot look at what the remote is doing directly , 
either because the robot is physically remote ( e . g .  , on Mars ) or the local has
to be shielded ( e . g .  , in a nuclear or pharmaceutical processing plant hot cell )  . 
SENSORS Therefore , the sensors which acquire information about the remote location , 
DISPLAY the display technology for allowing the operator to see the sensor data , and
COMMUNICATION LINK the communication link between the local and remote are critical components
of a telesystem . 141
Teleoperation is a popular solution for controlling remotes because AI technology is nowhere near human levels of competence , especially in terms of
1 . 5 Teleoperation 29
Sensor
Mobility
REMOTE
Effector
Power
Display
Control
LOCAL
Communication
Figure 1 . 6 Organization of a telesystem .  ( Photographs courtesy of Oak Ridge National Laboratory .  ) 
perception and decision making . One example of teleoperation is the exploration of underwater sites such as the Titanic . Having a human control a
robot is advantageous because a human can isolate an object of interest , even
partially obscured by mud in murky water as described by W . R . Uttal . 141
Humans can also perform dextrous manipulation ( e . g .  , screwing a nut on a
bolt )  , which is very difficult to program a manipulator to do . 
30 1 From Teleoperation To Autonomy
Figure 1 . 7 Sojourner Mars rover .  ( Photograph courtesy of the National Aeronautics
and Space Administration .  ) 
Another example is the Sojourner robot ( shown in Fig . 1 . 7 ) which explored
Mars from July 5 to September 27 , 1997 , until it ceased to reply to radio commands . Since there was little data before Sojourner on what Mars is like , 
it is hard to develop sensors and algorithms which can detect important attributes or even control algorithms to move the robot . It is important that any
unusual rocks or rock formations ( like the orange rock Dr . Schmitt found on
the Moon during Apollo 17 ) be detected . Humans are particularly adept at
perception , especially seeing patterns and anomalies in pictures . Current AI
perceptual abilities fall far short of human abilities . Humans are also adept
at problem solving . When the Mars Pathfinder craft landed on Mars , the
air bags that had cushioned the landing did not deflate properly . When the
petals of the lander opened , an airbag was in the way of Sojourner . The
solution ? The ground controllers sent commands to retract the petals and
open them again . That type of problem solving is extremely difficult for the
current capabilities of AI . 
But teleoperation is not an ideal solution for all situations . Many tasks
are repetitive and boring . For example , consider using a joystick to drive
a radio-controlled car ; after a few hours , it tends to get harder and harder
to pay attention . Now imagine trying to control the car while only looking
through a small camera mounted in front . The task becomes much harder
1 . 5 Teleoperation 31
because of the limited field of view ; essentially there is no peripheral vision . 
Also , the camera may not be transmitting new images very fast because the
communication link has a limited bandwidth , so the view is jerky . Most peoCOGNITIVE FATIGUE ple quickly experience cognitive fatigue ; their attention wanders and they may
even experience headaches and other physical symptoms of stress . Even if
SIMULATOR SICKNESS the visual display is excellent , the teleoperator may get simulator sickness due
to the discordance between the visual system saying the operator is moving
and the inner ear saying the operator is stationary . 141
Another disadvantage of teleoperation is that it can be inefficient to use for
applications that have a large time delay . 
128 A large time delay can result in
the teleoperator giving a remote a command , unaware that it will place the
remote in jeopardy . Or , an unanticipated event such as a rock fall might occur
and destroy the robot before the teleoperator can see the event and command
TELEOPERATION the robot to flee . A rule of thumb , or heuristic , is that the time it takes to do
HEURISTIC a task with traditional teleoperation grows linearly with the transmission
delay . A teleoperation task which took 1 minute for a teleoperator to guide a
remote to do on the Earth might take 2 . 5 minutes to do on the Moon , and 140
minutes on Mars . 142 Fortunately , researchers have made some progress with
PREDICTIVE DISPLAYS predictive displays , which immediately display what the simulation result of
the command would be . 
The impact of time delays is not limited to planetary rovers . A recent example of an application of teleoperation are unmanned aerial vehicles ( UAV ) 
used by the United States to verify treaties by flying overhead and taking
videos of the ground below . Advanced prototypes of these vehicles can fly
autonomously , but take-offs and landings are difficult for on-board computer
control . In this case of the Darkstar UAV ( shown in Fig . 1 . 8 )  , human operators were available to assume teleoperation control of the vehicle should
it encounter problems during take-off . Unfortunately , the contingency plan
did not factor in the 7 second delay introduced by using a satellite as the
communications link . Darkstar no . 1 did indeed experience problems on
take-off , but the teleoperator could not get commands to it fast enough before it crashed . As a result , it earned the unofficial nickname ‚Äú Darkspot .  ‚Äù 
Another practical drawback to teleoperation is that there is at least one
person per robot , possibly more . The Predator unmanned aerial vehicle has
been used by the United States for verification of the Dayton Accords in
Bosnia . One Predator requires at least one teleoperator to fly the vehicle and
another teleoperator to command the sensor payload to look at particular
areas . Other UAVs have teams composed of up to four teleoperators plus a
fifth team member who specializes in takeoffs and landings . These teleop-
32 1 From Teleoperation To Autonomy
Figure 1 . 8 Dark Star unmanned aerial vehicle .  ( Photograph courtesy of DefenseLink , Office of the Assistant Secretary of Defense-Public Affairs .  ) 
erators may have over a year of training before they can fly the vehicle . In
the case of UAVs , teleoperation permits a dangerous , important task to be
completed , but with a high cost in manpower . 
According to Wampler , 142 TASK teleoperation is best suited for applications
CHARACTERISTICS where : 
1 . The tasks are unstructured and not repetitive . 
2 . The task workspace cannot be engineered to permit the use of industrial
manipulators . 
3 . Key portions of the task intermittently require dextrous manipulation , especially hand-eye coordination . 
4 . Key portions of the task require object recognition , situational awareness , 
or other advanced perception . 
5 . The needs of the display technology do not exceed the limitations of the
communication link ( bandwidth , time delays )  . 
6 . The availability of trained personnel is not an issue . 
1 . 5 . 1 Telepresence
An early attempt at reducing cognitive fatigue was to add more cameras with
faster update rates to widen the field of view and make it more consistent
with how a human prefers to look at the world . This may not be practical
1 . 5 Teleoperation 33
for many applications because of limited bandwidth . Video telephones , picture phones , or video-conferencing over the Internet with their jerky , asynchronous updates are usually examples of annoying limited bandwidth . In
these instances , the physical restrictions on how much and how fast information can be transmitted result in image updates much slower than the rates
human brains expect . The result of limited bandwidth is jerky motion and
increased cognitive fatigue . So adding more cameras only exacerbates the
problem by adding more information that must be transmitted over limited
bandwidth . 
TELEPRESENCE One area of current research in teleoperation is the use of telepresence to
reduce cognitive fatigue and simulator sickness by making the human-robot
VIRTUAL REALITY interface more natural . Telepresence aims for what is popularly called virtual
reality , where the operator has complete sensor feedback and feels as if she
were the robot . If the operator turns to look in a certain direction , the view
from the robot is there . If the operator pushes on a joystick for the robot to
move forward and the wheels are slipping , the operator would hear and feel
the motors straining while seeing that there was no visual change . This provides a more natural interface to the human , but it is very expensive in terms
of equipment and requires very high bandwidth rates . It also still requires
one person per robot . This is better than traditional teleoperation , but a long
way from having one teleoperator control multiple robots . 
1 . 5 . 2 Semi-autonomous control
SEMI-AUTONOMOUS Another line of research in teleoperation is semi-autonomous control , often
CONTROL called supervisory control , where the remote is given an instruction or por- SUPERVISORY CONTROL tion of a task that it can safely do on its own . There are two flavors of
semi-autonomous control : continuous assistance , or shared control , and control trading . 
SHARED CONTROL In continuous assistance systems , the teleoperator and remote share control . The teleoperator can either delegate a task for the robot to do or can
do it via direct control . If the teleoperator delegates the task to the robot , 
the human must still monitor to make sure that nothing goes wrong . This is
particularly useful for teleoperating robot arms in space . The operator can
relax ( relatively ) while the robot arm moves into the specified position near
a panel , staying on alert in case something goes wrong . Then the operator
can take over and perform the actions which require hand-eye coordination . 
Shared control helps the operator avoid cognitive fatigue by delegating boring , repetitive control actions to the robot . It also exploits the ability of a
34 1 From Teleoperation To Autonomy
human to perform delicate operations . However , it still requires a high communication bandwidth . 
CONTROL TRADING An alternative approach is control trading , where the human initiates an action for the robot to complete autonomously . The human only interacts with
the robot to give it a new command or to interrupt it and change its orders . 
The overall scheme is very much like a parent giving a 10-year old child a
task to do . The parent knows what the child is able to do autonomously 
( e . g .  , clean their room )  . They have a common definition ( clean room means
go to the bedroom , make the bed , and empty the wastebaskets )  . The parent
doesn ‚Äô t care about the details of how the child cleans the room ( e . g .  , whether
the wastebasket is emptied before the bed is made or vice versa )  . Control
trading assumes that the robot is capable of autonomously accomplishing
certain tasks without sharing control . The advantage is that , in theory , the
local operator can give a robot a task to do , then turn attention to another
robot and delegate a task to it , etc . A single operator could control multiple
robots because they would not require even casual monitoring while they
were performing a task . Supervisory control also reduces the demand on
bandwidth and problems with communication delays . Data such as video
images need to be transferred only when the local is configuring the remote
for a new task , not all the time . Likewise , since the operator is not involved
in directly controlling the robot , a 2 . 5 minute delay in communication is irrelevant ; the robot either wrecked itself or it didn ‚Äô t . Unfortunately , control
trading assumes that robots have actions that they can perform robustly even
in unexpected situations ; this may or may not be true . Which brings us back
to the need for artificial intelligence . 
Sojourner exhibited both flavors of supervisory control . It was primarily
programmed for traded control , where the geologists could click on a rock
and Sojourner would autonomously navigate close to it , avoiding rocks , etc . 
However , some JPL employees noted that the geologists tended to prefer to
use shared control , watching every movement . A difficulty with most forms
of shared control is that it is assumed that the human is smarter than the
robot . This may be true , but the remote may have better sensor viewpoints
and reaction times . 
1 . 6 The Seven Areas of AI
Now that some possible uses and shortcomings of robots have been covered , 
it is motivating to consider what are the areas of artificial intelligence and
1 . 6 The Seven Areas of AI 35
how they could be used to overcome these problems . The Handbook of Artificial Intelligence 64 divides up the field into seven main areas : knowledge
representation , understanding natural language , learning , planning and problem
solving , inference , search , and vision . 
KNOWLEDGE 1 . Knowledge representation . An important , but often overlooked , issue is
REPRESENTATION how does the robot represent its world , its task , and itself . Suppose a robot
is scanning a pile of rubble for a human . What kind of data structure and
algorithms would it take to represent what a human looks like ? One way
is to construct a structural model : a person is composed of an oval head , 
a cylindrical torso , smaller cylindrical arms with bilateral symmetry , etc . 
Of course , what happens if only a portion of the human is visible ? 
UNDERSTANDING 2 . Understanding natural language . Natural language is deceptively chalNATURAL LANGUAGE lenging , apart from the issue of recognizing words which is now being
done by commercial products such as Via Voice and Naturally Speaking . 
It is not just a matter of looking up words , which is the subject of the
following apocryphal story about AI . The story goes that after Sputnik
went up , the US government needed to catch up with the Soviet scientists . 
However , translating Russian scientific articles was time consuming and
not many US citizens could read technical reports in Russian . Therefore , 
the US decided to use these newfangled computers to create translation
programs . The day came when the new program was ready for its first
test . It was given the proverb : the spirit is willing , but the flesh is weak . 
The reported output : the vodka is strong , but the meat is rotten . 
LEARNING 3 . Learning . Imagine a robot that could be programmed by just watching a
human , or by just trying the task repeatedly itself . 
PLANNING , PROBLEM 4 . Planning and problem solving . Intelligence is associated with the ability
SOLVING to plan actions needed to accomplish a goal and solve problems with those
plans or when they don ‚Äô t work . One of the earliest childhood fables , the
Three Pigs and the Big , Bad Wolf , involves two unintelligent pigs who
don ‚Äô t plan ahead and an intelligent pig who is able to solve the problem
of why his brothers ‚Äô houses have failed , as well as plan an unpleasant
demise for the wolf . 
INFERENCE 5 . Inference . Inference is generating an answer when there isn ‚Äô t complete
information . Consider a planetary rover looking at a dark region on the
ground . Its range finder is broken and all it has left is its camera and a
fine AI system . Assume that depth information can ‚Äô t be extracted from
36 1 From Teleoperation To Autonomy
the camera . Is the dark region a canyon ? Is it a shadow ? The rover will
need to use inference to either actively or passively disambiguate what
the dark region is ( e . g .  , kick a rock at the dark area versus reason that
there is nothing nearby that could create that shadow )  . 
SEARCH 6 . Search . Search doesn ‚Äô t necessarily mean searching a large physical space
for an object . In AI terms , search means efficiently examining a knowledge representation of a problem ( called a ‚Äú search space ‚Äù  ) to find the
answer . Deep Blue , the computer that beat the World Chess master Gary
Kasparov , won by searching through almost all possible combinations of
moves to find the best move to make . The legal moves in chess given the
current state of the board formed the search space . 
VISION 7 . Vision . Vision is possibly the most valuable sense humans have . Studies
by Harvard psychologist Steven Kosslyn suggest that much of problem
solving abilities stem from the ability to visually simulate the effects of
actions in our head . As such , AI researchers have pursued creating vision
systems both to improve robotic actions and to supplement other work in
general machine intelligence . 
Finally , there is a temptation to assume that the history of AI Robotics is the
story of how advances in AI have improved robotics . But that is not the
case . In many regards , robotics has played a pivotal role in advancing AI . 
Breakthroughs in methods for planning ( operations research types of problems ) came after the paradigm shift to reactivity in robotics in the late 1980 ‚Äô s
showed how unpredictable changes in the environment could actually be exploited to simplify programming . Many of the search engines on the world
wide web use techniques developed for robotics . These programs are called
SOFTWARE AGENTS software agents : autonomous programs which can interact with and adapt to
WEB-BOT their world just like an animal or a smart robot . The term web-bot directly
reflects on the robotic heritage of these AI systems . Even animation is being
changed by advances in AI robotics . According to a keynote address given
by Danny Hillis at the 1997 Autonomous Agents conference , animators for
Disney ‚Äô s Hunchback of Notre Dame programmed each cartoon character in the
crowd scenes as if it were a simulation of a robot , and used methods that will
be discussed in Ch . 4 . 
1 . 7 Summary 37
1 . 7 Summary
AI robotics is a distinct field , both historically and in scope , from industrial
robotics . Industrial robots has concentrated on control theory issues , particularly solving the dynamics and kinematics of a robot . This is concerned with
having the stationary robot perform precise motions , repetitively in a structured factory environment . AI robotics has concentrated on how a mobile
robot should handle unpredictable events in an unstructured world . The design of an AI robot should consider how the robot will represent knowledge
about the world , whether it needs to understand natural language , can it
learn tasks , what kind of planning and problem solving will it have to do , 
how much inference is expected , how can it rapidly search its database and
knowledge for answers , and what mechanisms will it use for perceiving the
world . 
Teleoperation arose as an intermediate solution to tasks that required automation but for which robots could not be adequately programmed to handle . Teleoperation methods typically are cognitive fatiguing , require high
communication bandwidths and short communication delays , and require
one or more teleoperators per remote . Telepresence techniques attempt to
create a more natural interface for the human to control the robot and interpret what it is doing and seeing , but at a high communication cost . Supervisory control attempts to delegate portions of the task to the remote , either to
do autonomously ( traded control ) or with reduced , but continuous , human
interaction ( shared control )  . 
1 . 8 Exercises
Exercise 1 . 1
List the four attributes for evaluating an architecture . Based on what you know from
your own experience , evaluate MS Windows 95 / 98 / 2000 as an architecture for teleoperating a robot . 
Exercise 1 . 2
Name the three primitives for expressing the components of a robotics paradigm . 
Exercise 1 . 3
Name the three robotic paradigms , and draw the relationship between the primitives . 
Exercise 1 . 4
What is an intelligent robot ? 
38 1 From Teleoperation To Autonomy
Exercise 1 . 5
What is a Luddite ? 
Exercise 1 . 6
Describe at least two differences between AI and Engineering approaches to robotics . 
Exercise 1 . 7
List three problems with teleoperation . 
Exercise 1 . 8
Describe the components and the responsibilities of the local and the remote members
of a telesystem . 
Exercise 1 . 9
Describe the difference between telepresence and semi-autonomous control . 
Exercise 1 . 10
List the six characteristics of applications that are well suited for teleoperation . Give
at least two examples of potentially good applications for teleoperation not covered
in the chapter . 
Exercise 1 . 11 [ World Wide Web ] 
Search the world wide web for sites that permit clients to use a robot remotely ( one
example is Xavier at Carnegie Mellon University )  . Decide whether each site is using
human supervisory or shared control , and justify your answer . 
Exercise 1 . 12 [ World Wide Web ] 
Search the world wide web for applications and manufacturers of intelligent robots . 
Exercise 1 . 13 [ World Wide Web ] 
Dr . Harrison ‚Äú Jack ‚Äù Schmitt is a vocal proponent for space mining of Near Earth
Objects ( NEOs ) such as mineral-rich asteroids . Because of the economics of manned
mission , the small size of NEOs , human safety concerns , and the challenges of working in micro-gravity , space mining is expected to require intelligent robots . Search
the web for more information on space mining , and give examples of why robots are
needed . 
Exercise 1 . 14 [ Programming ]  
( This requires a robot with an on-board video camera and a teleoperation interface .  ) 
Teleoperate the robot through a slalom course of obstacles while keeping the robot
in view as if controlling a RC car . Now looking only at the output of the video camera , repeat the obstacle course . Repeat the comparison several times , and keep track
of the time to complete the course and number of collisions with obstacles . Which
viewpoint led to faster completion of the course ? Fewer collisions ? Why ? 
1 . 9 End Notes 39
Exercise 1 . 15 [ Advanced Reading ] 
Read ‚Äú Silicon Babies ,  ‚Äù Scientific American , December 1991 , pp 125-134 , on the challenges of AI robotics . List the 7 topics of AI and give examples of robots or researchers
addressing each topic . 
Exercise 1 . 16 [ Science Fiction ] 
Read ‚Äú Stranger in Paradise ,  ‚Äù Isaac Asimov , The Complete Robot , Doubleday , 1982 , 
and enumerate the problems with telepresence illustrated by this story . 
Exercise 1 . 17 [ Science Fiction ] 
Watch the movie Star Gate . The military uses a teleoperated vehicle ( in reality , NASA
Jet Propulsion Laboratory ‚Äô s Hazbot ) to first go through the star gate and test the environmental conditions on the other side . Discuss other ways in which the team could
have continued to use the robot to their advantage . 
Exercise 1 . 18 [ Science Fiction ] 
Watch the 1971 movie , The Andromeda Strain , by Michael Crichton . The movie has
several nerve wracking scenes as the scientists try to telemanipulate an unknown , 
deadly organism as fast as possible without dropping it . What do you think can be
done with today ‚Äô s robots ? 
1 . 9 End Notes
Finding robots in the popular press
There is no one-stop-shopping publication or web site for robot applications . Robotics
World is a business oriented publication which often has interesting blurbs . Popular
Mechanics and Popular Science often contain short pieces on new robots and applications , although those short bits are often enthusiastically optimistic . A new magazine , 
Robot Science and Technology , appears to be bridging the gap between hobby ‚Äò bots and
research ‚Äò bots . In addition to surfing the web , annual proceedings from the IEEE
International Conference on Robotics and Automation ( ICRA ) and IEEE / RSJ International Conference on Intelligent Robots and Systems ( IROS ) contain scientific articles
on the newest robots emerging from university and government laboratories . Intelligent Robotic Systems , ed . by S . G . Tzafestas , Marcel Dekker Inc , NY , 1991 . This is a
collection of chapters by various researchers and laboratory managers . The work is a
bit dated now , but gives some feel for the variety of applications . 
About Joe Engleberger
Joe Engleberger is often referred to as the ‚Äú Father of Industrial Robots .  ‚Äù His impressive resume includes having formed Unimation . His most recent robotics company
made the HelpMate robot for hospitals . Engleberger participates in many robotics
40 1 From Teleoperation To Autonomy
forums , where his sartorial style ( he always wears a bow tie ) and verbal style ( stridently pro-robotics , and that his company should get more money ) make him easily
recognizable . 
Science fiction and robotics
For science fiction enthusiasts , take a look at Clute , John , and Nicholls , Peter ,  ‚Äú Grolier
Science Fiction : The Multimedia Encyclopedia of Science Fiction ,  ‚Äù Grolier Electronic
Publishing , Danbury , CT , 1995 . This entertaining CD provides a very detailed , crossreferenced look at robots as a theme in science fiction and a lengthy list ( and review ) 
of movies and books with robots in them . One of the most technically accurate movies
about robots is Silent Running . It was directed by Douglas Trumbull who gained fame
for his work in special effects , including 2001 : A Space Odyssey , Close Encounters of the
Third Kind , and Blade Runner . The bulk of the movie concerns the day to day life of
Bruce Dern and three waist-high robots . The robots and how they interact with Dern
and their environment are very realistic and consistent with AI robotics . The only
downside is a laughably ecologically correct plot ( written in part by Steven Bochco
and Michael Cimino ) complete with songs by Joan Baez . Well worth watching for the 
 ‚Äò bots , especially if the audience is curious about the hippie movement . 
Robot name trivia
Marvin , the mail robot , may be named after the cantankerous robot Marvin in The
Hitchhiker ‚Äô s Guide to the Galaxy . That Marvin is widely assumed to be named after the
cantankerous AI researcher , Dr . Marvin Minksy , at MIT . 
Have Spacesuit , Will Travel . 
John Blitch brought to my attention the difficulty the Apollo astronauts had in accomplishing simple tasks due to the bulky space suits . 
2 The Hierarchical Paradigm
Chapter Objectives :  Describe the Hierarchical Paradigm in terms of the three robot paradigms
and its organization of sensing .  Name and evaluate one representative Hierarchical architecture in terms
of : support for modularity , niche targetability , ease of portability to other
domains , robustness .  Solve a simple navigation problem using Strips , given a world model , operators , difference table , and difference evaluator . The state of the world
model should be shown after each step .  Understand the meaning of the following terms as applied to robotics : 
precondition , closed world assumption , open world , frame problem .  Describe the mission planner , navigator , pilot organization of the Nested
Hierarchical Controller .  List two advantages and disadvantages of the Hierarchical Paradigm . 
2 . 1 Overview
The Hierarchical Paradigm is historically the oldest method of organizing intelligence in mainstream robotics . It dominated robot implementations from
1967 , with the inception of the first AI robot , Shakey ( Fig . 2 . 1 ) at SRI , up until
the late 1980 ‚Äô s when the Reactive Paradigm burst onto the scene . 
This chapter begins with a description of the Hierarchical Paradigm in
terms of the SENSE , PLAN , ACT primitives and by its sensing representation . The chapter then covers the use of Strips in Shakey to reason and plan
42 2 The Hierarchical Paradigm
Figure 2 . 1 Shakey , the first AI robot . It was built by SRI for DARPA 1967‚Äì70 .  ( Photograph courtesy of SRI .  ) 
a path . Strips will serve to motivate the reader as to the computer challenges
inherent in even as simple a task as walking across a room . However , Strips
is not an architecture , per se , just an interesting technique which emerged
from trying to build an architecture . Two representative architectures are
presented , NHC and RCS , that serve as examples of robot architectures popular at the time . The chapter concludes with programming considerations . 
2 . 2 Attributes of the Hierarchical Paradigm
As noted in Part I , a robotic paradigm is defined by the relationship between
the three primitives ( SENSE , PLAN , ACT ) and by the way sensory data is
processed and distributed through the system . 
The Hierarchical Paradigm is sequential and orderly , as shown in Figs . 2 . 2
and 2 . 3 . First the robot senses the world and constructs a global world map . 
Then ‚Äú eyes ‚Äù closed , the robot plans all the directives needed to reach the
2 . 2 Attributes of the Hierarchical Paradigm 43
SENSE PLAN ACT
Figure 2 . 2 S , P , A organization of Hierarchical Paradigm . 
ROBOT PRIMITIVES INPUT OUTPUT
SENSE
PLAN
ACT
Sensor data Sensed information
Information ( sensed
and / or cognitive ) 
Sensed information
or directives
Directives
Actuator commands
Figure 2 . 3 Alternative description of how the 3 primitives interact in the Hierarchical Paradigm . 
goal . Finally , the robot acts to carry out the first directive . After the robot has
carried out the SENSE-PLAN-ACT sequence , it begins the cycle again : eyes
open , the robot senses the consequence of its action , replans the directives 
( even though the directives may not have changed )  , and acts . 
As shown in Fig . 2 . 3 , sensing in the Hierarchical Paradigm is monolithic : 
all the sensor observations are fused into one global data structure , which the
WORLD MODEL planner accesses . The global data structure is generally referred to as a world
model . The term world model is very broad ;  ‚Äú world ‚Äù means both the outside
world , and whatever meaning the robot ascribes to it . In the Hierarchical
Paradigm , the world model typically contains
A PRIORI 1 . an a priori ( previously acquired ) representation of the environment the
robot is operating in ( e . g .  , a map of the building )  , 
2 . sensing information ( e . g .  ,  ‚Äú I am in a hallway , based on where I ‚Äô ve traveled , I must be in the northwest hallway ‚Äù  )  , plus
3 . any additional cognitive knowledge that might be needed to accomplish
a task ( e . g .  , all packages received in the mail need to be delivered to Room
118 )  . 
44 2 The Hierarchical Paradigm
Creating a single representation which can store all of this information can
be very challenging . Part of the reason for the ‚Äú sub-turtle ‚Äù velocity was the
lack of computing power during the 1960 ‚Äô s . However , as roboticists in the
1980 ‚Äô s began to study biological intelligence , a consensus arose that even
with increased computing power , the hierarchical , logic-based approach was
unsatisfactory for navigational tasks which require a rapid response time to
an open world . 
2 . 2 . 1 Strips
Shakey , the first AI mobile robot , needed a generalized algorithm for planALGORITHM ning how to accomplish goals .  ( An algorithm is a procedure which is correct
and terminates .  ) For example , it would be useful to have the same program
allow a human to type in that the robot is in Office 311 and should go to
Office 313 or that the robot is in 313 and should the red box . 
GENERAL PROBLEM The method finally selected was a variant of the General Problem Solver
SOLVER ( GPS ) method , called Strips . Strips uses an approach called means-ends analysis , STRIPS
MEANS-ENDS ANALYSIS where if the robot can ‚Äô t accomplish the task or reach the goal in one ‚Äú movement ‚Äù  , it picks a action which will reduce the difference between what state
it was in now ( e . g .  , where it was ) versus the goal state ( e . g .  , where it wanted
to be )  . This is inspired by cognitive behavior in humans ; if you can ‚Äô t see how
to solve a problem , you try to solve a portion of the problem to see if it gets
you closer to the complete solution . 
Consider trying to program a robot to figure out how to get to the Stanford AI Lab ( SAIL )  . Unless the robot is at SAIL ( represented in Strips as a
GOAL STATE variable goal state )  , some sort of transportation will have to arranged . 
INITIAL STATE Suppose the robot is in Tampa , Florida ( initial state )  . The robot may
represent the decision process of how to get to a location as function called an
OPERATOR operator which would consider the Euclidean distance ( a variable named
DIFFERENCE difference ) between the goal state and initial state . The difference between locations could be computed for comparison purposes , or evalDIFFERENCE uation , by the square of the hypotenuse ( difference evaluator )  . For
EVALUATOR example using an arbitrary frame of reference that put Tampa at the center
of the world with made-up distances to Stanford : 
initial state : Tampa , Florida ( 0 , 0 ) 
goal state : Stanford , California ( 1000 , 2828 ) 
difference : 3 , 000
2 . 2 Attributes of the Hierarchical Paradigm 45
DIFFERENCE TABLE This could lead to a data structure called a difference table of how to
make decisions : 
difference operator
d200 fly
100 < d < 200 ride_train
d  100 drive
d < 1 walk
Different modes of transportation are appropriate for different distances . 
A mode of transportation , fly , ride_train , drive , walk , in the table
is really a function in the robot ‚Äô s program . It is also called an operator , because it reduces the value stored in difference as to the distance from being in the initial state of Tampa and wanting to be at the goal state . 
A robot following this difference table would begin by flying as close as it
could to SAIL . 
But suppose the robot flew into the San Francisco airport . It ‚Äô d be within
100 miles of SAIL , so the robot appears to have made an intelligent decision . But now the robot has a new difference to reduce . It examines the
difference table with a new value of difference . The table says the robot should drive . Drive what ? A car ? Ooops : if the robot did have a
personal car , it would be back in Tampa . The robot needs to be able to distinguish between driving its car and driving a rental car . This is done by listing
PRECONDITIONS the preconditions that have to be true in order to execute that particular operator . The preconditions are a column in the difference table , where
a single operator can have multiple preconditions . In practice , the list of
preconditions is quite lengthy , but for the purposes of this example , only
drive_rental , drive_personal will be shown with preconditions . 
difference operator preconditions
d200 fly
100 < d < 200 ride_train
d100 drive_rental at airport
drive_personal at home
d < 1 walk
The difference table is now able to handle the issue of deciding to drive a
rental car . But this introduces a new issue : how does the robot know where
it is at ? This is done by monitoring the state of the robot and its world . If
it took an airplane from Tampa to the San Francisco airport , its state has
changed . Its initial state is now at the San Francisco airport , and no
46 2 The Hierarchical Paradigm
longer Tampa . Therefore , whenever the robot executes an operator , there
is almost always something that has to be added to the robot ‚Äô s knowledge
ADD-LIST of the world state ( which is entered into a add-list ) and something that
DELETE-LIST has to be deleted ( delete-list )  . This two lists are stored in the difference
table so that when the robot picks an operator based on the difference and
operator , it can easily apply the appropriate modifications to the world . The
difference table below is expanded to show the add and delete lists . 
difference operator pre- add- deleteconditions list list
d200 fly at Y at X
at airport
100 < d < 200 train at Y at X
at station
d100 drive_rental at airport
drive_personal at home
d < 1 walk
Of course , the above difference table is fairly incomplete . Driving a rental
car should have a precondition that there is a rental car available .  ( And that
the robot have a waiver from the state highway patrol to drive as an experimental vehicle and a satisfactory method of payment .  ) The number of facts
and preconditions that have to be explicitly represented seem to be growing
explosively . Which is Very Bad from a programming standpoint . 
The main point is that the difference table appears to be a good data structure for representing what a robot needs in planning a trip . It should be
apparent that a recursive function can be written which literally examines
each entry in the table for the first operator that reduces the difference . The
resulting list of operators is actually the plan : a list of the steps ( operators ) 
that the robot has to perform in order to reach a goal . The robot actually
constructs the plan before handing it off to another program to execute . 
At this point in time , it isn ‚Äô t likely that a robot will get on a plane and then
drive . So perhaps the criticisms of Strips is because the example used too
complicated a task to be realistic . Let ‚Äô s see if Strips is more streamlined with
a simple task of getting from one room to another . 
2 . 2 . 2 More realistic Strips example
The first step in creating a Strips planner is to construct a Strips-based repCONSTRUCTING A resentation of the world , or world model . Everything in the world that is
WORLD MODEL
2 . 2 Attributes of the Hierarchical Paradigm 47
AXIOMS relevant to the problem is represented by facts , or axioms , in predicate logic . 
PREDICATES Predicates are functions that evaluate to TRUE or FALSE . By years of AI programming convention , predicates are usually written in uppercase . 
Consider the problem of a robot named IT in a room , R1 , who needs to go
to another room , R2 , in the house shown in Fig . 2 . 4 . In order to solve this
problem using Strips , the robot has to be given a way of representing the
world , which will in turn influence the difference table , a difference evaluator , and how the add and delete lists are written . The world model in the
previous example was never formally defined . 
A world model is generally built up from static facts ( represented as predicates ) from a set of candidates , and things in the world , like the robot . The
robot ‚Äô s name is in all capitals because it exists ( and therefore is TRUE )  . Lowercase identifiers indicate that the thing is a variable , that a real thing hasn ‚Äô t
been assigned to that placeholder yet . 
Suppose the robot was limited to knowing only whether a movable object
was in a room , next to a door or another movable object , and whether a door
was open or closed and what rooms it connected . In a programming sense , 
there would be only three types of things in the world : movable_object 
( such as the robot , boxes it should pick up )  , room , and door . The robot ‚Äô s
knowledge could be represented by the following predicates : 
INROOM ( x , r ) where x is an object of type movable_object , 
r is type room
NEXTTO ( x , t ) where x is a movable_object , 
t is type door or movable_object
STATUS ( d , s ) where d is type door , 
s is an enumerated type : OPEN or CLOSED
CONNECTS ( d , rx , ry ) where d is type door , 
rx , ry are the room
With the above predicates , the world model for the initial state of the world
in Fig . 2 . 4 would be represented by : 
initial state : 
INROOM ( IT , R1 ) 
INROOM ( B1 , R2 ) 
CONNECTS ( D1 , R1 , R2 ) 
CONNECTS ( D1 , R2 , R1 ) 
STATUS ( D1 , OPEN ) 
48 2 The Hierarchical Paradigm
Figure 2 . 4 An example for Strips of two rooms joined by an open door . 
This world model captures that a specific movable_object named IT
is in a room named R1 , and B1 is in another room labeled R2 . A door D1
connects a room named R1 to R2 and it connects R2 to R1 .  ( Two different
CONNECTS predicates are used to represent that a robot can go through the
door from either door .  ) A door called D1 has the enumerated value of being
OPEN . The NEXTTO predicate wasn ‚Äô t used , because it wasn ‚Äô t true and there
would be nothing to bind the variables to . 
Under this style of representation , the world model for the goal state
would be : 
goal state : 
INROOM ( IT , R2 ) 
INROOM ( B1 , R2 ) 
CONNECTS ( D1 , R1 , R2 ) 
CONNECTS ( D1 , R2 , R1 ) 
STATUS ( D1 , OPEN ) 
CONSTRUCTING THE Once the world model is established , it is possible to construct the differDIFFERENCE TABLE ence table . The partial difference table is : 
2 . 2 Attributes of the Hierarchical Paradigm 49
operator preconditions add-list delete-list
OP1 : INROOM ( IT , rk ) NEXTTO ( IT , dx ) 
GOTODOOR ( IT , dx ) CONNECT ( dx , rk , rm ) 
OP2 : CONNECT ( dx , rk , rm ) INROOM ( IT , rm ) INROOM ( IT , rk ) 
GOTHRUDOOR ( IT , dx ) NEXTTO ( IT , dx ) 
STATUS ( dx , OPEN ) 
INROOM ( IT , rk ) 
This difference table says the robot is programmed for only two operations : go to a door , and go through a door . The GOTODOOR operator can be
applied only if the following two preconditions are true :  INROOM ( IT , rk ) The robot is in a room , which will be assigned to the
identifier rk .  CONNECT ( dx , rk , rm ) There is a door , which will be assigned to the
identifier dx , which connects rk to some other room called rm . 
The label IT is used to constrain the predicates . Notice that only the variables dx and rk get bound when GOTODOOR is called . rm can be anything . 
If GOTODOOR is executed , the robot is now next to the door called dx . Nothing gets deleted from the world state because the robot is still in room rk , 
the door dx still connects the two rooms rk and rm . The only thing that has
changed is that the robot is now in a noteworthy position in the room : next
to the door . 
The difference table specifies that the GOTHRUDOOR operator will only work
if the robot is in the room next to the door , the door is open , and the door connects the room the robot is in to another room . In this case , predicates must
be added and deleted from the world model when the operator executes . 
When the robot is in room rk and goes through the door , it is now in room
rm ( which must be added to the world model ) and is no longer in room rk 
( which must be deleted )  . 
So far , the world model and difference table should seem reasonable , although tedious to construct . But constructing a difference table is pointless
without an evaluation function for differences .  ( Notice that there wasn ‚Äô t a
column for the difference in the above table .  ) The difference evaluator in
the travel example was Euclidean distance . In this example , the evaluator is
predicate calculus , where the initial state is logically subtracted from
the goal state . The logical difference between the initial state goal
state is simply :  : INROOM ( IT , R2 ) or INROOM ( IT , R2 )  = FALSE
50 2 The Hierarchical Paradigm
Reducing differences is a bit like a jigsaw puzzle where Strips tries different substitutions to see if a particular operator will reduce the difference . In
REDUCING order to reduce the difference , Strips looks in the difference table , starting at
DIFFERENCES the top , under the add-list column for a match . It looks in the add-list rather
than a separate differences column because the add-list expresses what the
result of the operator is . If Strips finds an operator that produces the goal
state , then that operator eliminates the existing difference between the initial
and goal states . 
The add-list in OP2 : GOTHRUDOOR has a match on form . If rm = R2 , then
the result of OP2 would be INROOM ( IT , R2 )  . This would eliminate the
difference , so OP2 is a candidate operator . 
Before the OP2 can be applied , Strips must check the preconditions . To do
this , rm must be replaced with R2 in every predicate in the preconditions . 
OP2 has two preconditions , only CONNECTS ( dx , rk , rm ) is affected . It
becomes CONNECTS ( dx , rk , R2 )  . Until dx and rk are bound , the predicate doesn ‚Äô t have a true or false value . Essentially dx , rk are wildcards , 
CONNECTS (  *  ,  *  , R2 )  . To fill in values for these variables , Strips looks at
the current state of the world model to find a match . The predicate in the current state of the world CONNECTS ( D1 , R1 , R2 ) matches CONNECTS (  *  ,  
*  , R2 )  . D1 is now bound to dx and R1 is bound to rk . 
Now Strips propagates the bindings to the next precondition on the list : 
NEXTTO ( IT , dx )  . NEXTTO ( IT , D1 ) is FALSE because the predicate is
FAILED not in the current world state . NEXTTO ( IT , D1 ) is referred to as a failed
PRECONDITIONS precondition . An informal interpretation is that GOTHRUDOOR ( IT , D1 ) will
get the robot to the goal state , but before it can do that , IT has to be next to
D1 . 
RECURSION TO Rather than give up , STRIPS recurses ( uses the programming technique of
RESOLVE DIFFERENCES recursion ) to repeat the entire procedure . It marks the original goal state as
G0 , pushes it on a stack , then it creates a new sub-goal state , G1 . 
The difference between NEXTTO ( IT , D1 ) and the current world state is :  : NEXTTO ( IT , D1 ) 
Strips once again searches through the add-list in the difference table to
find an operator that would negate this . Indeed , OP1 : GOTODOOR ( IT , 
dx ) has a match in the add-list of NEXTTO ( IT , dx )  . Strips has to start over
with reassigning values to the identifiers because the program has entered a
new programming scope , so dx = D1 . 
2 . 2 Attributes of the Hierarchical Paradigm 51
Again , Strips examines the preconditions . This time rk = R1 and rm = R2
can be matched with CONNECTS ( dx , rk , rm )  , and all preconditions are
satisfied ( that is , they evaluate to true )  . Strips puts the operator OP1 on the
plan stack and applies the operator to the world model , changing the state .  
( Note that this is the equivalent of a ‚Äú mental operation ‚Äù  ; the robot doesn ‚Äô t
actually physically go to the door , it just changes the state to imagine what
would happen if it did .  ) 
To recall , the initial state of the world model was : 
initial state : 
INROOM ( IT , R1 ) 
INROOM ( B1 , R2 ) 
CONNECTS ( D1 , R1 , R2 ) 
CONNECTS ( D1 , R2 , R1 ) 
STATUS ( D1 , OPEN ) 
Applying the operator OP1 means making the changes on the add-list and
delete-list . There is only a predicate on the add-list and none on the deletelist . After adding NEXTTO ( IT , D1 )  , the state of the world is : 
state after OP1 : 
INROOM ( IT , R1 ) 
INROOM ( B1 , R2 ) 
CONNECTS ( D1 , R1 , R2 ) 
CONNECTS ( D1 , R2 , R1 ) 
STATUS ( D1 , OPEN ) 
NEXTTO ( IT , D1 ) 
Strips then returns control back to the previous call . It resumes where it
left off in evaluating the preconditions for OP2 with dx = D1 , rm = R2 and
rk = R1 , only now the world model has changed . Both STATUS ( D1 , OPEN ) 
and INROOM ( IT , R1 ) are true , so all the preconditions for OP2 are satisfied . 
Strips puts OP2 on its plan stack and changes the world model by applying
the add-list and delete-list predicates . This results in what the state of the
world will be when the plan executes : 
state after OP2 : 
INROOM ( IT , R2 ) 
INROOM ( B1 , R2 ) 
52 2 The Hierarchical Paradigm
CONNECTS ( D1 , R1 , R2 ) 
CONNECTS ( D1 , R2 , R1 ) 
STATUS ( D1 , OPEN ) 
NEXTTO ( IT , D1 ) 
Strips exits and the plan for the robot to physically execute ( in reverse
order on the stack ) is : GOTODOOR ( IT , D1 )  , GOTHRUDOOR ( IT , D1 )  . 
2 . 2 . 3 Strips summary
Strips works recursively ; if it can ‚Äô t reach a goal directly , it identifies the problem ( a failed precondition )  , then makes the failed precondition a subgoal . 
Once the subgoal is reached , Strips puts the operator for reaching the subgoal on a list , then backs up ( pops the stack ) and resumes trying to reach the
previous goal . Strips plans rather than execute : it creates a list of operators
to apply ; it does not apply the operator as it goes . Strips implementations
requires the designer to set up a :  world model representation  difference table with operators , preconditions , add , and delete lists  difference evaluator
The steps in executing Strips are : 
1 . Compute the difference between the goal state and the initial state using
the difference evaluation function . If there is no difference , terminate . 
2 . If there is a difference , reduce the difference by selecting the first operator
from the difference table whose add-list has a predicate which negates the
difference . 
3 . Next , examine the preconditions to see if a set of bindings for the variables
can be obtained which are all true . If not , take the first false precondition , 
make it the new goal and store the original goal by pushing it on a stack . 
Recursively reduce that difference by repeating step 2 and 3 . 
4 . When all preconditions for an operator match , push the operator onto
the plan stack and update a copy of the world model . Then return to
the operator with the failed precondition so it can apply its operator or
recurse on another failed precondition . 
2 . 3 Closed World Assumption and the Frame Problem 53
2 . 3 Closed World Assumption and the Frame Problem
CLOSED WORLD Strips sensitized the robotics community to two pervasive issues : the closed
ASSUMPTION world assumption and the frame problem . As defined earlier , the closed world
FRAME PROBLEM assumption says that the world model contains everything the robot needs to
know : there can be no surprises . If the closed world assumption is violated , 
the robot may not be able to function correctly . But , on the other hand , it is
very easy to forget to put all the necessary details into the world model . As a
result , the success of the robot depends on how well the human programmer
can think of everything . 
But even assuming that the programmer did come up with all the cases , 
the resulting world model is likely to be huge . Consider how big and cumbersome the world model was just for moving between 2 rooms . And there
were no obstacles ! People began to realize that the number of facts ( or axioms ) that the program would have to sort through for each pass through
the difference table was going to become intractable for any realistic application . The problem of representing a real-world situation in a way that was
computationally tractable became known as the frame problem . The oppoOPEN WORLD site of the closed world assumption is known as the open world assumption . 
ASSUMPTION When roboticists say that ‚Äú a robot must function in the open world ,  ‚Äù they
are saying the closed world assumption cannot be applied to that particular
domain . 
The above example , although trivial , shows how tedious Strips is ( though
computers are good at tedious algorithms )  . In particular , the need to formally represent the world and then maintain every change about it is nonintuitive . It also illustrates the advantage of a closed-world assumption : 
imagine how difficult it would be to modify the planning algorithm if the
world model could suddenly change . The algorithm could get lost between
recursions . The example should also bring home the meaning of the frame
problem : imagine what happens to the size of the world model if a third
room is added with boxes for the robot to move to and pick up ! And this is
only for a world of rooms and boxes . Clearly the axioms which frame the
world will become too numerous for any realistic domain . 
One early solution was ABStrips which tried to divide the problem into
multiple layers of abstraction , i . e .  , solve the problem on a coarse level first . 
That had its drawbacks , and soon many people who had started out in robotics found themselves working on an area of AI called planning . The
two fields became distinct , and by the 1980 ‚Äô s , the planning and robotics researchers had separate conferences and publications . Many roboticists dur-
54 2 The Hierarchical Paradigm
ing the 1970 ‚Äô s and 1980 ‚Äô s worked on either computer vision related issues , 
trying to get the robots to be able to better sense the world , or on path planning , computing the most efficient route around obstacles , etc . to a goal location . 
2 . 4 Representative Architectures
As mentioned in Part I an architecture is a method of implementing a paradigm , of embodying the principles in some concrete way . Ideally , an architecture is generic ; like a good object-oriented program design , it should have
many reusable pieces for other robot platforms and tasks . 
Possibly the two best known architectures of the Hierarchical period are
the Nested Hierarchical Controller ( NHC ) developed by Meystel93 and the
NIST Realtime Control System ( RCS ) originally developed by Albus , 1 with
its teleoperation version for JPL called NASREM . 
2 . 4 . 1 Nested Hierarchical Controller
As shown in Fig . 2 . 5 , the Nested Hierarchical Controller architecture has
components that are easily identified as either SENSE , PLAN , or ACT . The
robot begins by gathering observations from its sensors and combining those
observations to form the World Model data structure through the SENSE
activity . The World Model may also contain a priori knowledge about the
world , for example , maps of a building , rules saying to stay away from the
foyer during the start and finish of business hours , etc . After the World
Model has been created or updated , then the robot can PLAN what actions
it should take . Planning for navigation has a local procedure consisting of
three steps executed by the Mission Planner , Navigator , and Pilot . Each of
these modules has access to the World Model in order to compute their portion of planning . The last step in planning is for the Pilot module to generate
specific actions for the robot to do ( e . g .  , Turn left , turn right , move straight at
a velocity of 0 . 6 meters per second )  . These actions are translated into actuator control signals ( e . g .  , Velocity profile for a smooth turn ) by the Low-Level
Controller . Together , the Low-Level Controller and actuators form the ACT
portion of the architecture . 
The major contribution of NHC was its clever decomposition of planning
into 3 different functions or subsystems aimed at supporting navigation : the
MISSION PLANNER Mission Planner , the Navigator , and the Pilot . As shown in Fig . 2 . 6 , the MisNAVIGATOR
PILOT
sion Planner either receives a mission from a human or generates a mission
2 . 4 Representative Architectures 55
Mission
Planner
Navigator
Pilot
Low-level
Controller
World 
Model / 
Knowledge
Base
sensors sensors sensors drive steer
SENSE PLAN
ACT
Figure 2 . 5 Nested Hierarchical Controller . 
for itself , for example : pick up the box in the next room . The Mission Planner is responsible for operationalizing , or translating , this mission into terms
that other functions can understand : box = B1 ; rm = ROOM311 . The Mission
Planner then accesses a map of the building and locates where the robot is
and where the goal is . The Navigator takes this information and generates a
path from the current location to the goal . It generates a set of waypoints , or
straight lines for the robot to follow . The path is passed to the Pilot . The Pilot
takes the first straight line or path segment and determines what actions the
robot has to do to follow the path segment . For instance , the robot may need
to turn around to face the way point before it can start driving forward . 
What happens if the Pilot gives directions for a long path segment ( say 50
meters ) or if a person suddenly walks in front of the robot ? Unlike Shakey , 
under NHC , the robot is not necessarily walking around with its eyes closed . 
After the Pilot gives the Low-Level Controller commands and the controller
56 2 The Hierarchical Paradigm
Mission
Planner
Navigator
Pilot
goal
path
path subsegment : 
Turn 83 degrees
move 5 meters
5 6
10
Figure 2 . 6 Examination of planning components in the NHC architecture . 
sends actuator signals , the robot polls its sensors again . The World Model
is updated . However , the entire planning cycle does not repeat . Since the
robot has a plan , it doesn ‚Äô t need to rerun the Mission Planner or the Navigator . Instead , the Pilot checks the World Model to see if the robot has drifted
off the path subsegment ( in which case it generates a new control signal )  , 
has reached the waypoint , or if an obstacle has appeared . If the robot has
reached its waypoint , the Pilot informs the Navigator . If the waypoint isn ‚Äô t
the goal , then there is another path subsegment for the robot to follow , and
so the Navigator passes the new subsegment to the Pilot . If the waypoint is
the goal , then the Navigator informs the Mission Planner that the robot has
reached the goal . The Mission Planner may then issue a new goal , e . g .  , Return to the starting place . If the robot has encountered an obstacle to its path , 
the Pilot passes control back to the Navigator . The Navigator must compute
a new path , and subsegments , based on the updated World Model . Then it
gives the updated path subsegment to the Pilot to carry out . 
2 . 4 Representative Architectures 57
NHC has several advantages . It differs from Strips in that it interleaves
planning and acting . The robot comes up with a plan , starts executing it , 
then changes it if the world is different than it expected . Notice that the decomposition is inherently hierarchical in intelligence and scope . The Mission
Planner is ‚Äú smarter ‚Äù than the Navigator , who is smarter than the Pilot . The
Mission Planner is responsible for a higher level of abstraction then the Navigator , etc . We will see that other architectures , both in the Hierarchical and
Hybrid paradigms , will make use of the NHC organization . 
One disadvantage of the NHC decomposition of the planning function is
that it is appropriate only for navigation tasks . The division of responsibilities seems less helpful , or clear , for tasks such as picking up a box , rather than
just moving over to it . The role of a Pilot in controlling end-effectors is not
clear . At the time of its initial development , NHC was never implemented
and tested on a real mobile robot ; hardware costs during the Hierarchical
period forced most roboticists to work in simulation . 
2 . 4 . 2 NIST RCS
Jim Albus at the National Bureau of Standards ( later renamed the National
Institute of Standards and Technology or NIST ) anticipated the need for intelligent industrial manipulators , even as engineering and AI researchers were
splitting into two groups . He saw that one of the major obstacles in applying AI to manufacturing robots was that there were no common terms , no
common set of design standards . This made industry and equipment manufacturers leery of AI , for fear of buying an expensive robot that would not
be compatible with robots purchased in the future . He developed a very detailed architecture called the Real-time Control System ( RCS ) Architecture to
serve as a guide for manufacturers who wanted to add more intelligence to
their robots . RCS used NHC in its design , as shown in Fig . 2 . 7 . 
SENSE activities are grouped into a set of modules under the heading
of sensory perception . The output of the sensors is passed off to the world
modeling module which constructs a global map using information in its
associated knowledge database about the sensors and any domain knowledge ( e . g .  , the robot is operating underwater )  . This organization is similar
to NHC . The main difference is that the sensory perception module introduces a useful preprocessing step between the sensor and the fusion into a
world model . As will be seen in Ch . 6 , sensor preprocessing is often referred
to as feature extraction . 
58 2 The Hierarchical Paradigm
SENSE MODEL ACT
SENSE PLAN ACT Sample Activities
Mission Planner
Navigator
Pilot
a . 
Sensory
Perception
World
Modeling
Behavior
Generation
Knowledge
Database
Value
Judgment
changes
and
events
observed
input
perception , 
focus of
attention
plans , 
state of
actions
commanded
actions
task
goals
simulated
plans
SENSE PLAN
ACT
b . 
Figure 2 . 7 Layout of RCS : a .  ) hierarchical layering of sense-model-act , and b .  ) functional decomposition . 
2 . 4 Representative Architectures 59
The Value Judgment module provides most of the functionality associated
with the PLAN activity : it plans , then simulates the plans to ensure they will
work . Then , as with Shakey , the Planner hands off the plan to another module , Behavior Generation , which converts the plans into actions that the robot
can actually perform ( ACT )  . Notice that the Behavior Generation module is
similar to the Pilot in NHC , but there appears to be less focus on navigation
tasks . The term ‚Äú behavior ‚Äù will be used by Reactive and Hybrid Deliberative / Reactive architectures .  ( This use of ‚Äú behavior ‚Äù in RCS is a bit of retrofit , 
as Albus and his colleagues at NIST have attempted to incorporate new advances . The integration of all sensing into a global world model for planning
and acting keeps RCS a Hierarchical architecture .  ) There is another module , 
operator interface , which is not shown which allows a human to ‚Äú observe ‚Äù 
and debug what a program constructed with the architecture is doing . 
The standard was adapted by many government agencies , such as NASA
and the US Bureau of Mines , who were contracting with universities and
companies to build robot prototypes . RCS serves as a blueprint for saying :  
 ‚Äú here ‚Äô s the types of sensors I want , and they ‚Äô ll be fused by this module into a
global map , etc .  ‚Äù The architecture was considered too detailed and restrictive
when it was initially developed by most AI researchers , who continued development of new architectures and paradigms on their own . Fig . 2 . 8 shows
three of the diverse mobile robots that have used RCS . 
A close inspection of the NHC and RCS architectures suggests that they
are well suited for semi-autonomous control . The human operator could
provide the world model ( via eyes and brain )  , decide the mission , decompose it into a plan , and then into actions . The lower level controller ( robot ) 
would carry out the actions . As robotics advanced , the robot could replace
more functions and ‚Äú move up ‚Äù the autonomy hierarchy . For example , taking over the pilot ‚Äô s responsibilities ; the human could instruct the robot to
stay on the road until the first left turn . As AI advanced , the human would
only have to serve as the Mission Planner :  ‚Äú go to the White House .  ‚Äù And so
on . Albus noted this and worked with JPL to develop a version of RCS for
NASREM teleoperating a robot arm in space . This is called the NASREM architecture
and is still in use today . 
2 . 4 . 3 Evaluation of hierarchical architectures
Recall from Part I that there are four criteria for evaluating an architecture : 
support for modularity , niche targetability , ease of portability to other domains , and robustness . NHC and RCS both provide some guidelines in how
60 2 The Hierarchical Paradigm
a . b . 
c . 
Figure 2 . 8 Three of the diverse mobile robots that have used RCS : a .  ) a commercial
floor cleaning robot , b .  ) a mining robot , and c .  ) a submersible or underwater robot .  
( Photographs courtesy of the National Institute of Standards and Technology .  ) 
to decompose a robot program into intuitive modules . The NHC decomposition of mission planner , navigator , and pilot was focused strictly on navigation , while RCS appears broader . Both have been used recently for successful
vehicle guidance , with RCS being used to control mining equipment , submarines , and cars . So both have reasonable niche targetability . The ease of
portability to other domains is unclear . The architectures are expressed at a
very broad level , akin to ‚Äú a house should have bedrooms , bathrooms , and a
kitchen .  ‚Äù Architectures which are more specific ,  ‚Äú there should be one bathroom for every two bedrooms ,  ‚Äù and which have associated techniques simplify portability . It is hard to see how the code written for a mining machine
2 . 5 Advantages and Disadvantages 61
could be reused for a submarine , especially since RCS is not object-oriented . 
In terms of robustness , RCS does attempt to provide some explicit mechanisms . In particular , it assumes the Value Judgment module simulates a
plan to confirm that it should be successful when deployed . The use of simulation is common for operating equipment in a well-known environment
where every piece of equipment is known . The most notable example is a
nuclear processing cell . With such detailed information , it is fairly straightforward ( although computationally expensive ) to simulate whether a particular course for a robot would collide with equipment and cause a spill . 
This is a very limited form of robustness . The disadvantage is the time delay
caused by the robot mentally rehearsing its actions prior to executing them . 
Simulation may not be appropriate for all actions ; if a piece of the ceiling is
falling on the robot , it needs to get out of the way immediately or risk coming
up with the best place to move too late to avoid being crushed . 
2 . 5 Advantages and Disadvantages
Robots built in the time period before 1990 typically had a Hierarchical style
of software organization . They were generally developed for a specific application rather than to serve as a generic architecture for future applications . 
The robots are interesting because they illustrate the diversity and scope of
applications being considered for mobile robots as far back as 15 or 20 years
ago . 
The primary advantage of the Hierarchical Paradigm was that it provides
an ordering of the relationship between sensing , planning , and acting . The
primary disadvantage was planning . Every update cycle , the robot had to
update a global world model and then do some type of planning . The sensing and planning algorithms of the day were extremely slow ( and many still
are )  , so this introduced a significant bottleneck . Notice also that sensing and
acting are always disconnected . This effectively eliminated any stimulusresponse types of actions (  ‚Äú a rock is crashing down on me , I should move
anywhere ‚Äù  ) that are seen in nature . 
The dependence on a global world model is related to the frame problem . 
In Strips , in order to do something as simple as opening a door , the robot had
to reason over all sorts of details that were irrelevant ( like other rooms , other
doors )  . NHC and RCS represent attempts to divide up the world model into
pieces best suited for the type of actions ; for example , consider the roles of
the Mission Planner , Navigator , and Pilot . Unfortunately , these decomposi-
62 2 The Hierarchical Paradigm
tions appear to be dependent on a particular application . As a result , robotics
gained a reputation as being more of an art than a science . 
Another issue that was never really handled by architectures in the Hierarchical Paradigm was uncertainty . Uncertainty comes in many forms , such
as semantic ( how close does NEXTTO mean anyway ?  )  , sensor noise , and actuator errors . Another important aspect of uncertainty is action completion : 
did the robot actually accomplish the action ? One robotics researcher said
that their manipulator was only able to pick up a cup 60% of the attempts ; 
therefore they had to write a program to check to see if it was holding a cup
and then restart the action if it wasn ‚Äô t . Because Shakey essentially closed its
eyes during planning and acting , it was vulnerable to uncertainty in action
completion . 
2 . 6 Programming Considerations
It is interesting to note that the use of predicate logic and recursion by Strips
favors languages like Lisp and PROLOG . These languages were developed
by AI researchers specifically for expressing logical operations . These languages do not necessarily have good real-time control properties like C or
C +  +  . However , during the 1960 ‚Äô s the dominant scientific and engineering
language was FORTRAN IV which did not support recursion . Therefore , researchers in AI robotics often chose the lesser of two evils and programmed
in Lisp . The use of special AI languages for robotics may have aided the split
between the engineering and AI approaches to robotics , as well as slowed
down the infusion of ideas from the the two communities . It certainly discouraged non-AI researchers from becoming involved in AI robotics . 
The Hierarchical Paradigm tends to encourage monolithic programming , 
rather than object-oriented styles . Although the NHC decomposes the planning portion of intelligence , the decomposition is strictly functional . In particular , NHC and RCS don ‚Äô t provide much guidance on how to build modular , reusable components . 
2 . 7 Summary
The Hierarchical Paradigm uses a SENSE then PLAN then ACT ( S , P , A )  . It
organizes sensing into a global data structure usually called a world model
that may have an associated knowledge base to contain a priori maps or
knowledge relevant to a task . Global data structures often flag that an ar-
2 . 8 Exercises 63
chitecture will suffer from the frame problem . The Hierarchical Paradigm
was introduced in the first mobile robot , Shakey . Strips is an important planning technique that came out of the Shakey project at SRI , which focused
on the PLAN primitive in robotics . Concepts and terms which emerged that
continue to play an important role in defining robotics are : preconditions , 
the closed and open world assumptions , and the frame problem . Hierarchical systems have largely fallen out of favor except for the NIST Realtime
Control Architecture . The decline in popularity is due in part to its focus on
strong niche targetability at the expense of true modularity and portability . 
However , as will be seen in the following chapters , insights from biology and
cognitive science have led to paradigms with more intuitive appeal . One often overlooked property of most hierarchical architectures is that they tend to
support the evolution of intelligence from semi-autonomous control to fully
autonomous control . 
2 . 8 Exercises
Exercise 2 . 1
Describe the Hierarchical Paradigm in terms of a ) the SENSE , PLAN , ACT primitives
and b ) sensing organization . 
Exercise 2 . 2
Define the following terms in one or two sentences . Give an example of how each
arises in a robotic system : 
a . precondition
b . closed world assumption
c . open world
d . frame problem
Exercise 2 . 3
Consider the frame problem . Suppose the World Model for a Strips-based robot consisted of 100 facts . Each fact requires 1KB of memory storage . Every time a new object
is added to the world model , it increases the model by 100 ( a linear increase )  . One
object , 100 facts , 100KB of storage ; two objects , 200 facts , 200KB . How many objects
would fill 64KB of memory ? 
Exercise 2 . 4
Redo the above exercise where the number of facts in the world model doubles every
time a new object is added ( exponential )  . One object , 100 facts , 1KB , two objects , 
200 facts , 200KB , three objects , 400 facts , 400KB . Which is a more reasonable rate to
assume the world model would increase at , linear or exponential ? 
64 2 The Hierarchical Paradigm
Exercise 2 . 5
Describe the mission planner , navigator , pilot organization of the Nested Hierarchical
Controller . Write down how it would handle the problem in Sec . 2 . 2 . 2 . 
Exercise 2 . 6
List 2 advantages and disadvantages of the Hierarchical Paradigm . 
Exercise 2 . 7
Solve the following navigation problem using Strips . Return to the world in Sec 2 . 2 . 2 . 
The robot will move to the box B1 and pick it up . 
a . Add a new operator pickup to the difference table . 
b . Use the world model , difference table , difference evaluator to construct a plan . 
Failed preconditions and new subgoals should be shown after each step . 
c . Show the changes in the world model after each operator is applied . 
Exercise 2 . 8
Name and evaluate one representative Hierarchical architecture in terms of : support
for modularity , niche targetability , ease of portability to other domains , robustness . 
Exercise 2 . 9 [ World Wide Web ] 
Search the web for interactive versions of Strips and experiment with them . 
2 . 9 End Notes
A robot that did take a cross-country trip . 
Robot vehicles do in fact need special authorization to drive on public roads . In
1996 , the Carnegie Mellon University Navlab vehicle project led by Dean Pomerleau
steered itself ( the driver handled the gas pedal and brakes ) over 90% of the way across
the USA from Washington , DC , to Los Angeles in the ‚Äú No Hands Across America ‚Äù 
trip . The Navlab ( a modified Saturn station wagon ) was reportedly pulled over by
the Kansas State Highway Patrol for driving an experimental vehicle without permission . The entire trip was placed in jeopardy , but eventually the Navlab was allowed
to continue , and the team and vehicle appeared on the David Letterman show in
Los Angeles . As impressive as the Carnegie Mellon feat was , a group of German
researchers under the direction of Ernst Dickmanns and Volker Graefe have been
fielding even more advanced autonomous highway driving vehicles since 1988 . 
Shakey . 
It can be debated whether Shakey is really the first mobile robot . There was a tortoise
built by Grey Walter , but this was never really on the main branch of AI research . See
Behavior-Based Robots 10 for details . 
2 . 9 End Notes 65
Robot name trivia . 
Regardless of how Shakey got its name , SRI continued the tradition with Shakey ‚Äô s
successor being called Flakey , followed by Flakey ‚Äô s successor , Erratic . 
Strips . 
The description of Strips and the robot examples are adapted from The Handbook of
Artificial Intelligence , A . Barr and E . Feigenbaum , editors , vol . 1 , William Kaufmann , 
Inc .  , Los Altos , CA , 1981 . 
Jim Albus . 
Jim Albus is one of the statesmen of robotics . Although his RCS architecture is a representative of the Hierarchical Paradigm , Albus was heavily by cognitive studies‚Äîso
much so that he wrote Brains , Behaviors and Robots ( 1981 )  . What ‚Äô s odd is that exploiting biological and cognitive studies is commonly associated with the Reactive
Paradigm . The RCS architecture was never meant to be totally static , and in recent
years , it has begun to resemble what will be referred to as a model-oriented style of
the Hybrid Paradigm . 

3 Biological Foundations of the
Reactive Paradigm
Chapter Objectives :  Describe the three levels in a computational theory .  Explain in one or two sentences each of the following terms : reflexes , taxes , 
fixed-action patterns , schema , affordance .  Be able to write pseudo-code of an animal ‚Äô s behaviors in terms of innate
releasing mechanisms , identifying the releasers for the behavior .  Given a description of an animal ‚Äô s sensing abilities , its task , and environment , identify an affordance for each behavior .  Given a description of an animal ‚Äô s sensing abilities , its task , and environment , define a set of behaviors using schema theory to accomplish the
task . 
3 . 1 Overview
Progress in robotics in the 1970 ‚Äô s was slow . The most influential robot was
the Stanford Cart developed by Hans Moravec , which used stereo vision to
see and avoid obstacles protruding from the ground . In the late 1970 ‚Äô s and
early 80 ‚Äô s , Michael Arbib began to investigate models of animal intelligence
from the biological and cognitive sciences in the hopes of gaining insight into
what was missing in robotics . While many roboticists had been fascinated
by animals , and many , including Moravec , had used some artifact of animal
behavior for motivation , no one approached the field with the seriousness
and dedication of Arbib . 
68 3 Biological Foundations of the Reactive Paradigm
At nearly the same time , a slender volume by Valentino Braitenberg , called
Vehicles : Experiments in Synthetic Psychology , 
25 appeared . It was a series of
gedanken or entirely hypothetical thought experiments , speculating as to
how machine intelligence could evolve . Braitenberg started with simple
thermal sensor-motor actuator pair ( Vehicle 1 ) that could move faster in warm
areas and slower in cold areas . The next , more complex vehicle had two thermal sensor-motor pairs , one on each side of the vehicle . As a result of the
differential drive effect , Vehicle 2 could turn around to go back to cold areas . 
Throughout the book , each vehicle added more complexity . This layering
was intuitive and seemed to mimic the principles of evolution in primates . 
Vehicles became a cult tract among roboticists , especially in Europe . 
Soon a new generation of AI researchers answered the siren ‚Äô s call of biological intelligence . They began exploring the biological sciences in search
of new organizing principles and methods of obtaining intelligence . As will
be seen in the next chapter , this would lead to the Reactive Paradigm . This
chapter attempts to set the stage for the Reactive Paradigm by recapping influential studies and discoveries , and attempting to cast them in light of how
they can contribute to robotic intelligence . 
The chapter first covers animal behaviors as the fundamental primitive
for sensing and acting . Next , it covers the work of Lorenz and Tinbergen in
defining how concurrent simple animal behaviors interact to produce complex behaviors through Innate Releasing Mechanisms ( IRMs )  . A key aspect
of an animal behavior is that perception is needed to support the behavior . The previous chapter on the Hierarchical Paradigm showed how early
roboticists attempted to fuse all sensing into a global world map , supplemented with a knowledge base . This chapter covers how the work of cognitive psychologists Ulrich Neisser 109 and J . J . Gibson59 provides a foundation
for thinking about robotic perception . Gibson refuted the necessity of global
world models , a direct contradiction to the way perception was handled in
the hierarchical paradigm . Gibson ‚Äô s use of affordances , also called direct perception , is an important key to the success of the Reactive Paradigm . Later
work by Neisser attempts to define when global models are appropriate and
when an affordance is more elegant . 
ETHOLOGY Many readers find the coverage on ethology ( study of animal behavior ) and
COGNITIVE cognitive psychology ( study of how humans think and represent knowledge ) 
PSYCHOLOGY to be interesting , but too remote from robotics . In order to address this concern , the chapter discusses specific principles and how they can be applied
to robotics . It also raises issues in transferring animal models of behavior
to robots . Finally , the chapter covers schema theory , an attempt in cognitive
3 . 1 Overview 69
psychology to formalize aspects of behavior . Schema theory has been used
successfully by Arbib to represent both animal and robot behavior . It is implicitly object-oriented and so will serve as the foundation of discussions
through out the remainder of this book . 
3 . 1 . 1 Why explore the biological sciences ? 
Why should roboticists explore biology , ethology , cognitive psychology and
other biological sciences ? There is a tendency for people to argue against
considering biological intelligence with the analogy that airplanes don ‚Äô t flap
their wings . The counter-argument to that statement is that almost everything else about a plane ‚Äô s aerodynamics duplicates a bird ‚Äô s wing : almost all
the movable surfaces on the wing of a plane perform the same functions as
parts of a bird ‚Äô s wing . The advances in aeronautics came as the Wright Brothers and others extracted aerodynamic principles . Once the principles of flight
were established , mechanical systems could be designed which adhered to
these principles and performed the same functions but not necessarily in the
same way as biological systems . The ‚Äú planes don ‚Äô t flap their wings ‚Äù argument turns out to be even less convincing for computer systems : animals
make use of innate capabilities , robots rely on compiled programs . 
Many AI roboticists often turn to the biological sciences for a variety of
reasons . Animals and man provide existence proofs of different aspects of
intelligence . It often helps a researcher to know that an animal can do a
particular task , even if it isn ‚Äô t known how the animal does it , because the
researcher at least knows it is possible . For example , the issue of how to
combine information from multiple sensors ( sensor fusion ) has been an open
question for years . At one point , papers were being published that robots
shouldn ‚Äô t even try to do sensor fusion , on the grounds that sensor fusion was
a phenomenon that sounded reasonable but had no basis in fact . Additional
research showed that animals ( including man ) do perform sensor fusion , although with surprisingly different mechanisms than most researchers had
considered . 
The principles of animal intelligence are extremely important to robotiOPEN WORLD cists . Animals live in an open world , and roboticists would like to overcome
ASSUMPTION the closed world assumption that presented so many problems with Shakey . CLOSED WORLD
ASSUMPTION Many ‚Äú simple ‚Äù animals such as insects , fish , and frogs exhibit intelligent behavior yet have virtually no brain . Therefore , they must be doing something
FRAME PROBLEM that avoids the frame problem . 
70 3 Biological Foundations of the Reactive Paradigm
3 . 1 . 2 Agency and computational theory
Even though it seems reasonable to explore biological and cognitive sciences
for insights in intelligence , how can we compare such different systems : carbon and silicon ‚Äú life ‚Äù forms ? One powerful means of conceptualizing the
different systems is to think of an abstract intelligent system . Consider someAGENT thing we ‚Äô ll call an agent . The agent is self-contained and independent . It has
its own ‚Äú brains ‚Äù and can interact with the world to make changes or to sense
what is happening . It has self-awareness . Under this definition , a person is
an agent . Likewise , a dog or a cat or a frog is an agent . More importantly , 
an intelligent robot would be an agent , even certain kinds of web search engines which continue to look for new items of interest to appear , even after
the user has logged off . Agency is a concept in artificial intelligence that allows researchers to discuss the properties of intelligence without discussing
the details of how the intelligence got in the particular agent . In OOP terms ,  
 ‚Äú agent ‚Äù is the superclass and the classes of ‚Äú person ‚Äù and ‚Äú robot ‚Äù are derived
from it . 
Of course , just referring to animals , robots , and intelligent software packages as ‚Äú agents ‚Äù doesn ‚Äô t make the correspondences between intelligence any
clearer . One helpful way of seeing correspondences is to decide the level at
which these entities have something in common . The set of levels of commonality lead to what is often called a computational theory88 COMPUTATIONAL after David
THEORY Marr . Marr was a neurophysiologist who tried to recast biological vision
processes into new techniques for computer vision . The levels in a computational theory can be greatly simplified as : 
Level 1 : Existence proof of what can / should be done . Suppose a roboticist
is interested in building a robot to search for survivors trapped in a building after an earthquake . The roboticist might consider animals which seek
out humans . As anyone who has been camping knows , mosquitoes are
very good at finding people . Mosquitoes provide an existence proof that
it is possible for a computationally simple agent to find a human being
using heat . At Level 1 , agents can share a commonality of purpose or
functionality . 
Level 2 : Decomposition of ‚Äú what ‚Äù into inputs , outputs , and transformations . This level can be thought of as creating a flow chart of ‚Äú black
boxes .  ‚Äù Each box represents a transformation of an input into an output . 
Returning to the example of a mosquito , the roboticist might realize from
biology that the mosquito finds humans by homing on the heat of a hu-
3 . 1 Overview 71
man ( or any warm blooded animal )  . If the mosquito senses a hot area , it
flies toward it . The roboticist can model this process as : input = thermal
image , output = steering command . The ‚Äú black box ‚Äù is how the mosquito transforms the input into the output . One good guess might be
to take the centroid of the thermal image ( the centroid weighted by the
heat in each area of the image ) and steer to that . If the hot patch moves , 
the thermal image will change with the next sensory update , and a new
steering command will be generated . This might not be exactly how the
mosquito actually steers , but it presents an idea of how a robot could
duplicate the functionality . Also notice that by focusing on the process
rather than the implementation , a roboticist doesn ‚Äô t have to worry about
mosquitoes flying , while a search and rescue robot might have wheels . At
Level 2 , agents can exhibit common processes . 
Level 3 : How to implement the process . This level of the computational theory focuses on describing how each transformation , or black box , is implemented . For example , in a mosquito , the steering commands might be implemented with a special type of neural network , while in a robot , it might
be implemented with an algorithm which computes the angle between the
centroid of heat and where the robot is currently pointing . Likewise , a researcher interested in thermal sensing might examine the mosquito to see
how it is able to detect temperature differences in such a small package ; 
electro-mechanical thermal sensors weigh close to a pound ! At Level 3 , 
agents may have little or no commonality in their implementation . 
It should be clear that Levels 1 and 2 are abstract enough to apply to any
agent . It is only at Level 3 that the differences between a robotic agent and
a biological agent really emerge . Some roboticists actively attempt to emulate biology , reproducing the physiology and neural mechanisms .  ( Most
roboticists are familiar with biology and ethology , but don ‚Äô t try to make exact
duplicates of nature .  ) Fig . 3 . 1 shows work at Case Western Reserve ‚Äô s Bio-Bot
Laboratory under the direction of Roger Quinn , reproducing a cockroach on
a neural level . 
In general , it may not be possible , or even desirable , to duplicate how a
biological agent does something . Most roboticists do not strive to precisely
replicate animal intelligence , even though many build creatures which resemble animals , as seen by the insect-like Genghis in Fig . 3 . 2 . But by focusing on Level 2 of a computational theory of intelligence , roboticists can gain
insights into how to organize intelligence . 
72 3 Biological Foundations of the Reactive Paradigm
a . b . 
Figure 3 . 1 Robots built at the Bio-Bot Laboratory at Case Western Reserve University which imitate cockroaches at Level 3 : a .  ) Robot I , an earlier version , and b .  ) Robot
III .  ( Photographs courtesy of Roger Quinn .  ) 
Figure 3 . 2 Genghis , a legged robot built by Colin Angle , IS Robotics , which imitates
an insect at Levels 1 and 2 .  ( Photograph courtesy of the National Aeronautics and
Space Administration .  ) 
3 . 2 What Are Animal Behaviors ? 73
SENSOR BEHAVIOR
INPUT
PATTERN
OF MOTOR
ACTION
Figure 3 . 3 Graphical definition of a behavior . 
3 . 2 What Are Animal Behaviors ? 
Scientists believe the fundamental building block of natural intelligence is
BEHAVIOR a behavior . A behavior is a mapping of sensory inputs to a pattern of motor actions which then are used to achieve a task . For example , if a horse
sees a predator , it flattens its ears , lowers its head , and paws the ground . In
this case , the sensory input of a predator triggers a recognizable pattern of a
defense behavior . The defensive motions make up a pattern because the actions and sequence is always the same , regardless of details which vary each
episode ( e . g .  , how many times the horse paws the ground )  . See Fig . 3 . 3 . 
Scientists who study animal behaviors are called ethologists . They often
spend years in the field studying a species to identify its behaviors . Often
the pattern of motor actions is easy to ascertain ; the challenging part is to
discover the sensory inputs for the behavior and why the behavior furthers
the species survival . 
Behaviors can be divided into three broad categories . 10 REFLEXIVE BEHAVIOR Reflexive behaviors
STIMULUS-RESPONSE are stimulus-response ( S-R )  , such as when your knee is tapped , it jerks upward . Essentially , reflexive behaviors are ‚Äú hardwired ‚Äù  ; neural circuits ensure
that the stimulus is directly connected to the response in order to produce the
REACTIVE BEHAVIOR fastest response time . Reactive behaviors are learned , and then consolidated to
where they can be executed without conscious thought . Any behavior that
involves what is referred to in sports as ‚Äú muscle memory ‚Äù is usually a reactive behavior ( e . g .  , riding a bike , skiing )  . Reactive behaviors can also be
changed by conscious thought ; a bicyclist riding over a very narrow bridge
CONSCIOUS BEHAVIOR might ‚Äú pay attention ‚Äù to all the movements . Conscious behaviors are deliberative ( assembling a robot kit , stringing together previously developed behaviors , etc .  )  . 
The categorization is worthy of note for several reasons . First , the Reactive
Paradigm will make extensive use of reflexive behaviors , to the point that
some architectures only call a robot behavior a behavior if it is S-R . Second , 
the categorization can help a designer determine what type of behavior to
use , leading to insights about the appropriate implementation . Third , the
74 3 Biological Foundations of the Reactive Paradigm
use of the word ‚Äú reactive ‚Äù in ethology is at odds with the way the word is
used in robotics . In ethology , reactive behavior means learned behaviors or a
skill ; in robotics , it connotes a reflexive behavior . If the reader is unaware of
these differences , it may be hard to read either the ethological or AI literature
without being confused . 
3 . 2 . 1 Reflexive behaviors
Reflexive types of behaviors are particularly interesting , since they imply no
need for any type of cognition : if you sense it , you do it . For a robot , this
would be a hardwired response , eliminating computation and guaranteed to
be fast . Indeed , many kit or hobby robots work off of reflexes , represented
by circuits . 
Reflexive behaviors can be further divided into three categories : 10
REFLEXES 1 . reflexes : where the response lasts only as long as the stimulus , and the
response is proportional to the intensity of the stimulus . 
TAXES 2 . taxes : where the response is to move to a particular orientation . Baby turtles exhibit tropotaxis ; they are hatched at night and move to the brightest
light . Until recently the brightest light would be the ocean reflecting the
moon , but the intrusion of man has changed that . Owners of beach front
property in Florida now have to turn off their outdoor lights during hatching season to avoid the lights being a source for tropotaxis . Baby turtles
hatch at night , hidden from shore birds who normally eat them . It had
been a mystery as to how baby turtles knew which way was the ocean
when they hatched . The story goes that a volunteer left a flashlight on the
sand while setting up an experiment intended to show that the baby turtles used magnetic fields to orient themselves . The magnetic field theory
was abandoned after the volunteers noticed the baby turtles heading for
the flashlight ! Ants exhibit a particular taxis known as chemotaxis ; they
follow trails of pheromones . 
FIXED-ACTION 3 . fixed-action patterns : where the response continues for a longer duration
PATTERNS than the stimulus . This is helpful for fleeing predators . It is important to
keep in mind that a taxis can be any orientation relative to a stimulus , not
just moving towards . 
The above categories are not mutually exclusive . For example , an animal
going over rocks or through a forest with trees to block its view might persist
3 . 3 Coordination and Control of Behaviors 75 
( fixed-action patterns ) in orienting itself to the last sensed location of a food
source ( taxis ) when it loses sight of it . 
The tight coupling of action and perception can often be quantified by
mathematical expressions . An example of this is orienting in angelfish . In
IDIOTHETIC order to swim upright , an angelfish uses an internal ( idiothetic ) sense of gravALLOTHETIC ity combined with its vision sense ( allothetic ) to see the external percept of
the horizon line of the water to swim upright . If the fish is put in a tank with
prisms that make the horizon line appear at an angle , the angelfish will swim
cockeyed . On closer inspection , the angle that the angelfish swims at is the
vector sum of the vector parallel to gravity with the vector perpendicular to
the perceived horizon line ! The ability to quantify animal behavior suggests
that computer programs can be written which do likewise . 
3 . 3 Coordination and Control of Behaviors
KONRAD LORENZ Konrad Lorenz and Niko Tinbergen were the founding fathers of ethology . 
NIKO TINBERGEN Each man independently became fascinated not only with individual behaviors of animals , but how animals acquired behaviors and selected or coordinated sets of behaviors . Their work provides some insight into four different
ways an animal might acquire and organize behaviors . Lorenz and Tinbergen ‚Äô s work also helps with a computational theory Level 2 understanding of
how to make a process out of behaviors . 
The four ways to acquire a behavior are : 
INNATE 1 . to be born with a behavior ( innate )  . An example is the feeding behavior in
baby arctic terns . Arctic terns , as the name implies , live in the Arctic where
the terrain is largely shades of black and white . However , the Arctic tern
has a bright reddish beak . When babies are hatched and are hungry , they
peck at the beak of their parents . The pecking triggers a regurgitation
reflex in the parent , who literally coughs up food for the babies to eat . It
turns out that the babies do not recognize their parents , per se . Instead , 
they are born with a behavior that says : if hungry , peck at the largest red
blob you see . Notice that the only red blobs in the field of vision should
be the beaks of adult Arctic terns . The largest blob should be the nearest
parent ( the closer objects are , the bigger they appear )  . This is a simple , 
effective , and computationally inexpensive strategy . 
SEQUENCE OF INNATE 2 . to be born with a sequence of innate behaviors . The animal is born with a
BEHAVIORS sequence of behaviors . An example is the mating cycle in digger wasps . 
76 3 Biological Foundations of the Reactive Paradigm
A female digger wasp mates with a male , then builds a nest . Once it sees
the nest , the female lays eggs . The sequence is logical , but the important
point is the role of stimulus in triggering the next step . The nest isn ‚Äô t built
until the female mates ; that is a change in internal state . The eggs aren ‚Äô t
laid until the nest is built ; the nest is a visual stimulus releasing the next
step . Notice that the wasp doesn ‚Äô t have to ‚Äú know ‚Äù or understand the
sequence . Each step is triggered by the combination of internal state and
the environment . This is very similar to Finite State Machines in computer
science programming , and will be discussed later in Ch . 5 . 
INNATE WITH MEMORY 3 . to be born with behaviors that need some initialization ( innate with memory )  . An animal can be born with innate behaviors that need customizing
based on the situation the animal is born in . An example of this is bees . 
Bees are born in hives . The location of a hive is something that isn ‚Äô t innate ; a baby bee has to learn what its hive looks like and how to navigate
to and from it . It is believed that the curious behavior exhibited by baby
bees ( which is innate ) allows them to learn this critical information . A
new bee will fly out of the hive for a short distance , then turn around and
come back . This will get repeated , with the bee going a bit farther along
the straight line each time . After a time , the bee will repeat the behavior
but at an angle from the opening to the hive . Eventually , the bee will have
circumnavigated the hive . Why ? Well , the conjecture is that the bee is
learning what the hive looks like from all possible approach angles . Furthermore , the bee can associate a view of the hive with a motor command 
(  ‚Äú fly left and down ‚Äù  ) to get the bee to the opening . The behavior of zooming around the hive is innate ; what is learned about the appearance of the
hive and where the opening is requires memory . 
LEARN 4 . to learn a set of behaviors . Behaviors are not necessarily innate . In mammals and especially primates , babies must spend a great deal of time
learning . An example of learned behaviors is hunting in lions . Lion
cubs are not born with any hunting behaviors . If they are not taught
by their mothers over a period of years , they show no ability to fend
for themselves . At first it might seem strange that something as fundamental as hunting for food would be learned , not innate . However , consider the complexity of hunting for food . Hunting is composed of many
sub-behaviors , such as searching for food , stalking , chasing , and so on . 
Hunting may also require teamwork with other members of the pride . It
requires great sensitivity to the type of the animal being hunted and the
terrain . Imagine trying to write a program to cover all the possibilities ! 
3 . 3 Coordination and Control of Behaviors 77
While the learned behaviors are very complex , they can still be represented by innate releasing mechanisms . It is just that the releasers and
actions are learned ; the animal creates the program itself . 
Note that the number of categories suggests that a roboticist will have a spectrum of choices as to how a robot can acquire one or more behaviors : from
being pre-programmed with behaviors ( innate ) to somehow learning them 
( learned )  . It also suggests that behaviors can be innate but require memory . The lesson here is that while S-R types of behaviors are simple to preprogram or hardwire , robot designers certainly shouldn ‚Äô t exclude the use
of memory . But as will be seen in Chapter 4 , this is a common constraint
placed on many robot systems . This is especially true in a popular style of
hobby robot building called BEAM robotics ( biology , electronics , aesthetics , 
and mechanics )  , espoused by Mark Tilden . Numerous BEAM robot web sites
guide adherents through construction of circuits which duplicate memoryless innate reflexes and taxes . 
An important lesson that can be extracted from Lorenz and Tinbergen ‚Äô s
INTERNAL STATE work is that the internal state and / or motivation of an agent may play a role
MOTIVATION in releasing a behavior . Being hungry is a stimulus , equivalent to the pain
introduced by a sharp object in the robot ‚Äô s environment . Another way of
looking at it is that motivation serves as a stimulus for behavior . Motivations can stem from survival conditions ( like being hungry ) or more abstract
goals ( e . g .  , need to check the mail )  . One of the most exciting insights is that
behaviors can be sequenced to create complex behaviors . Something as complicated as mating and building a nest can be decomposed into primitives
or certainly more simple behaviors . This has an appeal to the software engineering side of robotics . 
3 . 3 . 1 Innate releasing mechanisms
Lorenz and Tinbergen attempted to clarify their work in how behaviors are
INNATE RELEASING coordinated and controlled by giving it a special name innate releasing mechMECHANISMS anisms ( IRM )  . An IRM presupposes that there is a specific stimulus ( either
internal or external ) which releases , or triggers , the stereotypical pattern of
RELEASER action . The IRM activates the behavior . A releaser is a latch or a Boolean variable that has to be set . One way to think of IRMs is as a process of behaviors . 
In a computational theory of intelligence using IRMs , the basic black boxes
of the process would be behaviors . Recall that behaviors take sensory input
and produce motor actions . But IRMs go further and specify when a behav-
78 3 Biological Foundations of the Reactive Paradigm
BEHAVIOR
Releaser
Pattern of 
Motor Actions
Sensory
Input
Figure 3 . 4 Innate Releasing Mechanism as a process with behaviors . 
ior gets turned on and off . The releaser acts as a control signal to activate a
behavior . If a behavior is not released , it does not respond to sensory inputs
and does not produce motor outputs . For example , if a baby arctic tern isn ‚Äô t
hungry , it doesn ‚Äô t peck at red , even if there is a red beak nearby . 
Another way to think of IRMs is as a simple computer program . Imagine
the agent running a C program with a continuous while loop . Each execution through the loop would cause the agent to move for one second , then
the loop would repeat . 
enum Releaser =  { PRESENT , NOT_PRESENT }  ; 
Releaser predator ; 
while ( TRUE )  
{ 
predator = sensePredators (  )  ; 
if ( predator =  = PRESENT ) 
flee (  )  ;  
} 
In this example , the agent does only two things : sense the world and then
flees if it senses a predator . Only one behavior is possible : flee . flee is
released by the presence of a predator . A predator is of type Releaser and
has only two possible values : it is either present or it is not . If the agent does
not sense the releaser for the behavior , the agent does nothing . There is no 
 ‚Äú default ‚Äù behavior . 
This example also shows filtering of perception . In the above example , the
agent only looks for predators with a dedicated detection function , sensePredators (  )  . The dedicated predator detection function could be a specialized sense ( e . g .  , retina is sensitive to the frequency of motions associated
3 . 3 Coordination and Control of Behaviors 79
with predator movement ) or a group of neurons which do the equivalent of
a computer algorithm . 
Another important point about IRMs is that the releaser can be a compound
COMPOUND RELEASERS of releasers . Furthermore , the releaser can be a combination of either external 
( from the environment ) or internal ( motivation )  . If the releaser in the compound isn ‚Äô t satisfied , the behavior isn ‚Äô t triggered . The pseudo-code below
shows a compound releaser . 
enum Releaser =  { PRESENT , NOT_PRESENT }  ; 
Releaser food ; 
while ( TRUE )  
{ 
food = senseFood (  )  ; 
hungry = checkState (  )  ; 
if ( food =  = PRESENT &  & hungry =  = PRESENT ) 
feed (  )  ;  
} 
The next example below shows what happens in a sequence of behaviors , 
where the agent eats , then nurses its young , then sleeps , and repeats the
IMPLICIT CHAINING sequence . The behaviors are implicitly chained together by their releasers . 
Once the initial releaser is encountered , the first behavior occurs . It executes
for one second ( one ‚Äú movement ‚Äù interval )  , then control passes to the next
statement . If the behavior isn ‚Äô t finished , the releasers remain unchanged and
no other behavior is triggered . The program then loops to the top and the
original behavior executes again . When the original behavior has completed , 
the internal state of the animal may have changed or the state of the environment may have been changed as a result of the action . When the motivation
and environment match the stimulus for the releaser , the second behavior is
triggered , and so on . 
enum Releaser =  { PRESENT , NOT_PRESENT }  ; 
Releaser food , hungry , nursed ; 
while ( TRUE )  { 
food = sense (  )  ; 
hungry = checkStateHunger (  )  ; 
child = checkStateChild (  )  ; 
if ( hungry =  = PRESENT ) 
searchForFood (  )  ;  /  / sets food = PRESENT when done
if ( hungry =  = PRESENT &  & food =  = PRESENT ) 
feed (  )  ;  /  / sets hungry = NOT_PRESENT when done
80 3 Biological Foundations of the Reactive Paradigm
if ( hungry =  = NOT_PRESENT &  & parent =  = PRESENT ) 
nurse (  )  ;  /  / set nursed = PRESENT when done
if ( nursed =  = PRESENT ) 
sleep (  )  ;  
} 
The example also reinforces the nature of behaviors . If the agent sleeps
and wakes up , but isn ‚Äô t hungry , what will it do ? According to the releasers
created above , the agent will just sit there until it gets hungry . 
In the previous example , the agent ‚Äô s behaviors allowed it to feed and enable the survival of its young , but the set of behaviors did not include fleeing
or fighting predators . Fleeing from predators could be added to the program
as follows : 
enum Releaser =  { PRESENT , NOT_PRESENT }  ; 
Releaser food , hungry , nursed , predator ; 
while ( TRUE )  { 
predator = sensePredator (  )  ; 
if ( predator =  = PRESENT ) 
flee (  )  ; 
food = senseFood (  )  ; 
hungry = checkStateHunger (  )  ; 
parent = checkStateParent (  )  ; 
if ( hungry =  = PRESENT ) 
searchForFood (  )  ; 
if ( hungry =  = PRESENT &  & food =  = PRESENT ) 
feed (  )  ; 
if ( hungry =  = NOT_PRESENT &  & parent =  = PRESENT ) 
nurse (  )  ; 
if ( nursed =  = PRESENT ) 
sleep (  )  ;  
} 
Notice that this arrangement allowed the agent to flee the predator regardless of where it was in the sequence of feeding , nursing , and sleeping
because predator is checked for first . But fleeing is temporary , because it did
not change the agent ‚Äô s internal state ( except possibly to make it more hungry
which will show up on the next iteration )  . The code may cause the agent to
flee for one second , then feed for one second . 
One way around this is to inhibit , or turn off , any other behavior until
fleeing is completed . This could be done with an if-else statement : 
3 . 3 Coordination and Control of Behaviors 81
while ( TRUE )  { 
predator = sensePredator (  )  ; 
if ( predator =  = PRESENT ) 
flee (  )  ; 
else { 
food = senseFood (  )  ; 
hungry = checkStateHunger (  )  ;  
.  .  .  
}  
} 
The addition of the if-else prevents other , less important behaviors
from executing . It doesn ‚Äô t solve the problem with the predator releaser disappearing as the agents runs away . If the agent turns and the predator
is out of view ( say , behind the agent )  , the value of predator will go to
NOT_PRESENT in the next update . The agent will go back to foraging , feeding , nursing , or sleeping . Fleeing should be a fixed-pattern action behavior
which persists for some period of time , T . The fixed-pattern action effect can
be accomplished with :  
# define T LONG_TIME
while ( TRUE )  { 
predator = sensePredator (  )  ; 
if ( predator =  = PRESENT ) 
for ( time = T ; time > 0 ; time-- ) 
flee (  )  ; 
else { 
food = senseFood (  )  ;  
.  .  .  
}  
} 
The C code examples were implemented as an implicit sequence , where
the order of execution depended on the value of the releasers . An implementation of the same behaviors with an explicit sequence would be : 
Releaser food , hungry , nursed , predator ; 
while ( TRUE )  { 
predator = sensePredator (  )  ; 
if ( predator =  = PRESENT ) 
82 3 Biological Foundations of the Reactive Paradigm
flee (  )  ; 
food = senseFood (  )  ; 
hungry = checkStateHunger (  )  ; 
parent = checkStateParent (  )  ; 
if ( hungry =  = PRESENT ) 
searchForFood (  )  ; 
feed (  )  ; 
nurse (  )  ; 
sleep (  )  ;  
} 
The explicit sequence at first may be more appealing . It is less cluttered
and the compound releasers are hidden . But this implementation is not
equivalent . It assumes that instead of the loop executing every second and
the behaviors acting incrementally , each behavior takes control and runs to
completion . Note that the agent cannot react to a predator until it has finished the sequence of behaviors . Calls to the fleeing behavior could be inserted between each behavior or fleeing could be processed on an interrupt
basis . But every ‚Äú fix ‚Äù makes the program less general purpose and harder to
add and maintain . 
The main point here is : simple behaviors operating independently can lead to
what an outside observer would view as a complex sequence of actions . 
3 . 3 . 2 Concurrent behaviors
An important point from the examples with the IRMs is that behaviors can , 
and often do , execute concurrently and independently . What appears to be a
fixed sequence may be the result of a normal series of events . However , some
behaviors may violate or ignore the implicit sequence when the environment
presents conflicting stimuli . In the case of the parent agent , fleeing a predator
was mutually exclusive of the feeding , nursing , and sleeping behaviors . 
Interesting things can happen if two ( or more ) behaviors are released that
usually are not executed at the same time . It appears that the strange interactions fall into the following categories : 
EQUILIBRIUM  Equilibrium ( the behaviors seem to balance each other out )  : Consider feeding
versus fleeing in a squirrel when the food is just close enough to a person
on a park bench . A squirrel will often appear to be visibly undecided as
to whether to go for the food or to stay away . 
3 . 4 Perception in Behaviors 83
DOMINANCE  Dominance of one ( winner take all )  : you ‚Äô re hungry and sleepy . You do one
or the other , not both simultaneously . 
CANCELLATION  Cancellation ( the behaviors cancel each other out )  : Male sticklebacks ( fish ) 
when their territories overlap get caught between the need to defend their
territory and to attack the other fish . So the males make another nest ! 
Apparently the stimuli cancels out , leaving only the stimulus normally
associated with nest building . 
Unfortunately , it doesn ‚Äô t appear to be well understood when these different mechanisms for conflicting behaviors are employed . Clearly , there ‚Äô s no
one method . But it does emphasize that a roboticist who works with behaviors should pay close attention to how the behaviors will interact . This
will give rise to the differences in architectures in the Reactive and Hybrid
Paradigms , discussed in later chapters . 
3 . 4 Perception in Behaviors
While Lorenz and Tinbergen ‚Äô s work provides some insights into behaviors , 
it ‚Äô s clear that behaviors depend on perception . Ulrich Neisser , who literally
created the term ‚Äú cognitive psychology ‚Äù in his book , Cognition and Reality , 
argued that perception cannot be separated from action . 109 As will be seen
in this section , J . J . Gibson , a very controversial cognitive psychologist , spent
his career advocating an ecological approach to perception . The ecological
approach is the opposite of the top-down , model-based reasoning about the
environment approach favored by psychologists , including Neisser . Interestingly enough , Neisser took a position at Cornell where J . J . Gibson was , 
and they became close colleagues . Since then , Neisser has spent significant
time and thought trying to reconcile the two views based on studies ; this has
led to his identification of two perceptual systems . 
3 . 4 . 1 Action-perception cycle
ACTION-PERCEPTION The action-perception cycle illustrates that perception is fundamental to any inCYCLE telligent agent . A simple interpretation of the cycle is : When an agent acts , it
interacts with its environment because it is situated in that environment ; it is
an integral part of the environment . So as it acts , it changes things or how it
perceives it ( e . g .  , move to a new viewpoint , trigger a rock slide , etc .  )  . Therefore the agent ‚Äô s perception of the world is modified . This new perception is
84 3 Biological Foundations of the Reactive Paradigm
Figure 3 . 5 Action-Perception Cycle . 7
then used for a variety of functions , including both cognitive activities like
planning for what to do next as well as reacting . The term cognitive activity
includes the concepts of feedback and feedforward control , where the agent
senses an error in what it attempted to do and what actually happened . An
equally basic cognitive activity is determining what to sense next . That activity can be something as straightforward as activating processes to look for
releasers , or as complex as looking for a particular face in a crowd . 
Regardless of whether there is an explicit conscious processing of the senses
or the extraction of a stimulus or releaser , the agent is now directed in terms
of what it is going to perceive on the next update ( s )  . This is a type of selective
attention or focus-of-attention . As it perceives , the agent perceptually samples the world . If the agent actually acts in a way to gather more perception
before continuing with its primary action , then that is sometimes referred to
as active perception . Part of the sampling process is to determine the potential for action . Lorenz and Tinbergen might think of this as the agent having
a set of releasers for a task , and now is observing whether they are present
in the world . If the perception supports an action , the agent acts . The action modifies the environment , but it also modifies the agent ‚Äô s assessment of
the situation . In the simplest case , this could be an error signal to be used
for control or a more abstract difference such as at the level of those used in
STRIPS / MEA . 
In some regards , the action-perception cycle appears to bear a superficial
resemblance to the Hierarchical Paradigm of SENSE , PLAN , ACT . However , 
note that 1 ) there is no box which contains ACT , and 2 ) the cycle does not
require the equivalent of planning to occur at each update . Action is implicit
3 . 4 Perception in Behaviors 85
in an agent ; the interesting aspect of the cycle is where perception and cognition come in . The agent may have to act to acquire more perception or to
accomplish a task . Also , the agent may or may not need to ‚Äú plan ‚Äù an action
on each update . 
3 . 4 . 2 Two functions of perception
Perception in behavior serves two functions . First , as we saw with IRMs , 
RELEASE it serves to release a behavior . However , releasing a behavior isn ‚Äô t necessarily the same as the second function : perceiving the information needed to
accomplish the behavior . For example , consider an animal in a forest fire . 
The fire activates the fleeing . But the fleeing behavior needs to extract inGUIDE formation about open spaces to run through obstacles in order to guide the
behavior . A frightened deer might bolt right past a hunter without apparently noticing . 
In both roles as a releaser and as a guide for behavior , perception filters the incoming stimulus for the task at hand . This is often referred to
as action-oriented perception by roboticists , when they wish to distinguish
their perceptual approach from the more hierarchical global models style of
perception . Many animals have evolved specialized detectors which simplify perception for their behaviors . Some frogs which sit in water all day
with just half their eyes poking up have a split retina : the lower half is good
for seeing in water , the upper half in air . 
3 . 4 . 3 Gibson : Ecological approach
The central tenet of Gibson ‚Äô s approach is that ‚Äú  .  .  . the world is its own best
representation .  ‚Äù Gibson ‚Äô s work is especially interesting because it complements the role of perception in IRM and is consistent with the action-perception cycle . Gibson postulated ( and proved ) the existence of affordances . 
AFFORDANCES Affordances are perceivable potentialities of the environment for an action . For example , to a baby arctic tern , the color red is perceivable and represents the potential for feeding . So an affordance can be a more formal way of defining the
external stimulus in IRM . But like IRMs , an affordance is only a potential‚Äî
it doesn ‚Äô t count until all the other conditions are satisfied ( the baby tern is
hungry )  . An affordance can also be the percept that guides the behavior . The
presence of red to a hungry baby arctic tern releases the feeding behavior . 
But the feeding behavior consists of pecking at the red object . So in this case , 
red is also the percept being used to guide the action , as well as release it . 
86 3 Biological Foundations of the Reactive Paradigm
Gibson referred to his work as an ‚Äú ecological approach ‚Äù because he believed that perception evolved to support actions , and that it is silly to try
to discuss perception independently of an agent ‚Äô s environment , and its survival behaviors . For example , a certain species of bees prefers one special
type of poppy . But for a long time , the scientists couldn ‚Äô t figure out how the
bees recognized that type of poppy because as color goes , it was indistinguishable from another type of poppy that grows in the same area . Smell ? 
Magnetism ? Neither . They looked at the poppy under UV and IR light . In
the non-visible bands that type of poppy stood out from other poppy species . 
And indeed , the scientists were able to locate retinal components sensitive
to that bandwidth . The bee and poppy had co-evolved , where the poppy ‚Äô s
color evolved to a unique bandwidth while at the same time the bee ‚Äô s retina
was becoming specialized at detecting that color . With a retina ‚Äú tuned ‚Äù for
the poppy , the bee didn ‚Äô t have to do any reasoning about whether there was
a poppy in view , and , if so , was it the right species of poppy . If that color was
present , the poppy was there . 
Fishermen have exploited affordances since the beginning of time . A fishing lure attempts to emphasize those aspects of a fish ‚Äô s desired food , presenting the strongest stimulus possible : if the fish is hungry , the stimulus of
the lure will trigger feeding . As seen in Fig . 3 . 6 , fishing lures often look to a
human almost nothing like the bait they imitate . 
What makes Gibson so interesting to roboticists is that an affordance is diDIRECT PERCEPTION rectly perceivable . Direct perception means that the sensing process doesn ‚Äô t
require memory , inference , or interpretation . This means minimal computation , which usually translates to very rapid execution times ( near instantaneous ) on a computer or robot . 
But can an agent actually perceive anything meaningful without some
memory , inference , or interpretation ? Well , certainly baby arctic terns don ‚Äô t
need memory or inference to get food from a parent . And they ‚Äô re definitely
not interpreting red in the sense of :  ‚Äú oh , there ‚Äô s a red blob . It ‚Äô s a small oval , 
which is the right shape for Mom , but that other one is a square , so it must
be a graduate ethology student trying to trick me .  ‚Äù For baby arctic terns , it ‚Äô s
simply : red = food , bigger red = better . 
Does this work for humans ? Consider walking down the hall and somebody throws something at you . You will most likely duck . You also probably
ducked without recognizing the object , although later you may determine it
was only a foam ball . The response happens too fast for any reasoning :  ‚Äú Oh
look , something is moving towards me . It must be a ball . Balls are usually
hard . I should duck .  ‚Äù Instead , you probably used a phenomena so basic that
3 . 4 Perception in Behaviors 87
Figure 3 . 6 A collection of artificial bait , possibly the first example of humans exploiting affordances . Notice that the lures exaggerate one or more attributes of what
a fish might eat . 
OPTIC FLOW you haven ‚Äô t noticed it , called optic flow . Optic flow is a neural mechanism
for determining motion . Animals can determine time to contact quite easily
with it . You probably are somewhat familiar with optic flow from driving in
a car . When driving or riding in a car , objects in front seem to be in clear focus
but the side of the road is a little blurry from the speed . The point in space
that the car is moving to is the focus of expansion . From that point outward , 
there is a blurring effect . The more blurring on the sides , the faster the car is
going .  ( They use this all the time in science fiction movies to simulate fasterthan-light travel .  ) That pattern of blurring is known as a flow field ( because
it can be represented by vectors , like a gravitational or magnetic field )  . It is
TIME TO CONTACT straightforward , neurally , to extract the time to contact , represented in the
cognitive literature by  . 
Gannets and pole vaulters both use optic flow to make last-minute , precise movements as reflexes . Gannets are large birds which dive from high
altitudes after fish . Because the birds dive from hundreds of feet up in the
air , they have to use their wings as control surfaces to direct their dive at the
targeted fish . But they are plummeting so fast that if they hit the water with
their wings open , the hollow bones will shatter . Gannets fold their wings just
before hitting the water . Optic flow allows the time to contact ,  , to be a stimulus : when the time to contact dwindles below a threshold , fold those wings ! 
88 3 Biological Foundations of the Reactive Paradigm
Pole vaulters also make minute adjustments in where they plant their pole
as they approach the hurdle . This is quite challenging given that the vaulter
is running at top speed . It appears that pole vaulters use optic flow rather
than reason ( slowly ) about where the best place is for the pole .  ( Pole vaulting
isn ‚Äô t the only instance where humans use optic flow , just one that has been
well-documented .  ) 
In most applications , a fast computer program can extract an affordance . 
However , this is not the case ( so far ) with optic flow . Neural mechanisms in
the retina have evolved to make the computation very rapid . It turns out that
computer vision researchers have been struggling for years to duplicate the
generation of an optical flow field from a camera image . Only recently have
we seen any algorithms which ran in real-time on regular computers . 48 The
point is that affordances and specialized detectors can be quite challenging
to duplicate in computers . 
Affordances are not limited to vision . A common affordance is knowing
when a container is almost filled to the top . Think about filling a jug with
water or the fuel tank of a car . Without being able to see the cavity , a person
knows when the tank is almost filled by the change in sound . That change
in sound is directly perceivable ; the person doesn ‚Äô t need to know anything
about the size or shape of the volume being filled or even what the liquid is . 
One particularly fascinating application of affordances to robotics , which
also serves to illustrate what an affordance is , is the research of Louise Stark
and Kevin Bowyer . 135 A seemingly unsurmountable problem in computer
vision has been to have a computer recognize an object from a picture . Literally , the computer should say ,  ‚Äú that ‚Äô s a chair ‚Äù if the picture is of a chair . 
STRUCTURAL MODELS The traditional way of approaching the problem has been to use structural
models . A structural model attempts to describe an object in terms of physical
components :  ‚Äú A chair has four legs , a seat , and a back .  ‚Äù But not all chairs fit
the same structural model . A typing chair has only one leg , with supports
at the bottom . Hanging baskets don ‚Äô t have legs at all . A bench seat doesn ‚Äô t
have a back . So clearly the structural approach has problems : instead of one
structural representation , the computer has to have access to many different
models . Structural models also lack flexibility . If the robot is presented with a
new kind of chair ( say someone has designed a chair to look like your toilet
or an upside down trash can )  , the robot would not be able to recognize it
without someone explicitly constructing another structural model . 
Stark and Bowyer explored an alternative to the structual approach called
GRUFF . GRUFF identifies chairs by function rather than form . Under Gibsonian perception , a chair should be a chair because it affords sitting , or serves
3 . 4 Perception in Behaviors 89
a . 
b . 
Figure 3 . 7 The GRUFF system : a .  ) input , and b .  ) different types of chairs recognized
by GRUFF .  ( Figures courtesy of Louise Stark .  ) 
the function of sittability . And that affordance of sittability should be something that can be extracted from an image :  Without memory ( the agent doesn ‚Äô t need to memorize all the chairs in the
world )  .  Without inference ( the robot doesn ‚Äô t need to reason :  ‚Äú if it has 4 legs , and a
seat and a back , then it ‚Äô s a chair ; we ‚Äô re in an area which should have lots
of chairs , so this makes it more likely it ‚Äô s a chair ‚Äù  )  .  Without an interpretation of the image ( the robot doesn ‚Äô t need to reason :  
 ‚Äú there ‚Äô s an arm rest , and a cushion ,  .  .  .  ‚Äù  )  . A computer should just be able
to look at a picture and say if something in that picture is sittable or not . 
90 3 Biological Foundations of the Reactive Paradigm
Stark and Bowyer represented sittability as a reasonably level and continuous surface which is at least the size of a person ‚Äô s butt and at about the
height of their knees .  ( Everything else like seat backs just serve to specify
the kind of chair .  ) Stark and Bowyer wrote a computer program which accepted CAD / CAM drawings from students who tried to come up with nonintuitive things that could serve as chairs ( like toilets , hanging basket chairs , 
trash cans )  . The computer program was able to correctly identify sittable
surfaces that even the students missed . 
It should be noted that Stark and Bowyer are hesitant to make claims about
what this says about Gibsonian perception . The computer vision algorithm
can be accused of some inference and interpretation (  ‚Äú that ‚Äô s the seat , that ‚Äô s
the right height ‚Äù  )  . But on the other hand , that level of inference and interpretation is significantly different than that involved in trying to determine the
structure of the legs , etc . And the relationship between seat size and height
could be represented in a special neural net that could be released whenever
the robot or animal got tired and wanted to sit down . The robot would start
noticing that it could sit on a ledge or a big rock if a chair or bench wasn ‚Äô t
around . 
3 . 4 . 4 Neisser : Two perceptual systems
At this point , the idea of affordances should seem reasonable . A chair is a
chair because it affords sittability . But what happens when someone sits in
your chair ? It would appear that humans have some mechanism for recognizing specific instances of objects . Recognition definitely involves memory 
(  ‚Äú my car is a blue Ford Explorer and I parked it in slot 56 this morning ‚Äù  )  . 
Other tasks , like the kind of sleuthing Sherlock Holmes does , may require
inference and interpretation .  ( Imagine trying to duplicate Sherlock Holmes
in a computer . It ‚Äô s quite different than mimicking a hungry baby arctic tern .  ) 
So while affordances certainly are a powerful way of describing perception
in animals , it is clearly not the only way animals perceive . Neisser postulated
that there are two perceptual systems in the brain ( and cites neurophysiological data )  : 110
DIRECT PERCEPTION 1 . direct perception . This is the ‚Äú Gibsonian ,  ‚Äù or ecological , track of the brain , 
and consists of structures low in the brain which evolved earlier on and
accounts for affordances . 
RECOGNITION 2 . recognition . This is more recent perceptual track in the brain , which ties in
with the problem solving and other cognitive activities . This part accounts
3 . 5 Schema Theory 91
for the use of internal models to distinguish ‚Äú your coffee cup ‚Äù from ‚Äú my
coffee cup .  ‚Äù This is where top-down , model-based perception occurs . 
On a more practical note , Neisser ‚Äô s dichotomy suggests that the first decision in designing a behavior is to determine whether a behavior can be
accomplished with an affordance or requires recognition . If it can be accomplished with an affordance , then there may be a simple and straightforward
way to program it in a robot ; otherwise , we will most certainly have to employ a more sophisticated ( and slower ) perceptual algorithm . 
3 . 5 Schema Theory
Schema theory provides a helpful way of casting some of the insights from
above into an object-oriented programming format . 6 Psychologists have used
schema theory since the early 1900 ‚Äô s . It was first brought to the serious attention of AI roboticists by Michael Arbib while at the University of Massachusetts , and later used extensively by Arkin and Murphy for mobile robotics , Lyons and Iberall for manipulation , 75 and Draper et al . for vision . 46
Schemas were conceived of by psychologists as a way of expressing the
SCHEMA basic unit of activity . A schema consists both of the knowledge of how to act
and / or perceive ( knowledge , data structures , models ) as well as the computational process by which it is uses to accomplish the activity ( the algorithm )  . The idea of a schema maps nicely onto a class in object-oriented
SCHEMA CLASS programming ( OOP )  . A schema class in C +  + or Java would contain both data 
( knowledge , models , releasers ) and methods ( algorithms for perceiving and
acting )  , as shown below . 
Schema : 
Data
Methods
A schema is a generic template for doing some activity , like riding a bicycle . It is a template because a person can ride different bicycles without
starting the learning process all over . Since a schema is parameterized like a
class , parameters ( type of bicycle , height of the bicycle seat , position of the
handlebars ) can be supplied to the object at the time of instantiation ( when
an object is created from the class )  . As with object-oriented programming , 
SCHEMA the creation of a specific schema is called a schema instantiation ( SI )  . 
INSTANTIATION ( SI ) 
92 3 Biological Foundations of the Reactive Paradigm
BEHAVIOR
Releaser
Pattern of 
Motor Actions
Sensory
Input
Perceptual
Schema
Motor
Schema
Figure 3 . 8 Behaviors decomposed into perceptual and motor schemas . 
The schema instantiation is the object which is constructed with whatever
parameters are needed to tailor it to the situation . For example , there could
be a move_to_food schema where the agent always heads in a straight line
to the food . Notice that the ‚Äú always heads in a straight line ‚Äù is a template
of activity , and a reusable algorithm for motion control . However , it is just a
method ; until the move_to_food schema is instantiated , there is no specific
goal to head for , e . g .  , the candy bar on the table . The same schema could be
instantiated for moving to a sandwich . 
3 . 5 . 1 Behaviors and schema theory
In the Arbibian application of schema theory towards a computational theCOMPOSITION OF A ory of intelligence , a behavior is a schema which is composed of a motor
BEHAVIOR schema and a perceptual schema . The motor schema represents the template MOTOR SCHEMA for the physical activity , the perceptual schema embodies the sensing . The mo- PERCEPTUAL SCHEMA
tor schema and perceptual schema are like pieces of a puzzle ; both pieces
must be together in place before there is a behavior . This idea is shown below
in Fig . 3 . 8 . 
Essentially , the perceptual and motor schema concept fits in with ethology
and cognitive psychology as follows :  A behavior takes sensory inputs and produces motor actions as an output . 
3 . 5 Schema Theory 93
 A behavior can be represented as a schema , which is essentially an objectoriented programming construct .  A behavior is activated by releasers .  The transformation of sensory inputs into motor action outputs can be
divided into two sub-processes : a perceptual schema and a motor schema . 
In OOP terms , the motor schema and perceptual schema classes are derived from the schema class . A primitive behavior just has one motor and
one perceptual schema . 
Behavior :  : Schema
Data
Methods perceptual_schema (  ) 
motor_schema (  ) 
Recall from IRMs , more sophisticated behaviors may be constructed by
sequencing behaviors . In the case of a sequence of behaviors , the overall
behavior could be represented in one of two ways . One way is to consider
the behavior to be composed of several primitive behaviors , with the releasing logic to serve as the knowledge as to when to activate each primitive
behaviors . This is probably the easiest way to express a ‚Äú meta ‚Äù behavior . 
A meta-behavior composed of three behaviors can be thought of as : 
Behavior :  : Schema
Data releaser1
releaser2
releaser3
IRM_logic (  ) 
Methods behavior1 (  ) 
behavior2 (  ) 
behavior3 (  ) 
However , in more advanced applications , the agent may have a choice of
either perceptual or motor schemas to tailor its behavior . For example , a
person usually uses vision ( the default perceptual schema ) to navigate out
of a room ( motor schema )  . But if the power is off , the person can use touch 
( an alternate perceptual schema ) to feel her way out of a dark room . In this
case , the schema-specific knowledge is knowing which perceptual schema
94 3 Biological Foundations of the Reactive Paradigm
to use for different environmental conditions . Schema theory is expressive
enough to represent basic concepts like IRMs , plus it supports building new
behaviors out of primitive components . This will be discussed in more detail
in later chapters . 
This alternative way of creating a behavior by choosing between alternative perceptual and motor schemas can be thought of as : 
Behavior :  : Schema
Data environmental_state
Methods choose_PS ( environmental_state ) 
perceptual_schema_1 (  ) 
perceptual_schema_2 (  ) 
motor_schema (  ) 
Arbib and colleagues did work constructing computer models of visually
guided behaviors in frogs and toads . They used schema theory to represent
RANA COMPUTATRIX the toad ‚Äô s behavior in computational terms , and called their model rana computatrix ( rana is the classification for toads and frogs )  . The model explained
Ingle ‚Äô s observations as to what occasionally happens when a toad sees two
flies at once . 33 Toads and frogs can be characterized as responding visually
to either small , moving objects and large , moving objects . Small , moving objects release the feeding behavior , where the toad orients itself towards the
object ( taxis ) and then snaps at it .  ( If the object turns out not to be a fly , 
the toad can spit it out .  ) Large moving objects release the fleeing behavior , 
causing the toad to hop away . The feeding behavior can be modeled as a
behavioral schema , or template , shown in Fig . 3 . 9 . 
When the toad sees a fly , an instance of the behavior is instantiated ; the
toad turns toward that object and snaps at it . Arbib ‚Äô s group went one level
further on the computational theory . 7 They implemented the taxis behavior
as a vector field : rana computatrix would literally feel an attractive force
along the direction of the fly . This direction and intensity ( magnitude ) was
represented as a vector . The direction indicated where rana had to turn and
the magnitude indicated the strength of snapping . This is shown in Fig . 3 . 10 . 
What is particularly interesting is that the rana computatrix program predicts what Ingle saw in real toads and frogs when they are presented with
two flies simultaneously . In this case , each fly releases a separate instance of
the feeding behavior . Each behavior produces the vector that the toad needs
to turn to in order to snap at that fly , without knowing that the other be-
3 . 5 Schema Theory 95
Releaser
Pattern of 
Motor Actions
Sensory
Input
Perceptual
Schema
Motor
Schema
Get coordinates
of small , moving
object
Turn to
coordinates
of small , 
moving object
appearance of
small , moving object
toad ‚Äô s vision toad ‚Äô s legs
Feeding Behavior
Figure 3 . 9 Toad ‚Äô s feeding behavior represented as a behavior with schema theory . 
havior exists . According to the vector field implementation of the schema
model , the toad now receives two vectors , instead of one . What to do ? Well , 
rana computatrix summed the two vectors , resulting in a third vector in between the original two ! The toad snaps at neither fly , but in the middle . The
unexpected interaction of the two independent instances probably isn ‚Äô t that
much of a disadvantage for a toad , because if there are two flies in such close
proximity , eventually one of them will come back into range . 
This example illustrates many important lessons for robotics . First , it validates the idea of a computational theory , where functionality in an animal
and a computer can be equivalent . The concept of behaviors is Level 1 of the
computational theory , schema theory ( especially the perceptual and motor
schemas ) expresses Level 2 , and Level 3 is the vector field implementation
of the motor action . It shows the property of emergent behavior , where the
agent appears to do something fairly complex , but is really just the result of
interaction between simple modules . The example also shows how behaviors correspond to object-orienting programming principles . 
Another desirable aspect of schema theory is that it supports reflex behaviors . Recall that in reflex behaviors , the strength of the response is proportional to the strength of the stimulus . In schema theory , the perceptual
schema is permitted to pass both the percept and a gain to the motor schema . 
The motor schema can use the gain to compute a magnitude on the output
action . This is an example of how a particular schema can be tailored for a
behavior . 
96 3 Biological Foundations of the Reactive Paradigm
activation
condition
motor schema perceptual schema
behavior
motor SI perceptual SI
percept , 
gain
action , 
intensity
fly1
snap locate_fly
snap ( fly1 ) locate_fly ( fly1 )  
( x , y , z )  , 
100%
snap at ( x , y , z ) 
with all strength
Figure 3 . 10 Schema theory of a frog snapping at a fly . 
Schema theory does not specify how the output from concurrent behaviors
is combined ; that is a Level 3 , or implementation , issue . Previous examples in
this chapter have shown that in some circumstances the output is combined
or summed , in others the behaviors would normally occur in a sequence
and not overlap , and sometimes there would be a winner-take-all effect . The
winner-take-all effect is a type of inhibition , where one behavior inhibits the
instantiation of another behavior . 
INHIBITION Arbib and colleagues also modeled an instance of inhibition in frogs and
toads . 7 Returning to the example of feeding and fleeing , one possible way to
model this behavior is with two behaviors . The feeding behavior would
consist of a motor schema for moving toward an object , with a perceptual
schema for finding small , moving objects . The fleeing behavior would be
similar only with a motor schema for moving away from the perception of
large moving objects . Lesion studies with frogs showed something different . 
The feeding behavior actually consists of moving toward any moving object . So the perceptual schema is more general than anticipated . The frog
would try to eat anything , including predators . The perceptual schema in
the fleeing behavior detects large moving objects . It flees from them , but
3 . 6 Principles and Issues in Transferring Insights to Robots 97
activation
condition
motor schema perceptual schema
fly1
snap locate_fly
behavior
motor SI perceptual SI
percept , 
gain
action , 
intensity
snap ( fly1 ) locate_fly ( fly1 )  
( x , y , z )  , 
100%
snap at ( x , y , z ) 
with all strength
behavior
motor SI perceptual SI
percept , 
gain
action , 
intensity
snap ( fly2 ) locate_fly ( fly2 )  
( x , y , z )  , 
100%
snap at ( x , y , z ) 
with all strength
fly2
vector summation : 
snaps at the " average "  
( x , y , z ) 
Figure 3 . 11 Schema theory of a frog snapping at a fly when presented with two flies
equidistant . 
it also inhibits the perceptual schema for feeding . As a result , the inhibition
keeps the frog from trying to both flee from predators and eat them . 
3 . 6 Principles and Issues in Transferring Insights to Robots
To summarize , some general principles of natural intelligence which may be
PRINCIPLES FOR useful in programming robots : 
PROGRAMMING  Programs should decompose complex actions into independent behaviors , which tightly couple sensing and acting . Behaviors are inherently
parallel and distributed .  In order to simplify control and coordination of behaviors , an agent should
rely on straightforward , boolean activation mechanisms ( e . g . IRM )  . 
98 3 Biological Foundations of the Reactive Paradigm
 In order to simplify sensing , perception should filter sensing and consider
only what is relevant to the behavior ( action-oriented perception )  .  Direct perception ( affordances ) reduces the computational complexity of
sensing , and permits actions to occur without memory , inference , or interpretation .  Behaviors are independent , but the output from one 1 ) may be combined
with another to produce a resultant output , or 2 ) may serve to inhibit
another ( competing-cooperating )  . 
Unfortunately , studying natural intelligence does not give a complete picture of how intelligence works . In particular there are several unresolved
UNRESOLVED ISSUES issues :  How to resolve conflicts between concurrent behaviors ? Robots will be required to perform concurrent tasks ; for example , a rescue robot sent in
to evacuate a building will have to navigate hallways while looking for
rooms to examine for people , as well as look for signs of a spreading fire . 
Should the designer specify dominant behaviors ? Combine ? Let conflicting behaviors cancel and have alternative behavior triggered ? Indeed , one
of the biggest divisions in robot architectures is how they handle concurrent behaviors .  When are explicit knowledge representations and memory necessary ? Direct
perception is wonderful in theory , but can a designer be sure that an affordance has not been missed ?  How to set up and / or learn new sequences of behaviors ? Learning appears to be
a fundamental component of generating complex behaviors in advanced
animals . However , the ethological and cognitive literature is unsure of
the mechanisms for learning . 
It is also important to remember that natural intelligence does not map
perfectly onto the needs and realities of programming robots . One major
advantage that animal intelligence has over robotic intelligence is evolution . 
Animals evolved in a way that leads to survival of the species . But robots are
expensive and only a small number are built at any given time . Therefore , individual robots must ‚Äú survive ,  ‚Äù not species . This puts tremendous pressure
on robot designers to get a design right the first time . The lack of evolutionary pressures over long periods of time makes robots extremely vulnerable
to design errors introduced by a poor understanding of the robot ‚Äô s ecology . 
3 . 7 Summary 99
Ch . 5 will provide a case study of a robot which was programmed to follow white lines in a path-following competition by using the affordance of
white . It was distracted off course by the white shoes of a judge . Fortunately
that design flaw was compensated for when the robot got back on course by
reacting to a row of white dandelions in seed . 
Robots introduce other challenges not so critical in animals . One of the
most problematic attributes of the Reactive Paradigm , Ch . 4 , is that roboticists have no real mechanism for completely predicting emergent behaviors . 
Since a psychologist can ‚Äô t predict with perfect certainty what a human will
do under a stressful situation , it seems reasonable that a roboticist using principles of human intelligence wouldn ‚Äô t be able to predict what a robot would
do either . However , robotics end-users ( military , NASA , nuclear industry ) 
have been reluctant to accept robots without a guarantee of what it will do
in critical situations . 
3 . 7 Summary
A behavior is the fundamental element of biological intelligence , and will
serve as the fundamental component of intelligence in most robot systems . 
BEHAVIOR A behavior is defined as a mapping of sensory inputs to a pattern of motor
actions which then are used to achieve a task . Innate Releasing Mechanisms
are one model of how intelligence is organized . IRMs model intelligence at
Level 2 of a computational theory , describing the process but not the implementation . In IRM , releasers activate a behavior . A releaser may be either
an internal state ( motivation ) and / or an environmental stimulus . Unfortunately , IRMs do not make the interactions between concurrent , or potentially
concurrent , behaviors easy to identify or diagram . 
Perception in behaviors serves two roles , either as a releaser for a behavior
or as the percept which guides the behavior . The same percept can be used
both as a releaser and a guide ; for example , a fish can respond to a lure and
follow it . In addition to the way in which perception is used , there appear to
be two pathways for processing perception . The direct perception pathway
uses affordances : perceivable potentialities for action inherent in the environment . Affordances are particularly attractive to roboticists because they
can be extracted without inference , memory , or intermediate representations . 
The recognition pathway makes use of memory and global representations
to identify and label specific things in the world . 
Important principles which can be extracted from natural intelligence are : 
100 3 Biological Foundations of the Reactive Paradigm
 Agents programs should decompose complex actions into independent
behaviors ( or objects )  , which tightly couple sensing and acting . Behaviors
are inherently parallel and distributed .  In order to simplify control and coordination of behaviors , an agent should
rely on straightforward , boolean activation mechanisms ( e . g . IRM )  .  In order to simplify sensing , perception should filter sensing and consider
only what is relevant to the behavior ( action-oriented perception )  .  Direct perception ( affordances ) reduces the computational complexity of
sensing , and permits actions to occur without memory , inference , or interpretation .  Behaviors are independent , but the output from one 1 ) may be combined
with another to produce a resultant output , or 2 ) may serve to inhibit
another ( competing-cooperating )  . 
Schema theory is an object-oriented way of representing and thinking about
behaviors . The important attributes of schema theory for behaviors are :  Schema theory is used to represent behaviors in both animals and computers , and is sufficient to describe intelligence at the first two levels of a
computational theory .  A behavioral schema is composed of at least one motor schema and at
least one perceptual schema , plus local , behavior-specific knowledge about
how to coordinate multiple component schemas .  More than one behavior schema can be instantiated at a time , but the
schemas act independently .  A behavior schema can have multiple instantiations which act independently , and are combined .  Behaviors or schemas can be combined , sequenced , or inhibit one another . 
3 . 8 Exercises
Exercise 3 . 1
Describe the three levels in a Computational Theory . 
3 . 8 Exercises 101
Exercise 3 . 2
Explain in one or two sentences each of the following terms : reflexes , taxes , fixedaction patterns , schema , affordance . 
Exercise 3 . 3
Represent a schema , behavior , perceptual schema , and motor schema with an ObjectOriented Design class diagram . 
Exercise 3 . 4
Many mammals exhibit a camouflage meta-behavior . The animal freezes when it sees
motion ( an affordance for a predator ) in an attempt to become invisible . It persists until the predator is very close , then the animal flees .  ( This explains why squirrels freeze
in front of cars , then suddenly dash away , apparently flinging themselves under the
wheels of a car .  ) Write pseudo-code of the behaviors involved in the camouflage
behavior in terms of innate releasing mechanisms , identifying the releasers for each
behavior . 
Exercise 3 . 5
Consider a mosquito hunting for a warm-blooded mammal and a good place to bite
them . Identify the affordance for a warm-blooded mammal and the associated behavior . Represent this with schema theory ( perceptual and motor schemas )  . 
Exercise 3 . 6
One method for representing the IRM logic is to use finite state automata ( FSA )  , 
which are commonly used in computer science . If you have seen FSAs , consider a
FSA where the behaviors are states and releasers serve as the transitions between
state . Express the sequence of behaviors in a female digger wasp as a FSA . 
Exercise 3 . 7
Lego Mindstorms and Rug Warrior kits contain sensors and actuators which are coupled together in reflexive behaviors . Build robots which : 
a . Reflexive avoid : turn left when they touch something ( use touch sensor and two
motors ) 
b . Phototaxis : follow a black line ( use the IR sensor to detect the difference between
light and dark ) 
c . Fixed-action pattern avoid : back up and turn right when robot encounters a ‚Äú negative obstacle ‚Äù  ( a cliff ) 
Exercise 3 . 8
What is the difference between direct perception and recognition ? 
Exercise 3 . 9
Consider a cockroach , which typically hides when the lights are turned on . Do you
think the cockroach is using direct perception or recognition of a hiding place ? Explain why . What are the percepts for the cockroach ? 
102 3 Biological Foundations of the Reactive Paradigm
Exercise 3 . 10
Describe how cancellation could happen as a result of concurrency and incomplete
FSA . 
Exercise 3 . 11 [ Advanced Reading ] 
Read the first four chapters in Braitenberg ‚Äô s Vehicles . 
25 Write a 2-5 page paper : 
a . List and describe the principles of behaviors for robotics in Ch . 3 . 
b . Discuss how Vehicles is consistent with the biological foundations of reactivity . Be
specific , citing which vehicle illustrates what principle or attribute discussed in
the book . 
c . Discuss any flaws in the reasoning or inconsistency between Vehicles with the biological foundations of reactivity or computer science . 
Exercise 3 . 12 [ Advanced Reading ] 
Read ‚Äú Sensorimotor transformations in the worlds of frogs and robots ,  ‚Äù by Arbib and
Liaw . 7
a . List and describe how the principles of schema theory and potential fields for robotics given in Ch . 3 . 
b . Summarize the main contributions of the paper . 
3 . 9 End Notes
For the roboticist ‚Äô s bookshelf . 
Valentino Braitenberg ‚Äô s Vehicles : Experiments in Synthetic Psychology25 is the cult classic of AI roboticists everywhere . It doesn ‚Äô t require any hardware or programming
experience , just a couple hours of time and an average imagination to experience this
radical departure from the mainstream robotics of the 1970 ‚Äô s . 
About David Marr . 
David Marr ‚Äô s idea of a computational theory was an offshoot of his work bridging
the gap between vision from a neurophysiological perspective ( his ) and computer
vision . As is discussed in his book , Vision , 
88 Marr had come from England to work in
the MIT AI lab on computer vision . The book represented his three years there , and
he finished it while literally on his deathbed with leukemia . His preface to the book
is heartbreaking . 
A Brief History of Cognitive Science . 
Howard Gardner ‚Äô s The Mind ‚Äô s New Science 56 gives a nice readable overview of cognitive psychology . He conveys a bit of the controversy Gibson ‚Äô s work caused . 
3 . 9 End Notes 103
J . J . and E . J . Gibson . 
While J . J . Gibson is very well-known , his wife Jackie ( E . J . Gibson ) is also a prominent
cognitive psychologist . They met when he began teaching at Smith College , where
she was a student . She raised a family , finished a PhD , and publishes well-respected
studies on learning . At least two of the Gibson ‚Äô s students followed their husbandwife teaming : Herb Pick was a student of J . J . Gibson , while his wife , Anne Pick , was
a student of E . J . Gibson . The Picks are at the University of Minnesota , and Herb Pick
has been active in the mobile robotics community . 
Susan Calvin , robopsychologist . 
Isaac Asimov ‚Äô s robot stories often feature Dr . Susan Calvin , the first robopsychologist . In the stories , Calvin is often the only person who can figure out the complex
interactions of concurrent behaviors leading to a robot ‚Äô s emergent misbehavior . In
some regards , Calvin is the embarrassing Cold War stereotype of a woman scientist : 
severe , unmarried , too focused on work to be able to make small talk . 

4 The Reactive Paradigm
Chapter Objectives :  Define what the reactive paradigm is in terms of i ) the three primitives
SENSE , PLAN , and ACT , and ii ) sensing organization .  List the characteristics of a reactive robotic system , and discuss the connotations surrounding the reactive paradigm .  Describe the two dominant methods for combining behaviors in a reactive
architecture : subsumption and potential field summation .  Evaluate subsumption and potential fields architectures in terms of : support for modularity , niche targetability , ease of portability to other domains , robustness .  Be able to program a behavior using a potential field methodology .  Be able to construct a new potential field from primitive potential fields , 
and sum potential fields to generate an emergent behavior . 
4 . 1 Overview
This chapter will concentrate on an overview of the reactive paradigm and
two representative architectures . The Reactive Paradigm emerged in the late
1980 ‚Äô s . The Reactive Paradigm is important to study for at least two reasons . 
First , robotic systems in limited task domains are still being constructed using reactive architectures . Second , the Reactive Paradigm will form the basis
for the Hybrid Reactive-Deliberative Paradigm ; everything covered here will
be used ( and expanded on ) by the systems in Ch . 7 . 
106 4 The Reactive Paradigm
Sensors
Extract
Features
Combine
Features
into
Model
Plan
Tasks
Task
Execution
Motor
Control Actuators
SENSE PLAN ACT
Figure 4 . 1 Horizontal decomposition of tasks into the S , P , A organization of the Hierarchical Paradigm . 
The Reactive Paradigm grew out of dissatisfaction with the hierarchical
paradigm and with an influx of ideas from ethology . Although various reactive systems may or may not strictly adhere to principles of biological intelligence , they generally mimic some aspect of biology . The dissatisfaction with
the Hierarchical Paradigm was best summarized by Rodney Brooks , 27 who
HORIZONTAL characterized those systems as having a horizontal decomposition as shown in
DECOMPOSITION Fig . 4 . 1 . 
Instead , an examination of the ethological literature suggests that intelliVERTICAL gence is layered in a vertical decomposition , shown in Fig . 4 . 2 . Under a verDECOMPOSITION tical decomposition , an agent starts with primitive survival behaviors and
evolves new layers of behaviors which either reuse the lower , older behaviors , inhibit the older behaviors , or create parallel tracks of more advanced
behaviors . The parallel tracks can be thought of layers , stacked vertically . 
Each layer has access to sensors and actuators independently of any other
layers . If anything happens to an advanced behavior , the lower level behaviors would still operate . This return to a lower level mimics degradation
of autonomous functions in the brain . Functions in the brain stem ( such as
breathing ) continue independently of higher order functions ( such as counting , face recognition , task planning )  , allowing a person who has brain damage from a car wreck to still breathe , etc . 
Work by Arkin , Brooks , and Payton focused on defining behaviors and on
mechanisms for correctly handling situations when multiple behaviors are
active simultaneously . Brooks took an approach now known as subsumption
and built insect-like robots with behaviors captured in hardware circuitry . 
4 . 1 Overview 107
SENSE ACT
SENSE ACT
SENSE ACT
SENSE ACT
build maps
explore
wander
avoid collisions
actuators sensors
Figure 4 . 2 Vertical decomposition of tasks into an S-A organization , associated with
the Reactive Paradigm . 
Arkin and Payton used a potential fields methodology , favoring software
implementations . Both approaches are equivalent . The Reactive Paradigm
was initially met with stiff resistance from traditional customers of robotics , particularly the military and nuclear regulatory agencies . These users of
robotic technologies were uncomfortable with the imprecise way in which
discrete behaviors combine to form a rich emergent behavior . In particular , 
reactive behaviors are not amenable to mathematical proofs showing they
are sufficient and correct for a task . In the end , the rapid execution times
associated with the reflexive behaviors led to its acceptance among users , 
just as researchers shifted to the Hybrid paradigm in order to fully explore
layering of intelligence . 
108 4 The Reactive Paradigm
The major theme of this chapter is that all reactive systems are composed
of behaviors , though the meaning of a behavior may be slightly different in
each reactive architecture . Behaviors can execute concurrently and / or sequentially . The two representative architectures , subsumption and potential
fields , are compared and contrasted using the same task as an example . This
chapter will concentrate on how architecture handles concurrent behaviors
to produce an emergent behavior , deferring sequencing to the next chapter . 
4 . 2 Attributes of Reactive Paradigm
The fundamental attribute of the reactive paradigm is that all actions are
BEHAVIORS accomplished through behaviors . As in ethological systems , behaviors are a
direct mapping of sensory inputs to a pattern of motor actions that are then used
to achieve a task . From a mathematical perspective , behaviors are simply a
transfer function , transforming sensory inputs into actuator commands . For
the purposes of this book , a behavior will be treated as a schema , and will
consist of at least one motor schema and one perceptual schema . The motor schema contains the algorithm for generating the pattern of action in a
physical actuator and the perceptual schema contains the algorithm for extracting the percept and its strength . Keep in mind that few reactive robot
architectures describe their behaviors in terms of schemas . But in practice , 
most behavioral implementations have recognizable motor and perceptual
routines , even though they are rarely referred to as schemas . 
The Reactive Paradigm literally threw away the PLAN component of the
SENSE , PLAN , ACT triad , as shown in Fig . 4 . 3 . The SENSE and ACT components are tightly coupled into behaviors , and all robotic activities emerge
as the result of these behaviors operating either in sequence or concurrently . 
SENSE-ACT The S-A organization does not specify how the behaviors are coordinated
ORGANIZATION and controlled ; this is an important topic addressed by architectures . 
BEHAVIOR-SPECIFIC Sensing in the Reactive Paradigm is local to each behavior , or behavior- 
( LOCAL ) SENSING specific . Each behavior has its own dedicated sensing . In many cases , this is
implemented as one sensor and perceptual schema per behavior . But in other
cases , more than one behavior can take the same output from a sensor and
process it differently ( via the behavior ‚Äô s perceptual schema )  . One behavior
literally does not know what another behavior is doing or perceiving . Fig . 4 . 4
graphically shows the sensing style of the Reactive Paradigm . 
Note that this is fundamentally opposite of the global world model used
in the hierarchical paradigm . Sensing is immediately available to the be-
4 . 2 Attributes of Reactive Paradigm 109
Figure 4 . 3 S-A organization of the Reactive Paradigm into multiple , concurrent behaviors . 
havior ‚Äô s perceptual schema , which can do as little or as much processing
as needed to extract the relevant percept . If a computationally inexpensive
affordance is used , then the sensing portion of the behavior is nearly instantaneous and action is very rapid . 
As can be seen from the previous chapter on the biological foundations of
the reactive paradigm , behaviors favor the use of affordances . In fact , Brooks
was fond of saying ( loudly ) at conferences ,  ‚Äú we don ‚Äô t need no stinking representations .  ‚Äù It should be noted that often the perceptual schema portion of
the behavior has to use a behavior-specific representation or data structure
to substitute for specialized detectors capable of extracting affordances . For
example , extracting a red region in an image is non-trivial with a computer
compared with an animal seeing red . The point is that while a computer program may have to have data structures in order to duplicate a simple neural
function , the behavior does not rely on any central representation built up
from all sensors . 
In early implementations of the reactive paradigm , the idea of ‚Äú one sensor , 
one behavior ‚Äù worked well . For more advanced behaviors , it became useful
to fuse the output of multiple sensors within one perceptual schema to have
increased precision or a better measure of the strength of the stimulus . This
type of sensor fusion is permitted within the reactive paradigm as long as
the fusion is local to the behavior . Sensor fusion will be detailed in Ch . 6 . 
110 4 The Reactive Paradigm
sensor 1 sensor 2 actuators
Perceptual
Schema
percept Motor
Schema
behavior
Perceptual
Schema
percept Motor
Schema
behavior
Perceptual
Schema
percept Motor
Schema
behavior
Figure 4 . 4 Behavior-specific sensing organization in the Reactive Paradigm : sensing
is local , sensors can be shared , and sensors can be fused locally by a behavior . 
4 . 2 . 1 Characteristics and connotations of reactive behaviors
As seen earlier , a reactive robotic system decomposes functionality into behaviors , which tightly couple perception to action without the use of intervening abstract ( global ) representations . This is a broad , vague definition . 
Over the years , the reactive paradigm has acquired several connotations and
characteristics from the way practitioners have used the paradigm . 
The primary connotation of a reactive robotic system is that it executes
rapidly . The tight coupling of sensing and acting permits robots to operate in real-time , moving at speeds of 1-2 cm per second . Behaviors can be
implemented directly in hardware as circuits , or with low computational
complexity algorithms ( O ( n )  )  . This means that behaviors execute quickly regardless of the processor . Behaviors execute not only fast in their own right , 
they are particularly fast when compared to the execution times of Shakey
and the Stanford Cart . A secondary connotation is that reactive robotic systems have no memory , limiting reactive behaviors to what biologists would
call pure stimulus-response reflexes . In practice , many behaviors exhibit a
4 . 2 Attributes of Reactive Paradigm 111
fixed-action pattern type of response , where the behavior persists for a short
period of time without the direct presence of the stimulus . The main point is
that behaviors are controlled by what is happening in the world , duplicating
the spirit of innate releasing mechanisms , rather than by the program storing and remembering what the robot did last . The examples in the chapter
emphasize this point . 
The five characteristics of almost all architectures that follow the Reactive
Paradigm are : 
1 . Robots are situated agents operating in an ecological niche . As seen earlier in
SITUATED AGENT Part I , situated agent means that the robot is an integral part of the world . A
robot has its own goals and intentions . When a robot acts , it changes the
world , and receives immediate feedback about the world through sensing . What the robot senses affects its goals and how it attempts to meet
them , generating a new cycle of actions . Notice that situatedness is defined by Neisser ‚Äô s Action-Perception Cycle . Likewise , the goals of a robot , 
the world it operates in , and how it can perceive the world form the ecological niche of the robot . To emphasize this , many robotic researchers say
ECOLOGICAL ROBOTICS they are working on ecological robotics . 
2 . Behaviors serve as the basic building blocks for robotic actions , and the overall
behavior of the robot is emergent . Behaviors are independent , computational
entities and operate concurrently . The overall behavior is emergent : there
is no explicit ‚Äú controller ‚Äù module which determines what will be done , or
functions which call other functions . There may be a coordinated control
program in the schema of a behavior , but there is no external controller
of all behaviors for a task . As with animals , the ‚Äú intelligence ‚Äù of the robot is in the eye of the beholder , rather than in a specific section of code . 
Since the overall behavior of a reactive robot emerges from the way its
individual behaviors interact , the major differences between reactive architectures is usually the specific mechanism for interaction . Recall from
Chapter 3 that these mechanisms include combination , suppression , and
cancellation . 
3 . Only local , behavior-specific sensing is permitted . The use of explicit abstract
representational knowledge in perceptual processing , even though it is
behavior-specific , is avoided . Any sensing which does require represenEGO-CENTRIC tation is expressed in ego-centric ( robot-centric ) coordinates . For example , 
consider obstacle avoidance . An ego-centric representation means that it
does not matter that an obstacle is in the world at coordinates ( x , y , z )  , only
112 4 The Reactive Paradigm
where it is relative to the robot . Sensor data , with the exception of GPS , is
inherently ego-centric ( e . g .  , a range finder returns a distance to the nearest
object from the transducer )  , so this eliminates unnecessary processing to
create a world model , then extract the position of obstacles relative to the
robot . 
4 . These systems inherently follow good software design principles . The modularity of these behaviors supports the decomposition of a task into component behaviors . The behaviors are tested independently , and behaviors
may be assembled from primitive behaviors . 
5 . Animal models of behavior are often cited as a basis for these systems or a particular behavior . Unlike in the early days of AI robotics , where there was a
conscious effort to not mimic biological intelligence , it is very acceptable
under the reactive paradigm to use animals as a motivation for a collection
of behaviors . 
4 . 2 . 2 Advantages of programming by behavior
Constructing a robotic system under the Reactive Paradigm is often referred
to as programming by behavior , since the fundamental component of any
implementation is a behavior . Programming by behavior has a number of
advantages , most of them consistent with good software engineering principles . Behaviors are inherently modular and easy to test in isolation from the
system ( i . e .  , they support unit testing )  . Behaviors also support incremental
expansion of the capabilities of a robot . A robot becomes more intelligent by
having more behaviors . The behavioral decomposition results in an implementation that works in real-time and is usually computationally inexpensive . Although we ‚Äô ll see that sometimes duplicating specialized detectors 
( like optic flow ) is slow . If the behaviors are implemented poorly , then a reactive implementation can be slow . But generally , the reaction speeds of a
reactive robot are equivalent to stimulus-response times in animals . 
Behaviors support good software engineering principles through decomposition , modularity and incremental testing . If programmed with as high
LOW COUPLING a degree of independence ( also called low coupling ) as possible , and high coHIGH COHESION hesion , the designer can build up libraries of easy to understand , maintain , 
and reuse modules that minimize side effects . Low coupling means that the
modules can function independently of each other with minimal connections
or interfaces , promoting easy reuse . Cohesion means that the data and operations contained by a module relate only to the purpose of that module . 
4 . 3 Subsumption Architecture 113
Higher cohesion is associated with modules that do one thing well , like the
SQRT function in C . The examples in Sec . 4 . 3 and 4 . 4 attempt to illustrate the
choices a designer has in engineering the behavioral software of a robot . 
4 . 2 . 3 Representative architectures
In order to implement a reactive system , the designer must identify the set
of behaviors necessary for the task . The behaviors can either be new or use
existing behaviors . The overall action of the robot emerges from multiple , 
concurrent behaviors . Therefore a reactive architecture must provide mechanisms for 1 ) triggering behaviors and 2 ) for determining what happens when
multiple behaviors are active at the same time . Another distinguishing feature between reactive architectures is how they define a behavior and any
special use of terminology . Keep in mind that the definitions presented in
Sec . 4 . 2 are a generalization of the trends in reactive systems , and do not
necessarily have counterparts in all architectures . 
There are many architectures which fit in the Reactive Paradigm . The two
best known and most formalized are the subsumption and potential field
methodologies . Subsumption refers to how behaviors are combined . Potential Field Methodologies require behaviors to be implemented as potential
fields , and the behaviors are combined by summation of the fields . A third
RULE ENCODING style of reactive architecture which is popular in Europe and Japan is rule
encoding , where the motor schema component of behaviors and the combination mechanism are implemented as logical rules . The rules for combining behaviors are often ad hoc , and so will not be covered in this book . 
Other methods for combining behaviors exist , including fuzzy methods and
winner-take-all voting , but these tend to be implementation details rather
than an over-arching architecture . 
4 . 3 Subsumption Architecture
Rodney Brooks ‚Äô subsumption architecture is the most influential of the purely
Reactive Paradigm systems . Part of the influence stems from the publicity
surrounding the very naturalistic robots built with subsumption . As seen
in Fig . 4 . 5 , these robots actually looked like shoe-box sized insects , with
six legs and antennae . In many implementations , the behaviors are embedded directly in the hardware or on small micro-processors , allowing the
robots to have all on-board computing ( this was unheard of in the processorimpoverished mid-1980 ‚Äô s )  . Furthermore , the robots were the first to be able
114 4 The Reactive Paradigm
Figure 4 . 5 ‚Äú Veteran ‚Äù robots of the MIT AI Laboratory using the subsumption architecture .  ( Photograph courtesy of the MIT Artificial Intelligence Laboratory .  ) 
to walk , avoid collisions , and climb over obstacles without the ‚Äú move-thinkmove-think ‚Äù pauses of Shakey . 
The term ‚Äú behavior ‚Äù in the subsumption architecture has a less precise
meaning than in other architectures . A behavior is a network of sensing and
acting modules which accomplish a task . The modules are augmented finite
state machines AFSM , or finite state machines which have registers , timers , 
and other enhancements to permit them to be interfaced with other modules . 
An AFSM is equivalent to the interface between the schemas and the coordinated control strategy in a behavioral schema . In terms of schema theory , 
a subsumption behavior is actually a collection of one or more schemas into
an abstract behavior . 
Behaviors are released in a stimulus-response way , without an external
program explicitly coordinating and controlling them . Four interesting aspects of subsumption in terms of releasing and control are : 
LAYERS OF 1 . Modules are grouped into layers of competence . The layers reflect a hiCOMPETENCE erarchy of intelligence , or competence . Lower layers encapsulate basic
survival functions such as avoiding collisions , while higher levels create
4 . 3 Subsumption Architecture 115
more goal-directed actions such as mapping . Each of the layers can be
viewed as an abstract behavior for a particular task . 
LAYERS CAN SUBSUME 2 . Modules in a higher layer can override , or subsume , the output from beLOWER LAYERS haviors in the next lower layer . The behavioral layers operate concurrently and independently , so there needs to be a mechanism to handle
potential conflicts . The solution in subsumption is a type of winner-takeall , where the winner is always the higher layer . 
NO INTERNAL STATE 3 . The use of internal state is avoided . Internal state in this case means any
type of local , persistent representation which represents the state of the
world , or a model . Because the robot is a situated agent , most of its information should come directly from the world . If the robot depends on
an internal representation , what it believes may begin to dangerously diverge from reality . Some internal state is needed for releasing behaviors
like being scared or hungry , but good behavioral designs minimize this . 
4 . A task is accomplished by activating the appropriate layer , which then
activates the lower layers below it , and so on . However , in practice , subTASKABLE sumption style systems are not easily taskable , that is , they can ‚Äô t be ordered
to do another task without being reprogrammed . 
4 . 3 . 1 Example
These aspects are best illustrated by an example , extensively modified from
Brooks ‚Äô original paper 27 in order to be consistent with schema theory terminology and to facilitate comparison with a potential fields methodology . A
robot capable of moving forward while not colliding with anything could be
LEVEL 0 : AVOID represented with a single layer , Level 0 . In this example , the robot has multiple sonars ( or other range sensors )  , each pointing in a different direction , 
and two actuators , one for driving forward and one for turning . 
Following Fig . 4 . 6 , the SONAR module reads the sonar ranges , does any
POLAR PLOT filtering of noise , and produces a polar plot . A polar plot represents the range
readings in polar coordinates ,  ( r ;  )  , surrounding the robot . As shown in
Fig . 4 . 7 , the polar plot can be ‚Äú unwound .  ‚Äù 
If the range reading for the sonar facing dead ahead is below a certain
threshold , the COLLIDE module declares a collision and sends the halt signal
to the FORWARD drive actuator . If the robot was moving forward , it now
stops . Meanwhile , the FEELFORCE module is receiving the same polar plot . 
It treats each sonar reading as a repulsive force , which can be represented
116 4 The Reactive Paradigm
polar
plot
force heading
halt
TURN
FORWARD
FEEL
FORCE
RUN
AWAY
SONAR
COLLIDE
heading encoders
Figure 4 . 6 Level 0 in the subsumption architecture . 
0
1
2
3
4
5
6
7
0
sonar number
range reading
1 2 34567
a . b . 
Figure 4 . 7 Polar plot of eight sonar range readings : a .  )  ‚Äú robo-centric ‚Äù view of range
readings along acoustic axes , and b .  ) unrolled into a plot . 
as a vector . Recall that a vector is a mathematical construct that consists of
a magnitude and a direction . FEELFORCE can be thought of as summing
the vectors from each of the sonar readings . This results in a new vector . 
The repulsive vector is then passed to the TURN module . The TURN module
splits off the direction to turn and passes that to the steering actuators . TURN
also passes the vector to the FORWARD module , which uses the magnitude of
the vector to determine the magnitude of the next forward motion ( how far
or how fast )  . So the robot turns and moves a short distance away from the
obstacle . 
The observable behavior is that the robot will sit still if it is in an unoccupied space , until an obstacle comes near it . If the obstacle is on one side of
4 . 3 Subsumption Architecture 117
Figure 4 . 8 Level 0 recast as primitive behaviors . 
the robot , the robot will turn 180
the other way and move forward ; essentially , it runs away . This allows a person to herd the robot around . The robot
can react to an obstacle if the obstacle ( or robot ) is motionless or moving ; the
response is computed at each sensor update . 
However , if part of the obstacle , or another obstacle , is dead ahead ( someone tries to herd the robot into a wall )  , the robot will stop , then apply the
results of RUNAWAY . So it will stop , turn and begin to move forward again . 
Stopping prevents the robot from side-swiping the obstacle while it is turning and moving forward . Level 0 shows how a fairly complex set of actions
can emerge from very simple modules . 
It is helpful to recast the subsumption architecture in the terms used in this
book , as shown in Fig . 4 . 8 . Note how this looks like the vertical decomposition in Fig . 4 . 2 : the sensor data flows through the concurrent behaviors to
the actuators , and the independent behaviors cause the robot to do the right
thing . The SONAR module would be considered a global interface to the sensors , while the TURN and FORWARD modules would be considered part of
the actuators ( an interface )  . For the purposes of this book , a behavior must
consist of a perceptual schema and a motor schema . Perceptual schemas are
connected to a sensor , while motor schemas are connected to actuators . For
Level 0 , the perceptual schemas would be contained in the FEELFORCE and
COLLIDE modules . The motor schemas are RUNAWAY and COLLIDE modules . COLLIDE combines both perceptual processing ( extracts the vector for
the sonar facing directly ahead ) and the pattern of action ( halt if there is a
118 4 The Reactive Paradigm
polar
plot
force
heading
halt
TURN
FORWARD
FEEL
FORCE
RUN
AWAY
SONAR
COLLIDE
heading encoders
WANDER AVOID
heading
force
S
modified
heading
Figure 4 . 9 Level 1 : wander . 
reading )  . The primitive behaviors reflect the two paths through the layer . 
One might be called the runaway behavior and the other the collide behavior . Together , the two behaviors create a rich obstacle avoidance behavior , or
a layer of competence . 
It should also be noticed that the behaviors used direct perception , or affordances . The presence of a range reading indicated there was an obstacle ; 
the robot did not have to know what the obstacle was . 
Consider building a robot which actually wandered around instead of sitting motionless , but was still able to avoid obstacles . Under subsumption , a
LEVEL 1 : WANDER second layer of competence ( Level 1 ) would be added , shown in Fig . 4 . 9 . In
this case , Level 1 consists of a WANDER module which computes a random
heading every n seconds . The random heading can be thought of as a vector . 
It needs to pass this heading to the TURN and FORWARD modules . But it can ‚Äô t
be passed to the TURN module directly . That would sacrifice obstacle avoidance , because TURN only accepts one input . One solution is to add another
module in Level 1 , AVOID , which combines the FEELFORCE vector with the
WANDER vector . Adding a new avoid module offers an opportunity to create
a more sophisticated response to obstacles . AVOID combines the direction of
the force of avoidance with the desired heading . This results in the actual
heading being mostly in the right direction rather than having the robot turn
4 . 3 Subsumption Architecture 119
Figure 4 . 10 Level 1 recast as primitive behaviors . 
around and lose forward progress .  ( Notice also that the AVOID module was
able to ‚Äú eavesdrop ‚Äù on components of the next lower layer .  ) The heading
output from AVOID has the same representation as the output of RUNAWAY , 
so TURN can accept from either source . 
The issue now appears to be when to accept the heading vector from which
layer . Subsumption makes it simple : the output from the higher level subsumes the output from the lower level . Subsumption is done in one of two
ways : 
INHIBITION 1 . inhibition . In inhibition , the output of the subsuming module is connected
to the output of another module . If the output of the subsuming module
is ‚Äú on ‚Äù or has any value , the output of the subsumed module is blocked
or turned ‚Äú off .  ‚Äù Inhibition acts like a faucet , turning an output stream on
and off . 
SUPPRESSION 2 . suppression . In suppression , the output of of the subsuming module is
connected to the input of another module . If the output of the subsuming module is on , it replaces the normal input to the subsumed module . 
Suppression acts like a switch , swapping one input stream for another . 
In this case , the AVOID module suppresses ( marked in the diagram with
a S ) the output from RUNAWAY . RUNAWAY is still executing , but its output
doesn ‚Äô t go anywhere . Instead , the output from AVOID goes to TURN . 
120 4 The Reactive Paradigm
The use of layers and subsumption allows new layers to be built on top
of less competent layers , without modifying the lower layers . This is good
software engineering , facilitating modularity and simplifying testing . It also
adds some robustness in that if something should disable the Level 1 behaviors , Level 0 might remain intact . The robot would at least be able to preserve
its self-defense mechanism of fleeing from approaching obstacles . 
Fig . 4 . 10 shows Level 1 recast as behaviors . Note that FEELFORCE was
used by both RUNAWAY and AVOID . FEELFORCE is the perceptual component 
( or schema ) of both behaviors , with the AVOID and RUNAWAY modules being
the motor component ( or schema )  . As is often the case , behaviors are usually named after the observable action . This means that the behavior ( which
consists of perception and action ) and the action component have the same
name . The figure does not show that the AVOID and RUNAWAY behaviors
share the same FEELFORCE perceptual schema . As will be seen in the next
chapter , the object-oriented properties of schema theory facilitate the reuse
and sharing of perceptual and motor components . 
LEVEL 2 : FOLLOW Now consider adding a third layer to permit the robot to move down corCORRIDORS ridors , as shown in Fig . 4 . 11 .  ( The third layer in Brooks ‚Äô original paper is 
 ‚Äú explore ,  ‚Äù because he was considering a mapping task .  ) The LOOK module examines the sonar polar plot and identifies a corridor .  ( Note that this
is another example of behaviors sharing the same sensor data but using it
locally for different purposes .  ) Because identifying a corridor is more computationally expensive than just extracting range data , LOOK may take longer
to run than behaviors at lower levels . LOOK passes the vector representing
the direction to the middle of the corridor to the STAYINMIDDLE module . 
STAYINMIDDLE subsumes the WANDER module and delivers its output to
the AVOID module which can then swerve around obstacles . 
But how does the robot get back on course if the LOOK module has not
computed a new direction ? In this case , the INTEGRATE module has been
observing the robots actual motions from shaft encoders in the actuators . 
This gives an estimate of how far off course the robot has traveled since the
last update by LOOK . STAYINMIDDLE can use the dead reckoning data with
the intended course to compute the new course vector . It serves to fill in
the gaps in mismatches between updates rates of the different modules . Notice that LOOK and STAYINMIDDLE are quite sophisticated from a software
perspective . 
INTEGRATE is an example of a module which is supplying a dangerous
internal state : it is actually substituting for feedback from the real world . If
for some reason , the LOOK module never updates , then the robot could op-
4 . 3 Subsumption Architecture 121
polar
plot
force
heading
halt
TURN
FORWARD
FEEL
FORCE
RUN
AWAY
SONAR
COLLIDE
heading encoders
WANDER AVOID
force
S
modified
heading
LOOK STAYIN
MIDDLE
corridor
heading
to middle
S
INTEGRATE
distance , direction traveled
Figure 4 . 11 Level 2 : follow corridors . 
erate without any sensor data forever . Or at least until it crashed ! Therefore , 
subsumption style systems include time constants on suppression and inhibition . If the suppression from STAYINMIDDLE ran for longer than n seconds
with out a new update , the suppression would cease . The robot would then
begin to wander , and hopefully whatever problem ( like the corridor being
totally blocked ) that had led to the loss of signal would fix itself . 
Of course , a new problem is how does the robot know that it hasn ‚Äô t started
going down the hallway it just came up ? Answer : it doesn ‚Äô t . The design
assumes that that a corridor will always be present in the robot ‚Äô s ecological
niche . If it ‚Äô s not , the robot does not behave as intended . This is an example
of the connotation that reactive systems are ‚Äú memory-less .  ‚Äù 
4 . 3 . 2 Subsumption summary
To summarize subsumption :  Subsumption has a loose definition of behavior as a tight coupling of sensing and acting . Although it is not a schema-theoretic architecture , it can
122 4 The Reactive Paradigm
be described in those terms . It groups schema-like modules into layers of
competence , or abstract behaviors .  Higher layers may subsume and inhibit behaviors in lower layers , but behaviors in lower layers are never rewritten or replaced . From a programming standpoint , this may seem strange . However , it mimics biological
evolution . Recall that the fleeing behavior in frogs ( Ch . 3 ) was actually the
result of two behaviors , one which always moved toward moving objects
and the other which actually suppressed that behavior when the object
was large .  The design of layers and component behaviors for a subsumption implementation , as with all behavioral design , is hard ; it is more of an art than
a science . This is also true for all reactive architectures .  There is nothing resembling a STRIPS-like plan in subsumption . Instead , 
behaviors are released by the presence of stimulus in the environment .  Subsumption solves the frame problem by eliminating the need to model
the world . It also doesn ‚Äô t have to worry about the open world being
non-monotonic and having some sort of truth maintenance mechanism , 
because the behaviors do not remember the past . There may be some
perceptual persistence leading to a fixed-action pattern type of behavior 
( e . g .  , corridor following )  , but there is no mechanism which monitors for
changes in the environment . The behaviors simply respond to whatever
stimulus is in the environment .  Perception is largely direct , using affordances . The releaser for a behavior
is almost always the percept for guiding the motor schema .  Perception is ego-centric and distributed . In the wander ( layer 2 ) example , the sonar polar plot was relative to the robot . A new polar plot was
created with each update of the sensors . The polar plot was also available to any process which needed it ( shared global memory )  , allowing
user modules to be distributed . Output from perceptual schemas can be
shared with other layers . 
4 . 4 Potential Fields Methodologies
Another style of reactive architecture is based on potential fields . The specific architectures that use some type of potential fields are too numerous to
4 . 4 Potential Fields Methodologies 123
describe here , so instead a generalization will be presented . Potential field
VECTORS styles of behaviors always use vectors to represent behaviors and vector sumVECTOR SUMMATION mation to combine vectors from different behaviors to produce an emergent
behavior . 
4 . 4 . 1 Visualizing potential fields
The first tenet of a potential fields architecture is that the motor action of a
behavior must be represented as a potential field . A potential field is an array , 
or field , of vectors . As described earlier , a vector is a mathematical construct
which consists of a magnitude and a direction . Vectors are often used to
represent a force of some sort . They are typically drawn as an arrow , where
the length of the arrow is the magnitude of the force and the angle of the
arrow is the direction . Vectors are usually represented with a boldface capital
letter , for example , V . A vector can also be written as a tuple ( m ; d )  , where m
stands for magnitude and d for direction . By convention the magnitude is a
real number between 0 . 0 and 1 , but the magnitude can be any real number . 
ARRAY REPRESENTING The array represents a region of space . In most robotic applications , the
A FIELD space is in two dimensions , representing a bird ‚Äô s eye view of the world just
like a map . The map can be divided into squares , creating a ( x , y ) grid . Each
element of the array represents a square of space . Perceivable objects in the
world exert a force field on the surrounding space . The force field is analogous to a magnetic or gravitation field . The robot can be thought of as a
particle that has entered the field exuded by an object or environment . The
vector in each element represents the force , both the direction to turn and the
magnitude or velocity to head in that direction , a robot would feel if it were
at that particular spot . Potential fields are continuous because it doesn ‚Äô t matter how small the element is ; at each point in space , there is an associated
vector . 
Fig . 4 . 12 shows how an obstacle would exert a field on the robot and make
it run away . If the robot is close to the obstacle , say within 5 meters , it is inside
the potential field and will fell a force that makes it want to face directly away
from the obstacle ( if it isn ‚Äô t already ) and move away . If the robot is not within
range of the obstacle , it just sits there because there is no force on it . Notice
that the field represents what the robot should do ( the motor schema ) based
on if the robot perceives an obstacle ( the perceptual schema )  . The field isn ‚Äô t
concerned with how the robot came to be so close to the obstacle ; the robot
feels the same force if it were happening to move within range or if it was
just sitting there and someone put their hand next to the robot . 
124 4 The Reactive Paradigm
Figure 4 . 12 Example of an obstacle exerting a repulsive potential field over the radius of 1 meter . 
One way of thinking about potential fields is to imagine a force field acting
on the robot . Another way is to think of them as a potential energy surface
in three dimensions ( gravity is often represented this way ) and the robot as
a marble . In that case , the vector indicates the direction the robot would 
 ‚Äú roll ‚Äù on the surface . Hills in the surface cause the robot to roll away or
around ( vectors would be pointing away from the ‚Äú peak ‚Äù of the hill )  , and
valleys would cause the robot to roll downward ( vectors pointing toward
the bottom )  . 
FIVE PRIMITIVE FIELDS There are five basic potential fields , or primitives , which can be combined
to build more complex fields : uniform , perpendicular , attractive , repulsive , and
UNIFORM FIELD tangential . Fig . 4 . 13 shows a uniform field . In a uniform field , the robot would
feel the same force no matter where it was . No matter where it got set down
and at what orientation , it would feel a need to turn to align itself to the
direction the arrow points and to move in that direction at a velocity propor-
4 . 4 Potential Fields Methodologies 125
Figure 4 . 13 Five primitive potential fields : a .  ) uniform , b .  ) perpendicular , c .  ) attraction , d .  ) repulsion , and e .  ) tangential . 
tional to the length of the arrow . A uniform field is often used to capture the
behavior of ‚Äú go in direction n 
.  ‚Äù 
PERPENDICULAR FIELD Fig . 4 . 13b shows a perpendicular field , where the robot is oriented perpendicular to some object or wall or border The field shown is directed away
from the gray wall , but a perpendicular field can be pointed towards an object as well . 
ATTRACTIVE FIELD Fig . 4 . 13c illustrates an attractive field . The circle at the center of the field
represents an object that is exerting an attraction on the robot . Wherever the
robot is , the robot will ‚Äú feel ‚Äù a force relative to the object . Attractive fields
are useful for representing a taxis or tropism , where the agent is literally
attracted to light or food or a goal . The opposite of an attractive field is a repulsive field , shown in Fig . 4 . 13d . Repulsive fields are commonly associated
with obstacles , or things the agent should avoid . The closer the robot is to
the object , the stronger the repulsive force 180
away from it . 
TANGENTIAL FIELD The final primitive field is the tangential field in Fig . 4 . 13e . The field is a
tangent around the object ( think of a tangent vector as being perpendicular to
126 4 The Reactive Paradigm
radial lines extending outward from the object )  . Tangential fields can ‚Äú spin ‚Äù 
either clockwise or counterclockwise ; Fig . 4 . 13 shows a clockwise spin . They
are useful for directing a robot to go around an obstacle , or having a robot
investigate something . 
4 . 4 . 2 Magnitude profiles
Notice that in Fig . 4 . 13 , the length of the arrows gets smaller closer to the
object . The way the magnitude of vectors in the field change is called the
MAGNITUDE PROFILE magnitude profile .  ( The term ‚Äú magnitude profile ‚Äù is used here because the
term ‚Äú velocity profile ‚Äù is used by control engineers to describe how a robot ‚Äô s
motors actually accelerate and decelerate to produce a particular movement
without jerking .  ) 
Consider the repulsive field in Fig . 4 . 12 . Mathematically , the field can be
represented with polar coordinates and the center of the field being the origin 
( 0 , 0 )  : 