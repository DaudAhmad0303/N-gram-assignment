Foreword
Th ere are some key factors driving the increasing adoption of augmented reality 
(AR) and virtual reality (VR) technologies, which depend mainly on the growing integration of technology and digitization in the fi eld of healthcare, as well as 
increasing healthcare expenditures which focus on delivery of effi cient health services and its signifi cance in training healthcare professionals. Th e advanced technologies related to AR and VR have a great eff ect on the healthcare industry with 
their adoption in virtual training of surgeons in 3D operating room simulations 
for diffi cult surgeries and as phobia buster in mental health treatment as well as for 
chronic pain management. Also, VR plays a major role in eye movement desensitization and reprocessing (EMDR) therapy to enable reframing of traumatic memories through certain eye movements. Furthermore, this technology off ers benefi ts 
in various areas of care management such as autism and depression therapy, cancer 
therapy, and assisted living. VR-based organ models have played a crucial part in 
preparing surgeons for delicate and complicated operations that demand greater 
precision, less complications, and reduced trauma. On the other hand, AR is considered a useful active and powerful tool for training and education. AR-based 
applications are eff ectively used to provide the improved care of many patients. 
For example, the vein visualization technology, developed by AccuVein Inc. was 
developed to handle scanning, which helps doctors and nurses successfully locate 
veins and valves at the fi rst go, reducing pain and the required time. Th ese applications are also used in the aft ercare of patients and assist elderly people in managing 
their medications. Th is book focuses on adopting robots in conjunction with VR 
and AR to help in healthcare and medicine applications; for instance, we discuss 
a training system developed for a lower limb rehabilitation robot based on virtual 
reality (VR), mainly including trajectory planning and VR control strategy. It can 
simulate bike riding and encourages patients to join in their recovery and rehabilitation through a built-in competitive game. Th e robot could achieve linear trajectory, circle trajectory and arbitrary trajectory based on speed control, in which the 
training velocity and acceleration in the trajectory planning have been simulated. 
A human-machine dynamics equation was built which is used to judge the intent 
of a patient’s movement. Th e VR training mode is a variable speed active training 
under the constraint trajectory, and it has an adapting training posture function 
which can provide an individual riding training track according to the leg length 
of patients. Th e movement synchronization between the robot and virtual model 
was achieved by interaction control strategy, and the robot can change the training 
velocity based on the signal from feedback terrains in the game. A serious game 
xxii Foreword
about a bike match in a forest was designed in which the user can select the training 
level as well as change perspective through the user interface.
Th e main purpose of this book is to publish the best papers submitted to the special session on VR/AR Healthcare and Medicine Applications at the International 
Conference on Communication, Management and Information Technology 
(ICCMIT 2018) in Madrid, Spain.1
 ICCMIT 2018 is an annual meeting for scientists, engineers and academicians to discuss the latest discoveries and realizations 
in the foundations, theory, models and applications of nature-inspired systems, 
and emerging areas related to the three tracks of the conference covering all aspects 
of communication, engineering, management, and information technology given 
by panels made up of world-class speakers and at workshops.
Prof. Ibrahiem El Emary
Prof. Musbah J. Aqel
International Cyprus University
1
 http://www.iccmit.net/
xxiii
Preface
With the current advances in technology innovation, the fi eld of medicine and 
healthcare is rapidly expanding and, as a result, many diff erent areas of human 
health diagnostics, treatment and care are emerging. Wireless technology is getting 
faster and 5G mobile technology allows the Internet of Medical Th ings (IoMT) to 
greatly improve patient care and more eff ectively prevent illness from developing. 
Th is book provides an overview and review of the current and anticipated changes 
in medicine and healthcare due to new technologies and faster communication 
between users and devices. In Chapter 1, Abdullah et al. review the implications 
of VR and AR healthcare applications, and Chapter 5 provides a review of current 
augmenting dental care, by Nayyar and Nguyen. Chapter 6 provides an overview of 
typical human-computer interaction (HCI) informed empirical experiments and 
psychophysiological measurement tools that can help inform the development of 
user interface designs and novel ways to evaluate human behavior to responses in 
virtual reality (VR) and with VR and other new technologies by Munoz et al. In 
Chapter 12, Puri and Tromp provide provide a review of telemedicine technologies.
Patient Empowerment
Patient empowerment is facilitated by the wide availability of medical information via the internet and the ability to share reliable medical information, personal 
experiences with medicines and medical assessments via social media, in social 
groups established based on shared interests and a desire to support each other. 
Th is enables patients to have a voice in their healthcare procedures and exert more 
control and infl uence on healthcare worldwide, making it a very powerful technology-enabled medicine and healthcare improvement. Th is internationally accessible 
crowd sourced medicine and healthcare resource has the potential to change the 
role of patients from being passive witnesses in their own treatment to informed 
citizens proactively involved in monitoring and choosing treatments.
Th e e-NABLING Future project is a great example of patient empowerment. It 
is a global network of volunteers that share 3D printing designs and instructions to 
create prosthetic hands for free, thus enabling people in underdeveloped countries 
who have no access to prosthetics make their own at low cost. Medical 3D printing 
is still in its infancy; however, 3D bio-printers are already commercially available, 
making the printing of human body parts from bio-ink containing real human cells 
a commonplace occurrence in the near future.
xxiv Preface
Chapter 3 describes various technologies that enable patient empowerment and 
build empathy in young children using AR, as shown in a case study in Malaysia 
by Zamin et al. In Chapter 4, Garcia et al. report on the empirical experiments 
used to test the eff ectiveness of VR for mock interview training. In Chapter 7, AI 
technologies for mobile stroke monitoring and rehabilitation robotics control are 
discussed by Elbagoury et al. In Chapter 9, Elbagoury et al. discuss an AI-powered 
“doctor brain” app, along with artifi cial intelligence (AI) for healthcare based on the 
Internet of Th ings (IoT). In Chapter 10, an artifi cial intelligence mobile cloud computing tool is discussed by Shalhoub et al., and the previously mentioned Chapters 
1 and 3 also include discussions on patient empowerment through new technologies for medicine and healthcare.
Smart Wearable Sensors
Smart wearable home sensor technologies contribute to the empowerment of 
patients. Th ese technologies, such as the popular Fitbit, give users more insight 
and control over their health and can help prevent illness by giving real-time feedback on health status by monitoring vital signs, allowing the user to adjust and 
target their activities to reach optimal fi tness or health results. In Chapter 14, Kolhe 
et al. discuss smart wearable sensors, along with automation of appliances using 
electroencephalography. In Chapter 18, Kolhe et al. discuss smart home personal 
assistants and baby monitoring systems, previously mentioned in Chapters 1, 3, 6, 
8 and 15.
Real-time health feedback is extremely suitable for gamification, as behavioral change and motivation regarding exercise can be influenced by adding 
points and badges and leader-boards to the data stored in the cloud and on 
the device. These wearable sensors are becoming smaller, less obtrusive and 
more integrated with the human body. For instance, Google’s digital contact 
lens will allow diabetes patients to monitor and manage their glucose levels 
from tears in real time.
Additional integration can be expected from digestible sensors, sensors placed 
in teeth and organs of the body and thin e-skin sensors or biometric tattoos and 
radio frequency identifi cation chips (RFID) implanted under the skin, which store 
vital health information and act as control devices for purposes such as automatically calling for assistance if vital signs signify that health problems are imminent. 
Early adopters of these new technologies are already using implants to give themselves superpowers; for instance, the use of recreational cyborgs to improve their 
eyesight or hearing.
Medicine and Healthcare Education
Another area that will benefi t greatly from technological advances is medicine and 
healthcare education. Medical students can now learn anatomy and practice operations in virtual reality, allowing them to interact with the human models in real 
Preface xxv
time and zoom in and out to focus on the details, in a way that has not been possible before. In Chapter 2, Le et al. discuss the use of 3D simulation in medical 
education, in which VR is used in extensive user (students and teachers) acceptance 
comparative testing for teaching anatomy. Additionally, augmented reality can help 
to provide real time instructions and visualizations, as discussed in the previously 
mentioned Chapter 5, such as the Microsoft Hololens app for use in the OR, showing where the blood veins are located in a body part. With the use of 360 degree 
video cameras, anyone can observe operations in progress in real time.
Artifi cial Intelligence
Artifi cial intelligence (AI) will be able to assist doctors in medical decision making. 
Th e IBM Watson computer system has already shown great potential in helping to 
analyze symptoms and prescribe the best treatment (for more details see https://
www.ibm.com/watson/). Watson can read 40 million documents in 15 seconds and 
suggests treatments based on the analysis. Watson will not replace human doctors 
because it does not answer medical questions, instead it analyzes medical information and comes up with the most relevant potential outcomes that can help them 
make the most informed decisions in the shortest amount of time. In Chapter 8, 
Shalhoub et al. address the topic of artifi cial intelligence for smart cancer diagnosis. and in Chapter 10 Shalhoub et al. discuss an artifi cial intelligence mobilecloud computing tool. Ionel-Alexandru et al. discuss advanced intelligent robot 
control interfaces for VR simulation in Chapter 11, and along with the previously 
mentioned Chapter 8 AI topics relevant for innovation of medicine and healthcare 
technologies.
Google’s DeepMind Health mines data from medical records with the aim of 
improving health services by making them faster, more accurate and more effi cient. 
It has the potential to be bigger than the Human Genome Project. Google is also 
working on the ultimate artifi cial intelligence-controlled brain under the supervision of Ray Kurzweil, director of engineering at Google. He predicts that the singularity (the moment when artifi cial intelligence exceeds man’s intellectual capacity) 
will only take about 10 years of further development. It will allow us to connect our 
neocortex to the internet and develop our creativity.
Artifi cial intelligence also drives medical robot assistants that will be of great 
use in care homes and hospitals and even for home care. Robots can be made to lift 
more weight than humans and have already been developed to assist in carrying 
medical equipment and patients, helping patients get out of bed into their wheelchairs, etc. More complex robots equipped with image analysis techniques are 
under development to help with more complex tasks. In Chapter 13, Migdalovici 
et al. discuss an environment model applied on the critical position of the walking 
robots. and in Chapter 14, Pop et al. discuss walking robot equilibrium recovery 
applied on the NAO robot. In Chapter 15, Zamin et al. discuss the development of 
a robotic teaching aid for disabled children in Malaysia; and the previously mentioned Chapters 1, 3, 6 and 10 discuss various applications of robotics in medicine 
and healthcare innovation.
xxvi Preface
Real-Time Diagnostics
Real-time diagnostics tools will provide technological advances and new application areas, and help reduce the complexity of medical procedures and analysis, such 
as, for instance, the iKnife, an intelligent surgical knife that can identify malignant 
tissue to remove as the operation is in progress.
Other New Technologies in the Technology Innovation fi elds 
for Medicine and Healthcare
In order to complete the overview of current predictions, we discuss a few more 
new technologies that are expected to revolutionize the medicine and healthcare 
industries and services. Th e technology advancements discussed here are in-silico 
organs-on-chips technology, optogenetics and multifunctional radiology. Finally, 
we discuss some of the perceived risks and dangers that need to be considered 
before adopting some of these new technologies into our medicine and healthcare 
treatments.
A huge advance in clinical trials is predicted from the in-silico organs-on-chips 
technology. Microchips simulate cells and whole human organs and systems, so 
that drugs can be tested without risk to human or animal subjects, making clinical 
trials more effi cient and accurate. Th e Human Genome Project which mapped all 
the human genes, generating the fi eld of genomics, makes it possible to use DNA 
analysis to customize health procedures and medicines. Th e Personalized Medicine 
Coalition aims to help bring about the paradigm shift to personalized healthcare 
(see their latest report1
).
Optogenetics is a promising new technique used in neuroscience. It uses genes 
of proteins that are sensitive to light. Th ese are then used to precisely monitor and 
control their activity by using light signals aft er introducing them in specifi c brain 
cells. Th is allows researchers to control how nerve cells communicate in real time, 
with completely wireless techniques so that complex behaviors can be observed 
while the experimental subjects can freely move around. Th is technology will be 
very helpful in understanding the neural codes for psychiatric and neurological 
disorders.
Multifunctional radiology is developing very fast and within the next 10 years 
great progress can be expected from this technology advancement. Radiology uses 
medical imaging to diagnose and sometimes also treat diseases within the body. 
Multifunctional radiology consists of one machine that can detect many diff erent 
medical problems at once. Th is will make practitioners more productive and one 
machine will take up less space than multiple devices, making the workspace more 
effi cient.
Th e most profound risks regarding the adoption of the Internet of Medical 
Th ings (IoMT) are the fi nances and ability to adapt to the changing healthcare and 
medicine industry itself, in addition to all the other institutions that need to adopt 
1
 http://www.personalizedmedicinecoalition.org /Userfi les/PMC-Corporate/fi le/Th e PM 
Report.pdf
Preface xxvii
these new technologies. Th is also includes the fi nances for the implementation of 
new regulations. As new technologies are used for medicine and healthcare, governments will have to keep up with the change, by providing the best regulations 
for these new services to the public. Th is requires signifi cant resources from multiple regulatory bodies and governments.
Another problem is caused by the diversity in medical record keeping technologies, and the lack of compatibility and interoperability between the diff erent systems used by institutions. If data cannot be shared effi ciently, it cannot be merged 
and aggregated for improvement of information exchange and patient record sharing between the diff erent medical experts the patient may have to deal with. Th is 
can signifi cantly slow down the progress of big data analysis and communications 
between institutions with diff erent or incompatible database designs.
Major demographic shift s are taking place in the populations around the world. 
Populations are growing and aging and the number of patient cases are rising as a 
result, which drives the costs of healthcare up. If current trends persist there will 
be nearly 1.5 billion people ages 65 or older by 2050 and they will signifi cantly 
outnumber children younger than 5. It is projected that more than 60% of the Baby 
Boomer generation will be managing more than one chronic condition by 2030. 
Our medicine and healthcare systems need to help these patients by managing 
the increased cost of healthcare, as they are expected to make twice as many visits to physicians and hospitals by 2030. With improved healthcare, life expectancy 
is increasing, and while the prevalence of severe disabilities can be expected to 
decrease along with this improvement, milder chronic diseases and the need for 
solutions, such as remote disease management, engagement and patient responsibility for monitoring their own symptoms and treatments, will increase.
Dr. Jolanda G. Tromp
University of New York in Oswego
New York, USA
xxix
Acknowledgments
First of all, I would like to thank the authors for contributing their excellent chapters 
to this book. Without their contributions, this book would not have been possible. 
Th anks to all my friends for sharing my happiness at the start of this project and following up with their encouragement when it seemed too diffi cult to completed.
I would like to acknowledge and thank the most important people in my life, 
my grandfather, grandmother, and fi nally thanks to my wife. Th is book has been a 
long-cherished dream of mine which would not have been turned into reality without the support and love of these amazing people, who encouraged me despite my 
not giving them the proper time and attention. I am also grateful to my best friends 
for their blessings and unconditional love, patience and encouragement.
Dr. Dac-Nhuong Le
Deputy-Head, Faculty of Information Technology
Haiphong University, Haiphong, Vietnam
xxxi
Acronyms
AAL Ambient Assistive Learning
ADR Adverse Drug Reaction
AI Artifi cial Intelligence
ANN Artifi cial Neural Network
ANS Autonomic Nervous System
API Application Program Interface
AR Augmented Reality
ASQ Aft er Scenario Questionnaire
ASD Autism Spectrum Disorder
ATA American Telemedicine Association
BAN Body Area Network
BCI Brain Computer Interface
BoS Boundary of Support
BP Blood Pressure
CAD/CAM Computer-Aided Design/Computer-Aided Manufacturing
CAVE Cave Automatic VEs
CHI Child Health Information
CTA Computed Tomographic Angiography
CV Consumer Version
CBR Case-based Reasoning
CLR Common Language Runtime
CW Cognitive Walkthrough
DC Direct Current
DSS Decision Support System
DWT Discrete Wavelet Transform
ECG Electrocardiogram
ECoG ElectroCorticoGram
ECP Embedded Context Prediction
EDA Electrodermal Activity
EDD Empathy Defi cit Disorder
EEG Electroencephalography
EER Energy Effi ciency Ratio
EGC Embedded Gateway Confi guration
EKG Electrocardiography
EMG Electromyography
EMDR Eye Movement Desensitization and Reprocessing
EMR Electronics Medical Records
xxxii Acronyms
EZW Embedded Zero Tree Wavelet
FCM Fuzzy C-means
fMRI Functional Magnetic Resonance Imaging
HMD Head Mounted Display
HHU Hospital in Home Unit
HCV Hepatitis C Virus
HR Heart Rate
HRV Heart Rate Variability
HE Heuristic Evaluation
ICT Information and Communication Technologies
IC Integrated Circuit
ICU Intensive Care Unit
IR Industrial Revolution
IC Integrated Circuit
IIT-D Indian Institute of Technology-Delhi
IEH Indirect Emergency Service
IoT Internet of Th ings
JIST Job Interview Simulation Training
GPS Global Positioning System
GND Ground
GSR Galvanic Skin Response
HR Heart Rate
HRV Heart Rate Variability
KIT Keep-in-Touch
LQR Linear Quadratic Gaussian
LED Light Emitting Diode
LDR Light Detector
MLP Multilayer Perceptron
MIME Multipurpose Internet Mail Extensions
MEG Magnetoencephalography
MEMS Micro Electro Mechanical System
ManMos Mandibular Motion Tracking System
NodeMCU Node Microcontroller Unit
NN Neural Network
PTSD Post-Traumatic Stress Disorder
PCA Principal Component Analysis
PSO Particle Swarm Optimization
PID Proportional Integral Derivative
PIR Passive Infra-Red
PSSUQ Post-Study System Usability Questionnaire
RBF Radial Basis Kernel function
RF Radio Frequency
RPA Robot Process Automation
RST Reset
ROI Region of Interest
RMS Root Mean Square
SCR Skin Conductance Response
Acronyms xxxiii
SCL Skin Conductance Level
SMS Short Message Services
SMA Semantic Medical Access
STT Speech To Text
SVM Support Vector Machine
SOA Service-Oriented Architecture
SOAP Simple Object Access Protocol
SUS System Usability Scale
TTS Text to Speech
UAT User Acceptance Test
URL Uniform Resource Locator
VCC Voltage Common Collector
VHA Veteran Health Administration
VR-JIT Virtual Reality Job Interviews Training
VR Virtual Reality
VE Virtual Environment
WBA Web Browser Automation
WDA Wearable Devices Access
WLAN Wireless LAN
WCF Windows Communication Foundation
XLM Extensible Markup Language
ZMP Zero Momentum Point
Part I
VIRTUAL REALITY,
AUGMENTED REALITY
TECHNOLOGIES AND
APPLICATIONS FOR HEALTH
AND MEDICINE
Dac-Nhuong Le et al. (eds.), Emerging Technologies for Health and Medicine, (1–284) 
© 2018 Scrivener Publishing LLC
3
Dac-Nhuong Le et al. (eds.), Emerging Technologies for Health and Medicine, (3–284) 
© 2018 Scrivener Publishing LLC
CHAPTER 1
REVIEWS OF THE IMPLICATIONS OF
VR/AR HEALTH CARE APPLICATIONS IN
TERMS OF ORGANIZATIONAL AND
SOCIETAL CHANGE
Muhammad Sharif1, Ghulam Jillani Ansari,1, Mussarat Yasmin1, Steven
Lawrence Fernandes2
1 Department of Computer Science, COMSATS University Islamabad, Wah Campus
2 Department of Electronics and Communication Engineering, Sahyadri College of Engineering &
Management, Mangaluru, India
Emails: muhammadsharifmalik@yahoo.com, steven.ec@sahyadri.edu.in
Abstract
Recently it has been observed that computer related applications are vigorously available to support training and learning of health care professionals being involved in diversified organizations to put into practice an impactful change in society. Virtual Reality
(VR) and Augmented Reality (AR) are swiftly becoming progressively more available, accessible and most importantly within an individual reach. Thus, services and applications
related to health care certainly improve the use of medical data. This will result in exploring new health care opportunities not only in the organizations but cover the whole society
for auxiliary transformation and enhancement. Furthermore, combination of VR/AR technologies with Artificial Intelligence (AI) and Internet of Things (IoT) will present powerful
and mainly unexplored application areas that will revolutionize health care and medicine
practice. Hence, the aim of this systematic review is to implicate to which extent VR/AR
health care services and applications are presently used to genuinely support for organizational and societal change.
Keywords: Health Care, Virtual Reality, Augmented Reality, Immersion, AI, IoT
4 Emerging Technologies for Health and Medicine
1.1 Introduction
Advances in technology directly affect our lives and behaviors. On one hand, it enhances
our learning abilities effectively when integrated with our curriculum to alleviate submissive experiences of lectures for large number of students in the class. On the other hand, it
acts as a tool for students to gain knowledge in a meaningful environment [52]. Therefore,
the goal is to create a powerful interactive learning environment for students where they can
use their inborn capabilities of learning to clutch intricate notions and acquire knowledge
through participation, observation and simulation [28]. The term student is taken generally
in this chapter for medical students, doctors, and various types of medical professionals,
medical trainers and all those who are directly or indirectly using health care services and
applications in society or in any organization for learning and training purposes.
Simulation technology is now being increasingly used to improve the student’s learning
abilities in a variety of domains like marketing, engineering, education and most importantly in health care which is one of the biggest adopters of VR/AR like simulated surgery,
treatment of phobia, robotic based surgery, skill based training, dentistry and disabled
treatment are some of the examples. It is generally recognized that VR and AR are in
the forefront having strong potential to lead health care for impactful change in society
[10]. Nowadays, this can be possible with the development and provision of multimedia
information delivery tools such as apps, pod-casts, medical and educational software and
screen casts which can be easily used on personal computers and mobile devices specifically on smart phones [30, 51]. In addition, numerous visualization technologies have been
released such as Oculus Rift, Gear VR and Head Mounted Displays (HMD) to incorporate
VR in giving intuitive feeling of actually being engrossed in the simulated world [21]. AR
is also known as mixed reality that has lined a possibility to understand the concepts in a
novel way which was not ever possible in the past.
Figure 1.1 (a) Example of Virtual Reality [10], (b) Example of Augmented Reality Training in
Health care [4]
VR and AR can be viewed in Figure 1.1. Although there is a very slight difference
between both concepts which will be discussed later in the chapter, but today, AR (mixed
reality) shown in Figure 1.2, supersedes VR because of the collaboration of real world and
virtual objects rather than the whole computer generated virtual world. Therefore, VR is
now being transformed into AR in the near future gradually. Taking the advantage, we
schematically unfold rest of the chapter by briefly discussing VR/AR, their formats and
design elements, relationship among presence, reality and realism in context with VR/AR,
Reviews of the Implications of VR/AR Health Care Applications 5
features of VR/AR technologies and in detail the implications of VR/AR applications related to enhance health care issues using AI and IoT for impactful change in the society
and organization. To end this chapter, challenges and limitations of the technology along
with conclusion are finally discussed.
Figure 1.2 Relationship of Real and Virtual Environment (Augmented Reality or Mixed Reality)
[15]
1.2 Virtual Reality and Augmented Reality
The following section will explain VR/AR and how both differ from each other. Further,
Table 1.1 will summarize standard terms and definitions related to the simulating environment.
Table 1.1 Standard terms in Virtual Reality and Augmented Reality
1.2.1 Virtual Realty
Virtual revolution has emerged to impart VR simulation technology for clinical and medical purposes since 1995. Although VR has been emerged since 1950s when Morton Heiling invented Sensorama which enabled users to watch television in three dimensional ways.
Today, technology advances in the areas of power, image processing and acquisition, computer vision, graphics, speed, interface technology, HMD devices, a variety of software,
6 Emerging Technologies for Health and Medicine
body tracking, AI and IoT have given rise to build cost effective and functional VR applications capable of operating on mobile devices and/or on personal computers [61].
In context, VR states: It typically refers to use interactive simulations created by computer software and hardware to engage users with an opportunity in an environment that
generates feelings similar to real world events and objects [71]. In another definition, VR is
interpreted as: VR systems are deployed in the form of concert to perform sensory fantasy
or illusion that construct more or less believable simulation of reality [12]. Comprehensively, VR can be defined as a way to replicate real life situations using immersive learning
environment, high visualization and three dimensional characteristics by involving physical or other interfaces like motion sensors, haptic devices and head mounted display in
addition to computer mouse, keyboard, voice and speech recognition. In general, the user
interacts and feels that it is real world but the focus of interaction is digital environment
[52].
Hence, VR systems have been widely applied in phobia, neuroscience, rehabilitation,
disorders and different forms of therapeutic issues for students learning and health care to
uplift the society in productive manners incorporating serious games and other techniques
[14].
1.2.2 Augmented Reality or Mixed Reality
AR is a subset of VR (not a complete virtual reality) that overlays digitized computer
generated information on objects, places and entities from real world for the purpose of
enhancing the learning experience of user. Therefore, its ability to combine physical elements and virtual objects makes it popular in studying and implementing health care,
medicine, fashion and several other fields since 2008 [11]. According to Moro, et al.,
2017, AR is a form of VR in which a synthetic stimuli is super imposed on real world objects to enhance user learning experience with the help of head mounted display, wearable
computers (displays projection onto humans and mannequins) and overlays of computer
screen. The result of AR is to focus interaction in performing tasks within the real world
instead of digital world.
In short, AR is a set of technologies which help to integrate digital and real. Although
there are many flavors and versions of implementing AR but common among all are computers, displays, input devices (especially pointing device of any sort) and tracking mechanism. Merely, displays are required for the user to distinguish between realities and digitally supplied information. Pointing device (input device that must have GPS or some location based services for locating device and of course the user as well) like smart phones,
wireless wrist bands etc. are needed to make sure that the digital information is appropriately placed or aligned with what the user is seeing (tracking). Finally computer software
must exist to manage and run the application.
1.2.3 Line of Difference between VR/AR
The definitions above clarify that everything is virtual and digital or simulation of reality
in VR whereas AR exhibits virtual learning experiences embedded in a physical context. It
means AR is a process of overlaying computer generated information on any geographical
place or object in reality for the sake of enhancing the understanding and experience of
user [78].
Reviews of the Implications of VR/AR Health Care Applications 7
1.2.4 Formats and Design Elements of VR/AR Technology
This section reflects general understanding about the available formats of VR/AR and
which one is best accepted for health care. In addition, Table 1.2 summarizes the design
elements for implementing VR/AR. The contents of Table 1.2 are taken from Lemheney et
al., 2016.
Table 1.2 Design Elements in Virtual Reality and Augmented Reality
Formats of VR: VR systems have three formats namely non immersive, semi-immersive
and fully immersive. The main concept which is frequently used is ”immersion” with VR.
”Immersion” refers to the sense of being involved in task environment without considering
the time and real world and up to which extent high fidelity important inputs (e.g. sound
waves, light samples) are supplied to diverse sensory modalities (touch, audition, vision)
for the purpose of building powerful illusion of realism [36, 39]. The three formats also
refer to the level of immersion:
A non immersive VR system utilizes usual graphics terminal with a monitor typically
desktop system to view VR environment using some portal or window. This format
imitates a three dimensional graphics environment on television or flat panel within
which the user can interact and navigate. Hence, this format is less popular [49, 60];
A semi immersive VR system is relatively a new implementation which comprises of
comparatively high performance graphics computing system together with an outsized
projection plane to display scenes [49];
A fully immersive system gives a sense of presence but the level of immersion depends on various factors like the field of view of resolution, contrast, update rate and
illumination of display. Generally, an immersive VR system clubs computer, body
tracking sensors, specialized interactive interface such as head mounted display or an
outsized projection screen encasing the user (e.g. CAVE–Cave Automatic VEs where
VE is projected on a concave surface) and real time graphics to immerse the participant in a computer generated world of simulation to perform alterations in a natural
way with body and head motion [56, 60]. Thus, this format leads us to adopt immersive learning environment for health care services and applications presently and also
for future. Figure 1.3 presents some snap shots of various immersion levels.
8 Emerging Technologies for Health and Medicine
Figure 1.3 Levels of VR Immersion (a) A Non Immersive VR System (b) A Semi Immersive VR
System (c) A Fully Immersive VR System [10]
Formats of AR: Since the advent and extreme usage of smart phones in recent times,
most of AR applications are based on this new invention. Hence focusing the smart phones,
there are two major AR formats. According to Pence, 2010:
(a) Marked or mark based AR system utilizes two dimensional barcode normally QR
code (quick response code) to connect a mobile phone and/or personal computer for
overlaying information digitally on real world object or usually on a website;
(b) Mark less AR system employs location based services like GPS (Global Positioning
System) used by cell phone to serve as a platform of adding native information on a
camera vision [11].
Figure 1.4 shows snap shots related to formats of AR.
Figure 1.4 AR Systems Formats (a) Marked AR System. (b) Mark less AR System [38]
1.2.5 Presence, Reality and Realism
Following section briefly explains the cognitive aspects of user perception related to virtual
environment and to some extent augmented environment as well.
Presence: According to Heeter, 1992, presence is a complex feeling with three dimensions:
Personal or physical presence which gives sense of actually being in VR environment,
a room where immersion takes place;
Environmental existence means that VR environment seems to be responsive on user’s
action;
Reviews of the Implications of VR/AR Health Care Applications 9
Social presence refers that user is not alone in VR environment. Put simply, it is
an ability to describe interaction among the user and virtual objects, locations and
animated entities.
Reality: Reality refers through which the user experiences the immersion as genuine in
reply of stimulus. Thus, higher level of reality is related to higher level of realism [7, 8].
Realism: Realism is a fact which relates to level of convergence among the user’s
expectation and actual experience in VR environment. The key factor here is to consider
that how much the virtual stimulus converges expectations of the user [7].
1.3 Features of VR/AR Technology in Health Care
The most emerging feature of VR/AR technology in health care is E-Health with many
enlightening features to support health care like patients can explain their symptoms in a
better way; nurses can easily find veins, pharmaceutical companies can supply innovative
drug information, surgeons can get assistance, invoking empathy, treatment for post therapeutic stress disorder, support for physical therapy, pain management, doctor or hospital
visits, surgical trainings with the help of visualization and maintenance of labs etc.
1.3.1 Implications of VR/AR Technology in Health Care Services and Applications
VR and AR are being predicted to become more and more a part of reality and for the
betterment of humanity presently and over the next coming years. Here a question is raised
that how well health care services and applications capitalize on VR/AR since most of
health care issues employ both technologies to counter clinical practices, medical trainings,
surgery, phobia, rehabilitation and emergency medicine since 2008. Nevertheless, there
is still ample room to develop suitable applications with the involvement of AI and IoT
because health care demands precise, accurate, flexible, robust and efficient agents, expert
systems, gadgets, apps, software and hardware not only to meet the requirements of society
but also helpful for flourishing the working environment of an organization for radical
change. It is further mandatory that people must have computer science expertise and
understanding about the potential implications of these technologies which may lead them
to envision practice in their area of interest.
Following section discusses in detail the implications of health care services and applications keeping in context with AI implicitly and IoT explicitly.
1.3.2 Health Care Services
AI and IoT are two main factors in recent days to make possible the range of health care
services, where each service makes available a set of health care solution. This section
endorses that services are generic in nature and have the possibilities to become building block for a set of way outs and applications. Therefore, these services might include
feedback or notification services, internet services; agents based services, connectivity and
protocol services etc [37]. The subsequent discussion highlights various kinds of health
care services.
Exergaming (Digital gaming technology) is new where health care issues are tackled
with the help of serious games. These methodologies when applied to a user encountering with any kind of medical disease, not only makes himher energetic but also resolves
10 Emerging Technologies for Health and Medicine
his/her health issue in an entertaining way. Such services are gaining popularity and drawn
scientific attention to the emergent health dilemma of childhood diabetes, obesity and in
nursing domain. The central notion of exergaming is to involve energetic body activities
as an input to integrate with digital game with an expectation to succeed the sedentary activity rather than conventional gaming style [45, 61]. This health care problem can also be
tackled using off-the-shelf game console systems like Sony Eyetoy, Nintendo Wii games
and Konami DDR [18, 21, 29, 43, 44].
Phobia: means that an individual is experiencing extreme anxiety to a certain stimulus;
the stimulus might be any animal or any situation like addressing the people, height, blackout, driving and swimming etc. In this situation, an individual feels anxiety and stress
which may result increase heart beat, high blood pressure, dry mouth and sweating [1].
To address this health care service, the researchers point out that exposure based therapy
is suitable for a variety of anxiety and disorders. It means that the exposure works by
allowing the patient to interact fully with activation and subsequent reduction of fear in a
natural way in the presence of phobia stimulus such as the use of ”crutches” (e.g. entertaining exercises) or absolute avoidance behavior (psychologically, cognitively or behaviorally
overlooking the phobia stimulus) [2, 17, 26, 59].
Child health information (CHI) is an IoT based health service which is gaining popularity in a sense that it helps to raise child understanding and educating society as well
as the children themselves about how mental, health and emotional issues and problems
among family members are important [37, 68]. An IoT based interactive setup is placed
in pediatric ward of any hospital for CHI services such as totem with an aim to empower,
amuse and educate hospitalized children [21, 63, 69].
Adverse drug reaction: An injury occurrence from taking medication refers to adverse
drug reaction (ADR). This injury can happen due to some factors:
Taking a single dose of drug;
Combination of two or more drugs;
Taking drugs for long period of time.
Hence ADR is inherently generic. So there is a need to have some ADR services based
on common technical issues and their solutions to design them. The implication regarding
ADR is to have such systems where the patient’s terminal accesses the information of
a particular drug with the help of pharmaceutical AI based information system and then
synchronizes to whether the drug is well matched with his/her energy profile and e-health
record. An example of such implication is iMedPack developed as a part of iMedBox to
overcome ADR by using control delamination technologies [48. 74].
Indirect emergency health care: Health care services have been vigorously involved
in lot of emergencies like accidents, transportation (e.g. ship, bus, car, airplanes and trains
etc.), adverse weather conditions, fire and earthen sites collapse. Therefore, in this context
the health care services are known as indirect emergency services (IEH). Such services
can offer lot of techniques and solutions to counter the situation on the basis of available
information of site, record keeping, post accident measures and notifications [34, 62].
Ambient assistive living: Health care services are readily available for elderly individuals in the society. One popular and important health care service is Ambient Assistive
Learning (AAL) which is available with IoT powered by AI to address aging and injured
individuals. The main objective of AAL services is to make elderly individuals powerful
and confident by giving them independence and human servant like assistance to resolve
their issues. It has been further noticed that keep-in-touch (KIT) smart objects and blocked
Reviews of the Implications of VR/AR Health Care Applications 11
loop health care services can make AAL possible. Both KIT smart objects and closed loop
services function through IoT and AI, therefore an open source cloud based application is
available to implicate AAL which is proposed by researches with minor changes to this
service [41, 58, 77].
Community health care: This idea has been emerged to design and develop a network
on local level around a small area for monitoring community health care. This can be
accomplished with the use of AI based IoT infrastructure. The structure of community
network health care can be seen as ”virtual hospital”. A tenant health information service platform based on functional requirements can be established for the distribution and
sharing of data between medical facilities and service platform in order to acquire medical
advice and health record remotely. Therefore, a specialized community health care (CH)
is unavoidable for providing the technical requirements under one umbrella for impactful
change in the society [48, 70].
Semantic medical access: Sharing huge amount of medical information and knowledge
by a significant application to somewhere else can be possible with the use of semantics and
ontologies and the service is called semantic medical access (SMA). This service helps the
researchers and designers to prepare such health care applications in which semantics and
ontologies can be obtained simultaneously. Implementation of SMA application requires
sensors, medical statue based engines to analyze huge amounts of sensor data stored on
a cloud and all time available data access methods to collect, merge and interoperate for
medical emergency services [73].
Medical Training: VR/AR has immense implications for medical training and considered very beneficial in health care training programs and/or student’s learning. Numerous
software (apps) are available for the society to run them on smart phone for immediate
training, learning and treatment in an emergency. The medical training program provides
a list of medical measures for health care personal to select from it. Once any measure
is selected by health care personal, the screen will display and search the tracking pattern
situated in the patient body. Further, the training program will show an animated solution in three dimensional views representing when, where and in what conditions different
exercises should be carried out. Also the user can amend point of view of the mock-up
(simulation) by moving mobile device back and forth [4, 29].
1.3.3 Health Care Applications
In addition to health care services, there are numerous health care applications which can
help to revolutionize not only the society but organizations as well. It has been noticed that
health care services (see above section) are the basis for health care applications whereas
these applications are directly used by patients and users. Hence, services are developer
centric and applications are user centric. Moreover, there exist lot of gadgets, wearable
devices and other health care devices to work with some health care application. Figure
1.5 shows some of the gadgets used in health care applications.
Electrocardiogram monitoring: Electrocardiogram (ECG) is the measure of electrical activity of heart recorded using electrocardiography on the basis of heart rate, focusing
of basic rhythm and diagnosis of myocardial ischemia, extended QT intervals and versatile arrhythmias [3, 16, 75]. The ECG monitoring system can be formed using portable
wireless acquisition transmitter and wireless receiving processor. Both together search out
automation methods to notice abnormal data in order to identify cardiac function on real
time basis [35].
12 Emerging Technologies for Health and Medicine
Figure 1.5 Gadgets and Wearable Devices used in Health Care Applications [37]
Rehabilitation systems: Rehabilitation represents vital branch of medicine. It means
that physical medicine can improve and overcome the working ability and quality of life
of people having some sort of physical injury or disability. The intelligent and IoT based
smart rehabilitation systems and upper limb rehabilitation systems are given by many profound researchers [20, 47, 67]. This design successfully demonstrates all essential resources to offer real-time information connections. Other rehabilitation systems such as
prison rehabilitation system, smart city medical rehabilitation system, language training
system for childhood autism and rehabilitation training for hemiplegic patients have been
addressed in the past but require advancements to meet today’s requirements.
Blood pressure monitoring: Monitoring blood pressure (BP) is a fundamental aspect
in our daily life. Now it is possible to monitor BP remotely with the involvement of AI and
IoT. To accomplish this notion, there exists a remote communication between health post
and health center which is responsible to monitor the BP of patients. A device is used for
collecting and transmitting BP data to BP apparatus having communication module along
with location intelligent terminal for monitoring BP gradually on real time basis [19, 31,
72]. However, advances are necessary to overcome upcoming flaws to incorporate with
recent research.
Glucose level sensing: Diabetes (blood glucose or sugar) is a common metabolic disease and prime health care application. It is necessary to make it our habit to monitor blood
glucose on daily basis. Diabetes monitoring for an individual discloses changes in blood
glucose patterns and helps to adjust activities, medication and meal timings [50]. A utility
model reveals the transmission of somatic blood glucose data on a device comprised of a
Reviews of the Implications of VR/AR Health Care Applications 13
mobile phone, a computer, a blood glucose collector and a background processor [32]. The
whole setup is a combination of AI and IoT features.
Medication management: One of the serious threats to society is non compliance of
medication as a result of which the patient has to bear enormous financial loss. To overcome this issue, an AI based packaging method is introduced in medicine boxes for medication management, for example IoT based iMedBox is proposed by [55]. The packaging
method has controlled sealing based on delaminating control by wireless communications.
Body temperature monitoring: Monitoring body temperature is a vital habit in health
care service because body temperature is a critical indicator for monitoring and maintenance of homeostasis. Therefore, m-IoT has the successful solution which uses body
temperature sensor embedded in TelosB mote to attain body temperature variations in an
effective manner [37].
Smart phones and health care solutions: Recently, smart phone has become the driver
and rise of health care applications because this device has smart phone controlled sensor.
Smart phone is now considered a popular health care device because of the invention of
multiple types of hardware and software (apps) which can be easily and freely available
for download and can be used by any user for his/her personal health care and satisfaction.
Table 1.3 summarizes some of general smart phone health care apps in detail.
Table 1.3 Smart phones health care apps
The current section discussed in detail some of health care services and applications.
However, few other health care services such as wearable devices access (WDA), the
internet of m-Health things (m-IoT), embedded context prediction (ECP) and embedded
gateway configuration (EGC) require more implications and advances to overcome future health care issues. In the same manner, certain health care applications need to be
addressed vigorously like wheel chair management, imminent health care solution and
oxygen saturation monitoring for potential resolution and integration of new ideas [37].
14 Emerging Technologies for Health and Medicine
1.4 Future Assessments in VR/AR Technology
This section introduces some of the prominent implications and applicable researches made
by researchers for the sake to transform their ideas into VR/AR applications which not
only offer an immense change in an organization but will become useful for the society as
well. Mostly, these researches are based on medical imaging and its related areas under
the domain of image processing and computer vision. This will help the novice VR/AR
professional to build new VR/AR applications that can specifically run on mobile devices
for the betterment of health care issues. Following are some of the research ideas available
for transformation.
Glaucoma detection is a vital task in eye care especially when fundus imaging is available for glaucoma. This could be handled using implications and ideas proposed by [13]
while detection of lung nodule [53], lung cancer [24], brain tumor [5], diabetic retinopathy [6], skin cancer [22] and extraction of cotton wool from retinal images [64] can also
become smart applications in future to improve health.
An important property is the colorization of medical images in order to retrieve required
medical image from a database. This idea and technique has been proposed and available
for developing VR/AR applications [54, 76]. VR/AR environment has an ability to absorb
diversified domains hence we can have applications to classify facial expressions [66],
simulation based facial recognition [22] and biometric based person re-identification [65]
on our mobile devices for enhancing ourselves not only as an individual but also as a
society. Despite all that, potential research has been proposed to build numerous intelligent
systems in future for improving health care services and applications [22, 25].
1.5 Key Challenges for Adopting VR/AR Technology
In recent times, no doubt there is no comparison of any kind of technology with VR/AR
technology. Nevertheless, there still exist certain challenges and gaps in adopting such
diversified technology in this modern era. In this section, some challenges are mentioned
for the reader interest to provide baseline in overcoming these in future:
Funding and monetary issue, which means the organization must have enough funds
for product development, research and coping marketing cost;
Technical limitations is a broad spectrum which reflects that VR/AR systems limit
their use in certain clinical settings and mobile VR/AR systems limit to the pocket size
computer which can be enhanced to take out from constraints. Moreover resolution,
memory and processing are challenging in this aspect;
Organizational issues concerns about having an infrastructure to adopt technology
like blue tooth connectivity, platform compatibility, provision and usage of health
care software and hardware, networking, privacy issues, provision of digital medical
data, vendor relationship and above all the prime factor is acceptability of technology
within the organization;
Lack of knowledge is a primary challenge because most of the people are unaware
with the use of VR/AR technology in health care domain rather than using it as an
entertaining medium. Disseminating knowledge will be an important goal to make
the people aware in using these technologies in health care domain;
Reviews of the Implications of VR/AR Health Care Applications 15
Lack of research studies around VR/AR. It has been observed that there may only be
a handful of useful research studies. So, this needs to be enhanced in future.
Some other additional challenges also need to be focused and emphasized like market
issues and cultural obstacles; regulation and insurance policies, resistance from end user,
lack of interest about concerned side effects etc. which are significant in adopting these
technologies.
1.6 Conclusion
The foremost purpose and objective of this review is to discuss the implications of VR/AR
technologies in health care services and applications for improving societal and organizational change. This chapter highlights diversified priorities in health care services and
applications and efforts made by researchers in this respect taking AI and IoT as baseline.
Further, it also emphasizes on definitions, formats, differences, features, design elements,
cognitive aspects and challenges to VR/AR as a part of discussion.
Unlike VR which is accomplished through a complete virtual environment, AR limits
itself to involve certain virtual elements to merge them with physical world. Although
both technologies are being considered competent for the last two decades in view of some
professionals and researchers but another thought exists that these are still in their initial
phases. Therefore, research is needed to identify finest practices, determine optimal solutions to implement these technologies and facilitate for rapid adoption in society.
REFERENCES
1. Abate, A. F., Nappi, M., & Ricciardi, S. (2011). AR based environment for exposure therapy
to mottephobia. Paper presented at the International Conference on Virtual and Mixed Reality.
2. Abramowitz, J. S. (2013). The practice of exposure therapy: relevance of cognitive-behavioral
theory and extinction theory. Behavior therapy, 44(4), 548-558.
3. Agu, E., Pedersen, P., Strong, D., Tulu, B., He, Q., Wang, L., & Li, Y. (2013). The smartphone as a medical device: Assessing enablers, benefits and challenges. Paper presented at the
Sensor, Mesh and Ad Hoc Communications and Networks (SECON), 2013 10th Annual IEEE
Communications Society Conference on.
4. Alkhamisi, A. O., & Monowar, M. M. (2013). Rise of augmented reality: Current and future
application areas. International journal of internet and distributed systems, 1(04), 25.
5. Amin, J., Sharif, M., Yasmin, M., Ali, H., & Fernandes, S. L. (2017). A method for the detection and classification of diabetic retinopathy using structural predictors of bright lesions.
Journal of Computational Science, 19, 153-164.
6. Amin, J., Sharif, M., Yasmin, M., & Fernandes, S. L. (2017). A distinctive approach in brain
tumor detection and classification using MRI. Pattern Recognition Letters.
7. Baos, R., Quero, S., Salvador, S., & Botella, C. (2005). Role of presence and reality judgment
in virtual environments in clinical psychology. Paper presented at the CYBERPSYCHOLOGY
& BEHAVIOR.
8. Baos, R. M., Botella, C., Garcia-Palacios, A., Villa, H., Perpi, C., & Alcaniz, M. (2000).
Presence and reality judgment in virtual environments: a unitary construct? CyberPsychology
& Behavior, 3(3), 327-335.
16 Emerging Technologies for Health and Medicine
9. Barsom, E., Graafland, M., & Schijven, M. (2016). Systematic review on the effectiveness of
augmented reality applications in medical training. Surgical endoscopy, 30(10), 4174-4183.
10. Baus, O., & Bouchard, S. (2014). Moving from virtual reality exposure-based therapy to augmented reality exposure-based therapy: a review. Frontiers in human neuroscience, 8, 112.
11. Berryman, D. R. (2012). Augmented reality: a review. Medical reference services quarterly,
31(2), 212-218.
12. Bohil, C. J., Alicea, B., & Biocca, F. A. (2011). Virtual reality in neuroscience research and
therapy. Nature reviews neuroscience, 12(12), 752.
13. Bokhari, F., Syedia, T., Sharif, M., Yasmin, M., & Fernandes, S. L. (2018). Fundus Image Segmentation and Feature Extraction for the Detection of Glaucoma: A New Approach. Current
Medical Imaging Reviews, 14(1), 77-87.
14. Bovend’Eerdt, T., Koenig, S., & Lange, B. (2013). Max Ortiz-Catalan, Sharon Nijenhuis, Kurt
Ambrosch. Emerging Therapies in Neurorehabilitation, 4, 249.
15. Carmigniani, J., Furht, B., Anisetti, M., Ceravolo, P., Damiani, E., & Ivkovic, M. (2011).
Augmented reality technologies, systems and applications. Multimedia tools and applications,
51(1), 341-377.
16. Castillejo, P., Martinez, J.-F., Rodriguez-Molina, J., & Cuerva, A. (2013). Integration of wearable devices in a wireless sensor network for an E-health application. IEEE Wireless Communications, 20(4), 38-49.
17. Chicchi Giglioli, I. A., Pallavicini, F., Pedroli, E., Serino, S., & Riva, G. (2015). Augmented
reality: a brand new challenge for the assessment and treatment of psychological disorders.
Computational and mathematical methods in medicine, 2015.
18. Deutsch, J. E., Borbely, M., Filler, J., Huhn, K., & Guarrera-Bowlby, P. (2008). Use of a lowcost, commercially available gaming console (Wii) for rehabilitation of an adolescent with
cerebral palsy. Physical therapy, 88(10), 1196-1207.
19. Edwards, M. (2016). Virtual reality system including smart objects: Google Patents.
20. Fan, Y. J., Yin, Y. H., Da Xu, L., Zeng, Y., & Wu, F. (2014). IoT-based smart rehabilitation
system. IEEE transactions on industrial informatics, 10(2), 1568-1577.
21. Ferguson, C., Davidson, P. M., Scott, P. J., Jackson, D., & Hickman, L. D. (2015). Augmented
reality, virtual reality and gaming: An integral part of nursing: Taylor & Francis.
22. Fernandes, S. L., & Bala, G. J. (2016). Simulation-Level Implementation of Face Recognition
in Uncontrolled Environment. Paper presented at the Proceedings of the Second International
Conference on Computer and Communication Technologies.
23. Fernandes, S. L., Chakraborty, B., Gurupur, V. P., & Prabhu, G. (2016). Early skin cancer
detection using computer aided diagnosis techniques. Journal of Integrated Design and Process
Science, 20(1), 33-43.
24. Fernandes, S. L., Gurupur, V. P., Lin, H., & Martis, R. J. (2017). A Novel Fusion Approach
for Early Lung Cancer Detection Using Computer Aided Diagnosis Techniques. Journal of
Medical Imaging and Health Informatics, 7(8), 1841-1850.
25. Fernandes, S. L., Gurupur, V. P., Sunder, N. R., Arunkumar, N., & Kadry, S. (2017). A novel
nonintrusive decision support approach for heart rate measurement. Pattern Recognition Letters.
26. Freeman, D., Reeve, S., Robinson, A., Ehlers, A., Clark, D., Spanlang, B., & Slater, M. (2017).
Virtual reality in the assessment, understanding, and treatment of mental health disorders.
Psychological medicine, 47(14), 2393-2400.
27. Freina, L., & Ott, M. (2015). A literature review on immersive virtual reality in education:
state of the art and perspectives. Paper presented at the The International Scientific Conference
eLearning and Software for Education.
Reviews of the Implications of VR/AR Health Care Applications 17
28. Goodyear, P., & Retalis, S. (2010). Technology-enhanced learning. Rotterdam: Sense Publishers.
29. Graafland, M., Schraagen, J. M., & Schijven, M. P. (2012). Systematic review of serious games
for medical education and surgical skills training. British journal of surgery, 99(10), 1322-
1330.
30. Green, K. R., Pindergrover, T., & Millunchick, J. M. (2012). Impact of screencast technology:
Connecting the perception of usefulness and the reality of performance. Journal of Engineering
Education, 101(4), 717-737.
31. Guan, Z. J. (2013a). Internet-of-Things human body data blood pressure collecting and transmitting device. Chinese Patent, 202(821), 362.
32. Guan, Z. J. (2013b). Somatic data blood glucose collection transmission device for Internet of
Things. Chinese Patent, 202(838), 653.
33. Heeter, C. (1992). Being there: The subjective experience of presence. Presence: Teleoperators
& Virtual Environments, 1(2), 262-271.
34. Heinrichs, W. L., Youngblood, P., Harter, P., Kusumoto, L., & Dev, P. (2010). Training healthcare personnel for mass-casualty incidents in a virtual emergency department: VED II. Prehospital and disaster medicine, 25(5), 424-432.
35. Herron, J. (2016). Augmented reality in medical education and training. Journal of Electronic
Resources in Medical Libraries, 13(2), 51-55.
36. Howard-Jones, P., Ott, M., van Leeuwen, T., & De Smedt, B. (2015). The potential relevance of
cognitive neuroscience for the development and use of technology-enhanced learning. Learning, media and technology, 40(2), 131-151.
37. Islam, S. R., Kwak, D., Kabir, M. H., Hossain, M., & Kwak, K.-S. (2015). The internet of
things for health care: a comprehensive survey. IEEE Access, 3, 678-708.
38. Jackson, T., Angermann, F., & Meier, P. (2011). Survey of use cases for mobile augmented
reality browsers Handbook of Augmented Reality (pp. 409-431): Springer.
39. Jennett, C., Cox, A. L., Cairns, P., Dhoparee, S., Epps, A., Tijs, T., & Walton, A. (2008).
Measuring and defining the experience of immersion in games. International journal of humancomputer studies, 66(9), 641-661.
40. Khan, M. W., Sharif, M., Yasmin, M., & Fernandes, S. L. (2016). A new approach of cup to
disk ratio based glaucoma detection using fundus images. Journal of Integrated Design and
Process Science, 20(1), 77-94.
41. Konstantinidis, E. I., Antoniou, P. E., Bamparopoulos, G., & Bamidis, P. D. (2015). A
lightweight framework for transparent cross platform communication of controller data in ambient assisted living environments. Information Sciences, 300, 124-139.
42. L Fernandes, S., & G Bala, J. (2017). A novel decision support for composite sketch matching
using fusion of probabilistic neural network and dictionary matching. Current Medical Imaging
Reviews, 13(2), 176-184.
43. Laine, T. H., & Suk, H. J. (2016). Designing mobile augmented reality exergames. Games and
Culture, 11(5), 548-580.
44. Lange, B., Flynn, S., Proffitt, R., Chang, C.-Y., & Skip Rizzo, A. (2010). Development of
an interactive game-based rehabilitation tool for dynamic balance training. Topics in stroke
rehabilitation, 17(5), 345-352.
45. LaViola Jr, J. J., Kruijff, E., McMahan, R. P., Bowman, D., & Poupyrev, I. P. (2017). 3D user
interfaces: theory and practice: Addison-Wesley Professional.
46. Lemheney, A. J., Bond, W. F., Padon, J. C., LeClair, M. W., Miller, J. N., & Susko, M. T.
(2016). Developing virtual reality simulations for office-based medical emergencies. Journal
of Virtual Worlds Research, 9(1), 1.
18 Emerging Technologies for Health and Medicine
47. Levin, M. F., Weiss, P. L., & Keshner, E. A. (2015). Emergence of virtual reality as a tool
for upper limb rehabilitation: incorporation of motor control and motor learning principles.
Physical therapy, 95(3), 415-425.
48. Lv, Z., Chirivella, J., & Gagliardo, P. (2016). Bigdata oriented multimedia mobile health applications. Journal of medical systems, 40(5), 120.
49. Ma, M., & Zheng, H. (2011). Virtual reality and serious games in healthcare Advanced Computational Intelligence Paradigms in Healthcare 6. Virtual Reality in Psychotherapy, Rehabilitation, and Assessment (pp. 169-192): Springer.
50. McCulloch, D., Tsunoda, K., Lee, A. L., Hastings, R., & Scott, J. (2015). Augmented reality
help: Google Patents.
51. Molnar, A. (2017). Content type and perceived multimedia quality in mobile learning. Multimedia Tools and Applications, 76(20), 21613-21627.
52. Moro, C., Stromberga, Z., Raikos, A., & Stirling, A. (2017). The effectiveness of virtual and
augmented reality in health sciences and medical anatomy. Anatomical sciences education,
10(6), 549-559.
53. Naqi, S., Sharif, M., Yasmin, M., & Fernandes, S. L. (2018). Lung Nodule Detection Using Polygon Approximation and Hybrid Features from CT Images. Current Medical Imaging
Reviews, 14(1), 108-117.
54. Nida, N., Sharif, M., Khan, M. U. G., Yasmin, M., & Fernandes, S. L. (2016). A framework
for automatic colorization of medical imaging. IIOAB JOURNAL, 7, 202-209.
55. Pang, Z., Tian, J., & Chen, Q. (2014). Intelligent packaging and intelligent medicine box
for medication management towards the Internet-of-Things. Paper presented at the Advanced
Communication Technology (ICACT), 2014 16th International Conference on.
56. Parsons, T. D., & Rizzo, A. A. (2008). Affective outcomes of virtual reality exposure therapy
for anxiety and specific phobias: A meta-analysis. Journal of behavior therapy and experimental psychiatry, 39(3), 250-261.
57. Pence, H. E. (2010). Smartphones, smart objects, and augmented reality. The Reference Librarian, 52(1-2), 136-145.
58. Queirs, A., Silva, A., Alvarelho, J., Rocha, N. P., & Teixeira, A. (2015). Usability, accessibility
and ambient-assisted living: a systematic literature review. Universal Access in the Information
Society, 14(1), 57-66.
59. Riva, G., Baos, R. M., Botella, C., Mantovani, F., & Gaggioli, A. (2016). Transforming experience: the potential of augmented reality and virtual reality for enhancing personal and clinical
change. Frontiers in psychiatry, 7, 164.
60. Rizzo, A., Difede, J., Rothbaum, B. O., Reger, G., Spitalnick, J., Cukor, J., & Mclay, R. (2010).
Development and early evaluation of the Virtual Iraq/Afghanistan exposure therapy system for
combatrelated PTSD. Annals of the New York Academy of Sciences, 1208(1), 114-125.
61. Rizzo, A. S., Lange, B., Suma, E. A., & Bolas, M. (2011). Virtual reality and interactive digital
game technology: new tools to address obesity and diabetes: SAGE Publications.
62. Ruthenbeck, G. S., & Reynolds, K. J. (2015). Virtual reality for medical training: the state-ofthe-art. Journal of Simulation, 9(1), 16-26.
63. Serino, M., Cordrey, K., McLaughlin, L., & Milanaik, R. L. (2016). Pokmon Go and augmented virtual reality games: a cautionary commentary for parents and pediatricians. Current
opinion in pediatrics, 28(5), 673-677.
64. Shabbir, B., Sharif, M., Nisar, W., Yasmin, M., & Fernandes, S. L. (2016). Automatic cotton
wool spots extraction in retinal images using texture segmentation and gabor wavelet. Journal
of Integrated Design and Process Science, 20(1), 65-76.
Reviews of the Implications of VR/AR Health Care Applications 19
65. Shah, J. H., Chen, Z., Sharif, M., Yasmin, M., & Fernandes, S. L. (2017). A Novel
Biomechanics-Based Approach for Person Re-Identification by Generating Dense Color Sift
Salience Features. Journal of Mechanics in Medicine and Biology, 17(07), 1740011.
66. Shah, J. H., Sharif, M., Yasmin, M., & Fernandes, S. L. (2017). Facial expressions classification and false label reduction using LDA and threefold SVM. Pattern Recognition Letters.
67. Tan, B., & Tian, O. (2014). Short paper: Using BSN for tele-health application in upper limb
rehabilitation. Paper presented at the Internet of Things (WF-IoT), 2014 IEEE World Forum
on.
68. Vazquez-Briseno, M., Navarro-Cota, C., Nieto-Hipolito, J. I., Jimenez-Garcia, E., & SanchezLopez, J. (2012). A proposal for using the internet of things concept to increase children’s
health awareness. Paper presented at the Electrical Communications and Computers (CONIELECOMP), 2012 22nd International Conference on.
69. Vicini, S., Bellini, S., Rosi, A., & Sanna, A. (2012). An internet of things enabled interactive
totem for children in a living lab setting. Paper presented at the Engineering, Technology and
Innovation (ICE), 2012 18th International ICE Conference on.
70. Wang, W., Li, J., Wang, L., & Zhao, W. (2011). The internet of things for resident health
information service platform research.
71. Weiss, P. L., Rand, D., Katz, N., & Kizony, R. (2004). Video capture virtual reality as a flexible
and effective rehabilitation tool. Journal of neuroengineering and rehabilitation, 1(1), 12.
72. Xin, T., Min, B., & Jie, J. (2013). Carry-on blood pressure/pulse rate/blood oxygen monitoring
location intelligent terminal based on Internet of Things. Chinese Patent, 202(875), 315.
73. Xu, B., Da Xu, L., Cai, H., Xie, C., Hu, J., & Bu, F. (2014). Ubiquitous data accessing method
in IoT-based information system for emergency medical services. IEEE Transactions on Industrial Informatics, 10(2), 1578-1586.
74. Yang, G., Xie, L., Mntysalo, M., Zhou, X., Pang, Z., Da Xu, L., . . . Zheng, L.-R. (2014). A
health-iot platform based on the integration of intelligent packaging, unobtrusive bio-sensor,
and intelligent medicine box. IEEE transactions on industrial informatics, 10(4), 2180-2191.
75. Yang, L., Ge, Y., Li, W., Rao, W., & Shen, W. (2014). A home mobile healthcare system for
wheelchair users. Paper presented at the Computer Supported Cooperative Work in Design
(CSCWD), Proceedings of the 2014 IEEE 18th International Conference on.
76. Yasmin, M., Sharif, M., Irum, I., Mehmood, W., & Fernandes, S. L. COMBINING MULTIPLE
COLOR AND SHAPE FEATURES FOR IMAGE RETRIEVAL.
77. Zhang, X. M., & Zhang, N. (2011). An open, secure and flexible platform based on internet of
things and cloud computing for ambient aiding living and telemedicine. Paper presented at the
Computer and Management (CAMAN), 2011 International Conference on.
78. Zhu, E., Hadadgar, A., Masiello, I., & Zary, N. (2014). Augmented reality in healthcare education: an integrative review. PeerJ, 2, e469.
21
Dac-Nhuong Le et al. (eds.), Emerging Technologies for Health and Medicine, (21–284) 
© 2018 Scrivener Publishing LLC
CHAPTER 2
USING 3D SIMULATION IN MEDICAL
EDUCATION: A COMPARATIVE TEST OF
TEACHING ANATOMY USING VIRTUAL
REALITY
Chung Van Le, J.G. Tromp, Vikram Puri
Duy Tan University, Da Nang, Vietnam
Emails: levanchung@duytan.edu.vn, jolanda.tromp@duytan.edu.vn, purivikram@duytan.edu.vn
Abstract
Our project created a 3D model in Virtual Reality (VR) of the human body for anatomy
training. We modeled the skeletal, muscle, nervous, digestive, and cardiovascular systems
to teach anatomy. A study was conducted to assess the effectiveness of the application for
teaching anatomy at three major medical training universities. The research was conducted
with a total of 135 students participating in the research from these three universities.
45 students from each school were divided into three conditions based on the teaching
method: a plastic manikin, a real cadaver, or the 3D VR model. The scores of the groups
using the 3D VR simulation at the three universities were consistently higher than the other
conditions. The remarkable difference in the groups scores suggests that 3D VR simulation
technology can be effective and efficient for teaching anatomy.
Keywords: 3D Virtual Reality Simulation, Anatomy, 3D Human Body Simulation,
Medical Training
22 Emerging Technologies for Health and Medicine
2.1 Introduction
Teaching anatomy relies on observational practice [1, 2]. At most universities, students
learn through the plastic mannequins and pictures while progressively more universities
stopped using cadavers to teach anatomy [4, 7]. Using cadavers is still the best visual
teaching method for anatomy training. It helps learners identify every detail of the body
accurately and adequately. Only when learners understand a normal body structure, can
they identify abnormal changes caused by illness or injury. Knowledge of anatomy, therefore, is essential for all clinicians. Cadavers are less easily available than in past centuries,
so it has become an expensive anatomy models. However, when teaching with plastic mannequins, the number of mannequins [18, 19] in class, is generally not proportional to the
number of medical students in class.
A medical education system based on virtual patients has gradually become a reality,
thanks to the rapidly improving computer hardware, and computer graphics techniques for
Virtual Reality (VR) [12-15].
Figure 2.1 3D virtual reality simulation of human anatomy
Our medical training system (see Figure 2.1 for an impression of some of the 3D VR
models), consists of two basic components: The 3D interactive component is a fully interactive virtual biopsy model that allows users to perform surgical operations through virtual
anatomical [17] instruments; the two-dimensional user interface component provides interactive access to the 2D anatomical models as well as instructional information in training
sessions. Highly interactive training methods are promising advantages over other more
traditional teaching methods [22-24]. Firstly, unlike plastic models, during anatomy training in VR, the system is able to provide natural feedback similar to a living organism,
such as during operations trainees can see changes in heart rate, blood pressure. This gives
trainees the feeling of going through an operation in a real situation. Secondly, unlike practice on real patients, it is clear that the trainee’s mistakes during their training do not cause
risks to real patients. Practicing in VR also reduces the pressure on the trainees when performing a operation, which can help them feel more confident and pro-active during their
practice studies.
Using 3D Simulation in Medical Education 23
Figure 2.2 Practicing in Virtual Reality
2.2 Literature Review of Training with Medical VR
Many researchers are working on the implementation [20, 21] of 3D technology in the general field of anatomy education and investigating the different areas like teaching, training
and many more where the 3D is helpful for the learning anatomy. Several researchers have
evaluated the effectiveness of VR as a teaching method and compared it with student scores
with the traditional teaching methods [3, 4, 8-11, 16]. Marsh, Giffin, Lowrie Jr.’s research
found that [3] web-based learning can improve traditional teaching when they used it for
training about embryonic development of 2D and 3D model. Students learned more from
web based learning modules, they performed better than the students in the other condition
and test score of retention of the new knowledge over time is also higher for those students
who learned via the web based learning module. Researchers Abid, Hentati, Chevallier,
Ghorbel, Delmas, and Douard compared 3D anatomy models with board & chalk teaching [9] for student learning retention, from a class on embryogenesis. It was found that
for short term memorization, 3D teaching technique is more beneficial than he traditional
board & chalk technique, and that in future, further research is needed to assess the midterm, and long-term effects and the long term impact. Keedy, Durack, Sandhu, Chen,
Sullivan, Breiman assessed a interactive 3D teaching technique [11] for teaching liver and
biliary anatomy and found it a more effective technique than traditional textbooks. Seo,
Kim, Choe assessed how effective clay modeling [6] is for learning gross anatomy and
neuroanatomy and compare this clay models with the CT and MRIs. Bareither, Arbel,
Growe, Muszczynski, Rudd, Marone assessed the degree of knowledge improvement [16]
comparing clay modeling as teaching technique with written module and found that Clay
modeling is beneficial for the anatomy education but more research is needed.
Hu, Yu, Shao, Li, Wang assessed a new Dental 3D Multimedia system [5] for a junior
student’s education system for clinical practice and found that Dental 3D Multimedia system works faster and no worse than traditional group. Lundberg, Low, Patman, Turner,
Sinha found that medical students studying gross anatomy, prefered using the self-directed
study learning methods [25]. Hu, Wilson, Ladak, Haase, Fung assessed [8] the 3D educational model of the larynx and students found it easier to learn the larynx with 3D Model as
compared to traditional class lectures. Cui, Wilson, Rockhold, Lehman, Lynch have pro-
24 Emerging Technologies for Health and Medicine
posed the efficacy of 3D Vascular Stereoscopic Model in Anatomy Computed Tomographic
Angiography (CTA) [26] and found that 97% of the students agreed with the statement ”the
3D Model is interesting and occupied [the student] to learn the materials”, showing that
a 2D screen orientation is not as effective as 3D screen, i.e. a stereoscopic 3D vascular
model gives better opportunities to learn about head and neck vascular anatomy and 3D
learning sessions improve spatial abilities in students which have low spatial ability.
Hoyek, Collet, Di Rienzo, Da Almeida, and Guillot evaluated the effectiveness of 3D
teaching methods by comparing it to teaching with 2D drawings on PowerPoint slides and
the result showed that 3D digital animation was considered to be a very important tool to
teach the human anatomy, especially for knowledge retention.
2.3 Methodology of this Study
A total of 135 students were randomly selected from three largest medical training universities of Vietnam in three different regions.
We had to run the experiment at three universities in three different regions in order
to rule out effects on the experimental results cause by teaching style, or quality by only
testing in one university, in one region.
Figure 2.3 Design of the Study
Using 3D Simulation in Medical Education 25
The participants in all research conditions (the three different teaching methods) are
tested at the beginning and at the end of the course by a group of independent professors.
The first test is to assess the student’s level of knowledge, any differences in ability to
learn and acquire technical and scientific knowledge. The next test comes after 3 weeks
of learning about the human skeleton and skeletal muscle. The test is an official exam.
Then students in condition A: plastic manikin, swap condition and go to condition C: VR.
And they receive 3 more weeks of training about Neurology and Digestive System. This is
followed by an official exam (Post Achievement Test or Post Test 1 or Post Test 2 for short).
See Figure 2.3 for a diagram of the design of the experiment. The design is described in
further detail below.
A total of 135 students, sophomores of a Major to be General Practitioner, participated
in the research, which is 45 students from each these three universities:
Hai Phong University of Medicine and Pharmacy (HPMU),
Duy Tan University (DTU), and
Buon Ma Thuot Medical University (BMTU).
The lecturers who teach the anatomy classes during our experiment are from the universities that are participating in this experiment. These lecturers were assessed with regards
to their professional competence and teaching ability by an independent team of experts.
The results of these tests verified that the professional skills of the university lecturers are
relatively equal and that they have the required expertise in anatomy.
All students of at each of the universities first learn about anatomy theory in the normal
classroom. The lecturers introduce students to the basics of anatomy such as the concepts
of human skeleton and skeletal muscle, as well as their functions.
After the first week of theoretical lessons, students are tested of their knowledge and
ability to learn and acquire new theoretical knowledge.
This test was conducted by an independent review team of anatomical specialists. Based
on student score results of this first test, the students were allocated to the different teaching
method conditions (A, B, or C) in such a way that their level of knowledge and learning
abilities were equally distributed across all of the groups in each university.
Age, gender, and ability to study Anatomy were also taken into account to distribute student ability evenly across groups or experimental conditions, see Table 2.1 for an overview
of the resulting age and gender distributions, for each group.
At each school the 45 students were assigned to one of three conditions, the A, B or C
group. Each group consists of 15 students. The difference between groups is based on the
teaching method used:
Teaching Method A ”Manikin”: Using pictures, specimens, plastic models, which is
labeled by.
Teaching Method B ”Cadaver”: Learned directly on dead human bodies called
Teaching Method C ”VR”: Taught through a 3D virtual reality simulation.
After completing the pre-test, groups of students from the universities participated in the
experimental conditions, which consisted of practical anatomy training activities depending on the learning methods they were assigned to. Students practiced in A: laboratories,
or B: clinical practice rooms or C: simulation rooms. The duration of the anatomy lessons
was three weeks including the modular skeletal system.
26 Emerging Technologies for Health and Medicine
Table 2.1 Age and gender variation among groups and conditions (Group A plastic manikin, group
B real cadaver, group C Virtual Reality)
After three weeks of learning and practicing surgery, each group of students had an
examination of what they had learned about the human skeleton and skeletal muscle in
the previous 3 weeks. This exam is an official test, administered by independent, official
examiners. The exam consists of multiple choice and free response questions testing their
knowledge and understanding of anatomy, composed of 100 questions to be answered in
60 minutes by each student. The independent supervisory board determines the topic and
questions for the examination. This third party is traditionally responsible for ensuring the
security and marking the exam papers in the country.
Figure 2.4 Three teaching methods: A. Plastic models, B. Real cadaver, C. Virtual Reality
2.4 Results
The exam results show the scores of each group as follows:
Group A (Learning by observing plastic models, specimens) got the lowest score 5.81
(HPMU, M = 5.67; DTU, M = 5.97; BMTU, M = 5.80);
Group B (Cadaver) M = 6.69 (HPMU M = 6.70; DTU M = 6.60; BMTU M =
6.77) and
Using 3D Simulation in Medical Education 27
Group C (VR) the highest score M = 7.74 (HPMU M = 7.93; DTU M = 7.68;
BMTU, M = 7.63), (see Table 2.2, for the details on the scores of Post Test 1).
A statistical comparison between the scores on the pre-test and post-test 1 is not valid
here, because the pre-test measured current knowledge and ability to learn efficiently, the
post-test 1 measures the newly acquired knowledge that the students at each institution
(HPMU, DTU, BMHU) learned as expressed by their scores on the official end-exam for
this course. The exams were audited and scored by an independent official organization
that is responsible for the quality and security of the exam-procedures nationwide.
However, it is interesting to look at the scores of the students on post test 1 (Human
Skeleton and Skeletal Muscle exam) and post test 2 (Neurology and Digestive System exam)
and compare the scores between the three different conditions. The scores of the different
university students (HPMU, DTU and BMTU) after the first post-training exam can be
seen in Figure 2.5.
Figure 2.5 The scores of the different university students (HPMU, DTU and BMTU) after the first
post-training exam
The scores of the different university students (HPMU, DTU and BMTU) after the
second post-training exam can be seen in Figure 2.6.
Figure 2.6 The scores of the different university students (HPMU, DTU and BMTU) after the
second post-training exam
28 Emerging Technologies for Health and Medicine
It has to be noted that, again a statistical comparison of how much more or less the
students scores on the second exam compared to the first exam is not valid. Both exams
are about different topics and the participants have also a certain amount of learning effect.
The participants in the second test have previous experience with the training, through
the first condition they were assigned to during the first training and first post test. After
the first training and exam, the participants in the condition with the Cadaver, stay in that
condition for the second training and exam. However, the group that first learned via
the Plastic Manikin condition, now experiences the VR condition for the first time. The
participants who experienced the VR condition during the first training and exam, now
experience the Plastic Manikin condition.
After the first tests, the teaching / learning methods are swapped between group A
(learning with plastic manikin) and group C (learning with VR), to check the learning
effects of the different conditions has the same or similar effects again, and make sure the
order in which students are exposed to the different learning conditions does not have an
effect, the students who experienced VR, will now learn with the plastic manikin and those
of learning with the plastic manikin first, now go to the VR condition. After the students
from group A have swapped with the students in group C, the students start learning about
Neurology and Digestive System during 3 weeks.
The independent supervisory board provides all students with a final exam of 90 minutes, consisting of multiple choice and free response questions. See Table 2.3 for an
overview of the scores after the first training, compared to the scores after the second
training.
Figure 2.7 The scores of the different university students (HPMU, DTU and BMTU) grouped
together per condition (Manikin, Cadaver, VR) after the first post-training exam (Post test 1, yellow),
and the second post-training exam (Post test 2, red)
It was noteworthy that the (A → C) participants who firstly learned on plastic models, and secondly used 3D Models and Virtual Reality improved their scores significantly.
from the lowest score group on the first test to the high scoring group on the second test.
Using 3D Simulation in Medical Education 29
Participants in group (B), learning by observing directly from cadavers, during both sets
of lectures, had low scores on the Neurology and Digestive System Lesson. This result
could indicate that which of the learning methods is the most effective (plastic manikin,
cadavers or VR) may also depend on which anatomy topic needs to be learned and how,
during different (types of) lessons. See Figure 2.7 and Figure 2.8 for the results from each
university and each experimental condition respectively. In both independent test the participants in the VR condition scored the best exam scores, the cadaver condition continued
to be second best and the plastic manikin condition continued to have the lowest scores.
Figure 2.8 The aggregated scores of all university students (HPMU, DTU and BMTU) grouped
together per condition (Manikin, Cadaver, VR), with the first post-training exam scores (Post test 1,
orange), and the scores on the second post-training exam (Post test 2, green)
2.5 Discussion
The research findings suggest that anatomy teaching at universities can be improved by using 3D computer generated models, and that the application of VR technology in teaching
is a more efficient method than the traditional methods using plastic manikins or cadavers. To look further into the problems of using cadavers for teaching and learning, we also
collected feedback from 200 students who attended the anatomy training course at universities in Vietnam in previous years, when still learning directly with cadavers. Issues that
were brought forward were that the number of donated cadavers is very low for students
studying anatomy in universities. It is common practice in medical universities around the
world to continue using a cadaver many times, over a long period of time. Normally, a
cadaver is used for 6 months and is then replaced. However, in some countries, where
access to cadavers in difficult, such as Vietnam, it is used much longer, sometimes more
than 2 years. These cadavers are obviously not kept in the original shape, because students
practice surgery on them many times. Therefore, during lessons about the nerves or skeletal muscles, it is often no longer possible to recognize the organs in these bodies, which
makes using these used cadavers for study more difficult. The cadavers become deflated
and black, they are soaked in a formalin used and re-used for a long time and for many different practice tasks; as a result, the medical students’ need to practice in a realistic setting,
30 Emerging Technologies for Health and Medicine
is not fully met. For these additional reasons it is clear that teaching with 3D VR models
of the human body is a desirable alternative that promises to be a highly efficient teaching
method for universities offering medical and healthcare programs.
REFERENCES
1. Nicholson, D. T., Chalk, C., Funnell, W. R. J., & Daniel, S. J. (2006). Can virtual reality improve anatomy education? A randomised controlled study of a computergenerated three-dimensional anatomical ear model. Medical education, 40(11), 1081-1087.
https://doi.org/10.1111/j.1365-2929.2006.02611.x
2. Hurren, E. T. (2008). Whose body is it anyway? Trading the dead poor, coroner’s disputes, and
the business of anatomy at Oxford University, 1885-1929. Bulletin of the History of Medicine,
82(4), 775-818. https://doi.org/10.1353/bhm.0.0151
3. Marsh, K. R., Giffin, B. F., & Lowrie, D. J. (2008). Medical student retention of embryonic
development: impact of the dimensions added by multimedia tutorials. Anatomical sciences
education, 1(6), 252-257. https://doi.org/10.1002/ase.56
4. Donnelly, L., Patten, D., White, P., & Finn, G. (2009). Virtual human dissector as
a learning tool for studying cross-sectional anatomy. Medical teacher, 31(6), 553-555.
https://doi.org/10.1080/01421590802512953
5. Hu, J., Yu, H., Shao, J., Li, Z., Wang, J., & Wang, Y. (2009). Effects of dental 3D multimedia
system on the performance of junior dental students in preclinical practice: a report from
China. Advances in health sciences education, 14(1), 123-133. https://doi.org/10.1007/s10459-
007-9096-9
6. Oh, C. S., Kim, J. Y., & Choe, Y. H. (2009). Learning of cross-sectional anatomy using clay
models. Anatomical sciences education, 2(4), 156-159. https://doi.org/10.1007/978-1-4613-
8782-4 3
7. Huang, H. M., Rauch, U., & Liaw, S. S. (2010). Investigating learners’ attitudes toward virtual
reality learning environments: Based on a constructivist approach. Computers & Education,
55(3), 1171-1182. https://doi.org/10.1016/j.compedu.2010.05.014
8. Hu, A., Wilson, T., Ladak, H., Haase, P., Doyle, P., & Fung, K. (2010). Evaluation of a threedimensional educational computer model of the larynx: voicing a new direction. Journal of
Otolaryngology-Head & Neck Surgery, 39(3). https://doi.org/10.1001/archoto.2009.68
9. Abid, B., Hentati, N., Chevallier, J. M., Ghorbel, A., Delmas, V., & Douard, R. (2010). Traditional versus three-dimensional teaching of peritoneal embryogenesis: a comparative prospective study. Surgical and radiologic anatomy, 32(7), 647-652. https://doi.org/10.1007/s00276-
010-0653-1
10. Codd, A. M., & Choudhury, B. (2011). Virtual reality anatomy: is it comparable with traditional methods in the teaching of human forearm musculoskeletal anatomy? Anatomical
sciences education, 4(3), 119-125. https://doi.org/10.1002/ase.214
11. Keedy, A. W., Durack, J. C., Sandhu, P., Chen, E. M., O’Sullivan, P. S., &
Breiman, R. S. (2011). Comparison of traditional methods with 3D computer models
in the instruction of hepatobiliary anatomy. Anatomical sciences education, 4(2), 84-91.
https://doi.org/10.1002/ase.212
12. Codd, A. M., & Choudhury, B. (2011). Virtual reality anatomy: Is it comparable with traditional methods in the teaching of human forearm musculoskeletal anatomy?. Anatomical
sciences education, 4(3), 119-125. https://doi.org/10.1002/ase.214
Using 3D Simulation in Medical Education 31
13. Wang, S. S., Xue, L., Jing, J. J., & Wang, R. M. (2012). Virtual reality surgical anatomy of the
sphenoid sinus and adjacent structures by the transnasal approach. Journal of Cranio-maxillofacial Surgery, 40(6), 494-499. https://doi.org/10.1016/j.jcms.2011.08.008
14. Jenson, C. E., & Forsyth, D. M. (2012). Virtual reality simulation: using three-dimensional
technology to teach nursing students. CIN: Computers, Informatics, Nursing, 30(6), 312-318.
15. Yudkowsky, R., Luciano, C., Banerjee, P., Schwartz, A., Alaraj, A., Lemole Jr, G. M., ...
& Bendok, B. (2013). Practice on an augmented reality/haptic simulator and library of virtual
brains improves residents’ ability to perform a ventriculostomy. Simulation in Healthcare, 8(1),
25-31. https://doi.org/10.1097/sih.0b013e3182662c69
16. Bareither, M. L., Arbel, V., Growe, M., Muszczynski, E., Rudd, A., & Marone, J. R. (2013).
Clay modeling versus written modules as effective interventions in understanding human
anatomy. Anatomical sciences education, 6(3), 170-176. https://doi.org/10.1002/ase.1321
17. Mller-Stich, B. P., Lb, N., Wald, D., Bruckner, T., Meinzer, H. P., Kadmon, M., ...
& Fischer, L. (2013). Regular three-dimensional presentations improve in the identification of surgical liver anatomya randomized study. BMC medical education, 13(1), 131.
https://doi.org/10.1186/1472-6920-13-131
18. Khot, Z., Quinlan, K., Norman, G. R., & Wainman, B. (2013). The relative effectiveness
of computer-based and traditional resources for education in anatomy. Anatomical sciences
education, 6(4), 211-215
19. Hoyek, N., Collet, C., Rienzo, F., Almeida, M., & Guillot, A. (2014). Effectiveness of threedimensional digital animation in teaching human anatomy in an authentic classroom context.
Anatomical sciences education, 7(6), 430-437. https://doi.org/10.1002/ase.1355
20. Madsen, M. E., Konge, L., Nrgaard, L. N., Tabor, A., Ringsted, C., Klemmensen, . K., ...&
Tolsgaard, M. G. (2014). Assessment of performance measures and learning curves for use
of a virtual-reality ultrasound simulator in transvaginal ultrasound examination. Ultrasound in
Obstetrics & Gynecology, 44(6), 693-699. https://doi.org/10.1002/uog.13400
21. Freina, L., & Ott, M. (2015). A literature review on immersive virtual reality in education: state
of the art and perspectives. In The International Scientific Conference eLearning and Software
for Education (Vol. 1, p. 133). Carol I National Defence University.
22. Ruthenbeck, G. S., & Reynolds, K. J. (2015). Virtual reality for medical training: the state-ofthe-art. Journal of Simulation, 9(1), 16-26. https://doi.org/10.1057/jos.2014.14
23. Kononowicz, A. A., Zary, N., Edelbring, S., Corral, J., & Hege, I. (2015). Virtual patientswhat are we talking about? A framework to classify the meanings of the term in healthcare
education. BMC medical education, 15(1), 11. https://doi.org/10.1186/s12909-015-0296-3.
24. Azer, S. A., & Azer, S. (2016). 3D anatomy models and impact on learning: A
review of the quality of the literature. Health Professions Education, 2(2), 80-98.
https://doi.org/10.1016/j.hpe.2016.05.002
25. Choi-Lundberg, D. L., Low, T. F., Patman, P., Turner, P., & Sinha, S. N. (2016). Medical
student preferences for self-directed study resources in gross anatomy. Anatomical sciences
education, 9(2), 150-160. https://doi.org/10.1002/ase.1549
26. Cui, D., Wilson, T. D., Rockhold, R. W., Lehman, M. N., & Lynch, J. C. (2017). Evaluation of
the effectiveness of 3D vascular stereoscopic models in anatomy instruction for first year medical students. Anatomical sciences education, 10(1), 34-45. https://doi.org/10.1002/ase.1626
32 Emerging Technologies for Health and Medicine
Table 2.2 The statistical summary of pre-test and post-test 1 scores
Using 3D Simulation in Medical Education 33
Table 2.3 The statistics of scoring average after swapping participants from Manikin condition to
VR condition
35
Dac-Nhuong Le et al. (eds.), Emerging Technologies for Health and Medicine, (35–284) 
© 2018 Scrivener Publishing LLC
CHAPTER 3
BUILDING EMPATHY IN YOUNG
CHILDREN USING AUGMENTED REALITY:
A CASE STUDY IN MALAYSIA
N.Zamin1, F.A.Khairuddin1, D.R.A.Rambli2, E.N.M.Ibrahim3, M.S.A.Soobni4
1 University Malaysia of Computer Science and Engineering, Putrajaya, Malaysia
2 Universiti Teknologi PETRONAS, Seri Iskandar, Perak, Malaysia
3 Universiti Teknologi MARA, Shah Alam, Selangor, Malaysia
4 ARLETA Production, Cyberjaya, Selangor, Malaysia
Emails: norshuhani@unimy.edu.my, fuadah@student.unimy.edu.my, dayangrohaya.ar@utp.edu.my,
emma@tmsk.uitm.edu.my, mohdsuhairi@gmail.com
Abstract
Empathy is the feeling that a person can step out virtually from his/her own world and
enter the internal world of another person. In simple notation, empathy means the ability
to ’feel with’ other people, to sense what they are experiencing. Empathy is different than
sympathy. It is a hard-wired capacity many of the people today is lacking. A psychological
study has found that many people are suffering from Empathy Deficit Disorder (EDD).
EDD gets severe by the increasingly polarized social and political culture especially in the
under developed countries. Lack of empathy and social wellness can be very damaging
to the families, organizations and countries. This research investigates how compassion
can be trained as a coping strategy to build social wellness using augmented reality on our
young generations. Smart and empathic citizens are the key to the success of Industrial
Revolution (IR) 4.0.
Keywords: Empathy Deficit Disorder, Augmented Reality, Empathy, Smart Citizens.
36 Emerging Technologies for Health and Medicine
3.1 Introduction
There is a growing concern on the loss of empathy in today’s society. A study conducted
at University of Michigan [1] has found that college students today are showing less empathy than previous decades, a 40% decline in fact since 1980 with a steep drop in the last
decade. That is considered as an alarming number. A lack of empathy graduates will not
be successful change makers in the industries. A research in [2] has found that there is an
increase in social isolation because of the drop-in empathy. Since 1970s, Americans have
become more likely to live alone and less likely to assimilate with the societies. Several
other social studies also found that socially isolated community can take a toll on people’s
attitude towards others. They are less generous and more likely to take advantage on others. Loss of empathy effects the socio-economics of a country too. Research shows that
countries and regions in which there is little trust and respect outside one’s own family tend
to lag in economic development and growth [3]. Lack of trusts leads to a higher poverty
level and crime rate of a country. The less people trust each other, the more they need
for safety measures and regulations. Violence are created from less empathetic societies
because they fail to think what is right or wrong. Thus, empathy need to be fostered from
home as a secured foundation [4-5].
3.2 Motivations
Our research is inspired by the socio-economic problems in our country MALAYSIA.
Malaysians are currently living in hardship due to weak economy. Government intervention in our economy has increased the power of political action while reducing private action. This is a moral crisis. The increased cost of living has forced our fellow Malaysians
to work round the clock. According to the 2012 Vacation Deprivation Survey by Expedia, Malaysia ranks fourth to having the most dedicated workforce with 90% of employees
working even when they are on vacation. On average, Malaysians clock in about 40 hours
a week at work which is equivalent to eight hours a day, based on a five-day work week.
Some citizens are having multiple jobs just to survive the urban living. Parents are working
long hours and have less attachment and bonding with their children.
3.3 Literature Review
Virtual Reality (VR) is used to describe a three-dimensional, computer generated environment that can be interacted and explored by the user. VR provides the access to experiences
things outside the classroom space, for example like an immersion into a refugee camp.
These technologies give the opportunity to the users to immerse themselves in any activity
faster without many risks and do it more interactively than before.
A good example of a company that produces empathy games through VR is the Minority
Media. The company has become the catalyst for a burgeoning video game genre called
empathy gaming where players are forced to confront real human issues like bullying,
alcoholism or depression. The co-found of Minority Media, Earnest Webb said they are
trying to put people in a different mind-set and perspective through VR and taking the
advantage of the educational possibilities [9].
Big technology companies such as Google, Apple, Facebook, and Microsoft are aiming
for victory in the battle for AR. Each of them is working towards countering the others
Building Empathy in Young Children using Augmented Reality 37
work to stay on top of the charts. With the launch of Apple’s ARKit and Google’s ARCore,
in 2017, developers now have access to some powerful frameworks for creating AR apps.
It is now evident that AR has a high potential for growth as referred to Figure 3.1. Experts
predict the AR market could be worth 122 billion by 2024 [10].
Figure 3.1 AR Market Predictions
Our literature studies have found several existing empathic applications using AR and
are summarized in Table 3.1.
Table 3.1 Comparative studies on the existing AR applications for empathy development [6]
38 Emerging Technologies for Health and Medicine
In Table 3.1, most of the applications are not freely available to the users unless they
pay for it. Furthermore, all of them are in English language which does not support the
effective learning among the preschool students in Malaysia since most of them are not
fluent in English. However, as compared to our proposed app, it does not only involve
storytelling, but the user need to decide what to do in a situation. This is a good approach
to make them realize the consequence of their desired action. Moreover, the AR printed
book and the animation in our app are purely in our local language, Malay to make it more
personalized to our local culture and mother tongue. This is toe enable the app as teaching
and learning aid at both schools and homes in Malaysia. Our proposed app is a freemium
product whereby the app is provided free of charge, but profit is made through the sales of
the AR book. The users only need to purchase AR book which will cost around USD10
each.
3.4 Proposed Approach
This research investigates the pedagogy of virtue teaching and learning for early childhood
education and to develop the framework for virtue development using Augmented Reality
(AR). The idea to integrate the use of digital technologies in the development of emotions
and positive character traits is inspired by the advancement of technology that gives the
greatest influence on how the children think. Research has found that digital applications
such as video games can improve visual-spatial capabilities, cognitive skills and increase
attentional ability and reaction times [7]. On the other hand, there are many violent video
games are found to effect on empathy aggressive behavior, aggressive cognition, aggressive
affect, physiological arousal, empathy/desensitization, and pro-social behavior [8]. We are
developing 2D empathetic games in simulated virtual reality environment that presents
some situations that need the young users to respond with empathy. This serves as a roleplay but in virtual environment. At this stage, we have developed three scenes according to
the Malaysia’s preschool curriculum on virtue learning in Malay language. Few empathy
scenarios which will be implemented in the AR application. Each scenario will highlight
a moral value so that the player will acknowledge and learn the pro-social values. the
program will ask the player to choose a selection of answers to identify their empathy
level. After that, the program will explain the situation and tell the correct to make things
right in that scene. A printed AR book with object’s marker will be provided to initiate the
AR application.
Figure 3.2 and Figure 3.3 shows an example of a scene to teach the value of honesty
when a child is caught for a trouble he/she has made. The app will prompt the player to
select the right response related to the scenario. All scenes created were illustrated using
simply PowerPoint software. Next, the young player needs to select a button as a response
towards the played empathy scenario. Each button will play different scenario to show to
player the impact of the button that they have chosen.
3.5 Results and Discussions
A test was conducted on ten preschool students aged four to eight years old and seven
children with learning disabilities which includes children with autism and slow learners.
Other than students, some parents and teachers from both preschool and special school
also were involved during the field test. Interview was conducted to know the parents’
feedback’s on the proposed approach and the AR application.
Building Empathy in Young Children using Augmented Reality 39
Figure 3.2 An empathy scene
Figure 3.3 The response buttons
Figure 3.4 Among the Participants
User Acceptance Test (UAT) of the application was conducted to demonstrate that the
app meets with the performance criteria. Results of field test that was done on 10 preschool
students and 7 special school students at a school in Putrajaya, Malaysia shows the necessity to have such app as teaching aid in empathy pedagogy. The observation on the students
40 Emerging Technologies for Health and Medicine
Figure 3.5 Testing on Preschool Students
while they were using the app was recorded, followed by the distribution of the feedback
forms regarding the app and book.
Seventeen selected young children were given a freedom to explore the application by
themselves and later, they were given a survey paper with five statements. They need to
vote according to which scale they prefer the most for each statement. The voting was
done by sticking the stickers inside the small box next to the emoji face. Table 3.2 shows
the result of the survey form that consist of five statements.
Table 3.2 Overall Result of EMPARTI Evaluation Form
*Preschool: 10 students. Special school: 7 students
Building Empathy in Young Children using Augmented Reality 41
3.6 Conclusions
Empathy at the individual level can make real equality possible at the societal level. Our
proposed Empathic AR application is different than the existing applications where it is
tie with our local culture, language and current curriculum. We are currently in the testing
phase of our prototype at selected public schools in Malaysia. The effectiveness of teaching
and learning empathy via AR apps will be compared with the traditional methods through
series of interviews and observations with the young children, teachers and parents. The
results will be presented in future publications. There is a potential commercialization to
introduce digital emphatic technology as a teaching and learning aid for special schools
in Malaysia. The available empathy teaching and learning in Malaysian schools are undeniably lacking and unable to impart the understanding of the common senses in daily life
due to lack of comprehension from many parties - educators, learner, parents, country’s
policy makers, etc. This phenomenon, in many scenarios can result in permanent serious
socioeconomic problems that will totally shroud the children’s future.
REFERENCES
1. Swanbrow, D. (2010). Empathy: College Students Don’t Have as Much as They Used to.
University of Michigan News. University of Michigan.
2. Konrath, S. H., O’Brien, E. H., & Hsing, C. (2011). Changes in dis-positional empathy in
American college students over time: A meta-analysis. Personality and Social Psychology
Review, 15(2), 180-198.
3. Tabellini, G. (2010). Culture and institutions: economic development in the regions of Europe.
Journal of the European Economic association, 8(4), 677-716.
4. Gordon, M. (2003). Roots of empathy: Responsive parenting, caring societies. The Keio journal of medicine, 52(4), 236-243.
5. Flight, J. I., & Forth, A. E. (2007). Instrumentally violent youths: The roles of psychopathic
traits, empathy, and attachment. Criminal Justice and Behavior, 34(6), 739-751.
6. C. Beyerle. Augmented Reality for ED: Check out some of these Educational AR apps Retrieved 18 February 2018, from https://www.smore.com/u00w-augmented-reality-for-ed
7. Boot, Walter R., Daniel P. Blakely, and Daniel J. Simons (2011). Do action video games improve perception and cognition? Frontiers in psychology 2 .
8. Anderson, Craig A., Akiko Shibuya, Nobuko Ihori, Edward L. Swing, Brad J. Bushman, Akira
Sakamoto, Hannah R. Rothstein, and Muniba Saleem. Violent video game effects on aggression, empathy, and prosocial behavior in eastern and western countries: a meta-analytic review.
(2010): 151.
9. Hardman, S. (2017). These Video Games are Designed to Build Empathy. Retrieved March 7,
2018, from https://newlearningtimes.com/cms/article/4415/these-video-games-are-designedto-build-empathy.
10. BBC News. (2017). What Future for Augmented Reality? — Technology. Retrieved
January 17, 2018, from http://www.bbc.com/news/av/technology-41419109/what-future-foraugmented-reality
43
Dac-Nhuong Le et al. (eds.), Emerging Technologies for Health and Medicine, (43–284) 
© 2018 Scrivener Publishing LLC
CHAPTER 4
EFFECTIVENESS OF VIRTUAL REALITY
MOCK INTERVIEW TRAINING
J. Garcia1, J. Tromp1,2, H. Seaton3
1 State University of New York, Oswego, NY, USA
2 Duy Tan University, Da Nang, Vietnam
3 Aquinas Training, Inc., NY, USA
Emails: jgarcia2@oswego.edu, jolanda.tromp@duytan.edu.vn, hugh@aquinas.io
Abstract
Interviewing for potential employment is an anxiety filled process that affects many individuals. College students and long-term unemployed are among a demographic that is
predominantly susceptible to this anxiety when seeking jobs. Traditional interview training sessions have shown much success in preparing students for employment interviews,
increasing their popularity on college campuses. This study investigates the effectiveness
of Virtual Reality mock interview training in decreasing participants self-reported. The
results support the effectiveness of Virtual Reality interview training, further supporting
the need for institutions to utilize these trainings.
Keywords: Virtual Reality, Interview Training, Anxiety, Usability.
44 Emerging Technologies for Health and Medicine
4.1 Introduction
While seeking employment is necessary for survival in today’s society, the process can
be overwhelming. The lengthy, anxiety filled process, and the emotional response to rejection that come with it can seriously hinder and deter individuals from pursuing further
employment opportunities. This negative feedback can lead to unhealthy anxiety, which
in turn can lead to becoming another factor in being long-term unemployed. Traditional
interview training sessions have shown much success in preparing for employment interviews, increasing their popularity on college campuses. Our research aims at finding out
how effective Virtual Reality (VR) systems can be for mock interview training and the attitudes and opinions of the participants in terms of motivation to use VR for mock interview
training.
4.2 Virtual Reality Training Literature Review
Post-Traumatic Stress Disorder (PTSD) has led to challenges for veterans trying to obtain
competitive employment, especially pertaining to the interview portion of employment
search process. In their study Smith, Ginger, Wright, Wright, Humm, Olsen, Fleming
measured the effectiveness of virtual reality employment interviews for veterans (ages 18-
65) who suffer from PTSD [1]. The participants of their study were United States veterans
suffering from PTSD who were unemployed. These participants received up to ten hours of
Virtual Reality Job Interviews Training (VR-JIT) session. The participants were required
to complete pretest and post-test self-reports. They found positive correlations between
the use of VR exercises and increased scores of these veterans measured by the resource
experts. This positive correlation was most prevalent between the baseline neurocognition
and advanced social cognition, meaning that the VR exercises were successful in improving veterans’ communication skills. These findings support their hypotheses concerning
the effectiveness of VR-JIT among veterans suffering from PTSD, supporting its use with
this demographic.
In their study of 2014, Smith et al. used laboratory-training sessions involving VR-JIT
to assess the usability factor of the software to observe if mock interview scores increased
over time. The researchers found that the participants who utilized VR-JIT demonstrated
larger improvement in job interviewing skills when compared to the control group [2].
This was determined after the individuals using the VR-JIT software underwent 10 hours
of training using the simulator. The individuals who used the VR-JIT software also experienced an increase in the amount of time spent conducting themselves during a mock
interviewing.
Aysina, Maksimenko, and Nikiforov developed the Job Interview Simulation Training (JIST), software that would systematically improve the job interviewing process for
long-term unemployed individuals [3]. They aimed to evaluate the effectiveness of JIST in
preparing long-term unemployed individuals for potential job interviews. JIST consisted of
five sessions of simulated job interviews. The authors found in their study that participants
rated JIST as an easy to use tool. They also found that JIST helped improve individual’s
communication skills when compared to the control group. Additionally, participants indicated an increase in confidence on their self-reports after utilizing JIST. While the authors
felt confident in the effectiveness of their software, they believe that further research is required to see the true effectiveness of JIST software. Their work also concludes that there
is a need to test JIST with different demographics.
Effectiveness of Virtual Reality Mock Interview Training 45
4.3 Methodology
Our study investigates the use of VR mock interview training. Our hypotheses are that
mock interview training in VR will lower interview anxiety, and that their attitude and
opinion towards using mock interview training in VR as a positive addition to preparing
for interviews and will want to continue using it in the future to prepare for interviews:
H10: Participants will not indicate a change in anxiety on their post-test self-report
than on their pretest self-report.
H1A: After concluding the virtual reality mock interview session, participants will indicate having lower anxiety on their post-test self-report than indicated on the pretest
self-report.
H20: Participants will indicate on their post-test self-report that they would not utilize
virtual reality mock interview training tools for future interview preparation.
H2A: Participants will indicate on their post-test self-report that they would utilize
virtual reality mock interview training tools for future interview preparation.
4.3.1 Participants
The participants of this study were ten students (four male, six female), between the ages
of 19 and 24 (mean 22). This study required participants to use the Oculus Rift, a virtual
reality (VR) Head Mounted Display (HMD), see Figure 4.1.
Figure 4.1 Oculus Rift Consumer Version 1
Participants were informed that use of the Oculus Rift Consumer Version 1 (CV1) may
cause side effects such as: eye strain, dizziness, disorientation, vertigo, nausea, and discomfort. Participants were asked to notify the researcher if they experienced any of the
mentioned symptoms, or any additional symptoms. Due to the use of the Oculus Rift for
this study, participants who required eye glasses were excluded from participation of this
study. While it is possible to wear glasses while using the Oculus Rift CV1, it can be
uncomfortable.
46 Emerging Technologies for Health and Medicine
4.3.2 Materials
The multi-user VR software application High Fidelity was used as virtual interview space
for the mock interviews. High Fidelity is a VR application where users can immerse themselves in a self-created environment. See Figure 4.2 for an impression of two users engaged
in a conversation in a virtual space.
Figure 4.2 Example of two users communicating in a Virtual Reality space
Together, the Oculus Rift and High Fidelity application were used to provided participants with the virtual environment for the interview setting. The CV1 Xbox like controller
was used for participants to navigate through the virtual environment, but to do the task the
need to navigate was minimal. Headphones were used to improve the participant’s virtual
experience, effectively fully immersing the user into the experience by providing the audio
from the virtual world and to block outside noise while the interviewer (researcher) asked
the participant interview questions (see Figure 4.3).
Figure 4.3 Image of Virtual Reality Interview Training Session
Effectiveness of Virtual Reality Mock Interview Training 47
A microphone was used to support the dialog between the participant and interviewer
(researcher). The software application Qualtrics was utilized to collect demographics,
pretest self-report responses, and post-test responses from the participants. The use of
Qualtrics was very helpful because it provided digital data collection, decreasing potential
transcribing errors.
4.3.3 Procedure
This study utilized a repeated measures design to compare participants self-report responses before and after participating in the VR mock interview. First participants read
and signed the informed consent document, then they were asked to complete the demographics form and the pretest self-report using the Qualtrics software. Once the participant
completed the pretest self-report, they were asked to put on the Oculus headset and headphones. The researcher assisted participants in properly adjusting the headsets for participants safety. Once the headset was securely on the participants head, the researcher put
on the second Oculus headset and began to engage the participant in a mock interview.
The researcher acted as the interviewer for the session and asked participants five popular
interview questions from a prepared list of typical interview questions. Questions such as:
”Can you tell me a little about yourself?”,
”What are your strengths?”,
”What are your weaknesses”, etc.
Once the researcher asked all the interview questions and the participants answered
them, they ended the interview session. The participant was then asked to remove the
headset and headphones. Then, the researcher instructed the participants to complete the
post-test self-response questionnaire. Once the participant submitted their responses, the
researcher provided a debriefing.
4.4 Results
A paired-samples t-test was conducted to compare participants self-reported anxiety prior
to the VR mock interview and after the VR mock interview. There was a significant difference in the scores for participants self-reported anxiety prior-to (M = 2.8,SD = 2.348)
and post-to (M = 1.40,SD = 1.897) the VR mock interview; t(9) = 3.096, p = 0.013,
see Table 4.1.
Table 4.1 SPSS output for Paired Samples T-Test comparing participants anxiety levels prior-to
and post-to the VR mock interview
48 Emerging Technologies for Health and Medicine
These results reject the first null hypothesis (H10), demonstrating a change in anxiety
prior-to and post-to the VR mock interview.
On the post-test self-report, participants were asked if they believe the virtual reality
interview training session would be a useful tool to prepare for a real job interview. This
was used to investigate the second hypothesis (H2). Of the ten participants, 50% indicated
”definitely yes”, 20% indicted ”probably yes”, and 30% indicating ”might or might not”,
see Figure 4.4.
Figure 4.4 Participants response to measure 12
Participants were also asked if they would continue to use VR interview training as a
method to prepare for real job interviews. 70% of participants indicated ”probably yes”
and ”definitely yes”, 30% indicated ”probably not”, see Figure 4.5.
Figure 4.5 Participants responses to measure 13
4.5 Disscussion
The purpose of this study was to investigate the effectiveness of VR mock interview training in decreasing interview-anxiety in college students. Additionally, we wanted to know
Effectiveness of Virtual Reality Mock Interview Training 49
about the participants’ perceived usefulness of VR mock interview training, and participants self-reported likeliness of continuing to use VR mock interview training as a method
to prepare for real job interviews.
The results from the paired-samples t-test support the researchers first hypothesis (H1),
further supporting the use of VR interview training as an effective tool for decreasing
interview-anxiety and increasing interview preparedness. These findings are in agreement
with the work of Aysina et al. and Smith et al. regarding the effectiveness of VR interview
training, but with a different demographic (college students), adding to the overall effectiveness of VR interview training across various demographics. The second hypothesis
(H2) was also supported by participants responses to questions twelve and thirteen. The
results obtained from this study demonstrate an interest by college students in utilizing VR
mock interview training to prepare for future interviews. These findings could be used at
universities by students, faculty, and staff, to advocate for VR interview training programs.
Limitations of this Study: The biggest limitation of this study was that most of the
participants had never experienced virtual reality technologies. The novelty of using this
technology could have caused participants to provide better ratings on their post-test selfreport. Additionally, the use of this software could have caused the participants to feel
anxious and uncomfortable. Due to the recent interest in multi-user VR applications, the
VR software available is still in a beta phase, and not developed specifically for the purpose
of conducting interview training sessions. The experience of practicing for an interview in
VR can be made more realistic by creating a VR interview room that looks like a typical
interview room in the real world, to assess whether transfer of the learned skill will be
more likely.
4.6 Conclusions
The results of this study support previous research investigating the effectiveness of VR
interview training. While these findings support the claims that VR interview training is
an effective tool, the limitations should be considered. Future research is needed investigating effectiveness of VR interview training on a population that has had prior experience
to virtual reality technologies, thus eliminating novelty as a possible confounding variable.
However, these results should be used to advocate for VR interview training sessions on
college campuses and with long-term unemployed individuals, as these trainings assist individuals practice for real interviews. College graduates face an extremely competitive
job search after completion of their degrees. The long-term unemployed population are
also face an extremely competitive job market. The long-term unemployed are another,
highly vulnerable population, that will benefit from extra training to help them enter the
extremely competitive job market. These technologies can aid students and the long-term
unemployed in better preparing for potential interviews, thus giving them an advantage.
Additionally, these trainings have shown effectiveness in various demographics, demonstrating a need to also advocate for these training in additional agencies, such as the government. With high unemployment, VR interview training sessions could benefit many in
preparing to re-enter the workforce. Advances in virtual reality technologies have allowed
for great growth in various sectors, such as medical, educational, and commercial. By investigating these technologies for other domains, we can provide quantitative data to help
advocate for further investments in these technologies. The complete report of the research
described above can be found in [4].
50 Emerging Technologies for Health and Medicine
REFERENCES
1. Smith, M.J., Boteler Humm, L, Fleming,M.F., Jordan, N., Wright, M.A., Ginger, E.J.,
Wright, K., Olsen, D., and Belle, M.D. (2015). Virtual reality job interview training for veterans with posttraumatic stress disorder. Journal of vocational rehabilitation, 42(3), 271-279.
https://doi.org/10.3233/jvr-150748
2. Smith, M. J., Ginger, E. J., Wright, M., Wright, K., Humm, L. B., Olsen, D.,
& Fleming, M. F. (2014). Virtual reality job interview training for individuals with
psychiatric disabilities. Journal of Nervous and Mental Disease, 202(9), 659-667.
https://doi.org/10.1097/NMD.0000000000000187.
3. Aysina, R.M., Maksimenko, Zh. A., & Nikiforov, M.V. (2016). Feasibility and Efficacy of Job
Interview Simulation Training for Long-Term Unemployed Individuals. Psychology Journal,
14(1), 41-60. Retrieved May 11, 2017, from www.psychnology.org.
4. Garcia, J. (2017). HCI Master’s Research Project II Report: Evaluation: Using Virtual Reality
for Mock Interview Training, SUNY VR First Lab, State University of New York, NY, USA.
51
Dac-Nhuong Le et al. (eds.), Emerging Technologies for Health and Medicine, (51–284) 
© 2018 Scrivener Publishing LLC
CHAPTER 5
AUGMENTING DENTAL CARE: A
CURRENT PERSPECTIVE
Anand Nayyar, Gia Nhu Nguyen
Duy Tan University, Da Nang, Vietnam
Emails: anandnayyar@duytan.edu.vn, nguyengianhu@duytan.edu.vn
Abstract In recent years, there has been an increasing interest in applying Augmented
Reality (AR) in diverse medical applications. Augmented Reality technology is regarded
as combination of virtual information in addition to real-world information which opens
wide possibilities in various medical activities like Surgery, Education, Consultancy and
even basic diagnosis. Augmented Reality is utilized in various medical surgeries like Laparoscopic Surgery, Brain Surgery, Plastic Surgery, Heart Surgery etc. Surgeons are even
making use of Medical Augmented Reality to increase the vision power to undertaken
medical procedures which is regarded as high-end advancement over conventional surgical
methods. In Dentistry, Augmented Reality is increasing its roots by providing applications
and AR/VR based equipment’s for oral and maxillofacial surgery, dental implant, Orthognathic surgery and even other clinical applications. The objective of this chapter is to
summarize basic history, definition, features of Augmented Relaity and highlight various
AR technologies contributing towards betterment in dentistry.
Keywords: Augmented Reality, Dentistry, AR Technologies, Image-Guided Surgery,
AR Dental Simulators, AR Dental Apps.
52 Emerging Technologies for Health and Medicine
5.1 Introduction
Augmented Reality (AR) [1, 2] is a strong variation of Virtual Reality (VR) / Virtual Environment (VE). Augmented Reality and Virtual Reality (VR) industry has touched $1.1
billion investment in 2018 and it is expected to reach $4 billion by 2022 and AR and VR
are considered as foundation stepping stones for future computing. Virtual Reality technology completely steps a user inside a virtual synthetic environment, the user can see almost
everything in virtual sense like happening in real world. In comparison to VR, Augmented
Reality technology takes information generated from computers like images, audio, video
or touch inputs and transforms completely into a real-time environment. Augmented reality creates a strong foundation to enhance user experience in five senses, especially Visual Sense. Augmented Reality enables the user to see the real world, via virtual objects
combined with the real world. These days, Augmented Reality is producing strong advancements to various fields like Gaming, Advertising, Entertainment, Military, Retail and
Marketing, Education, Travel and Tourism, Automobiles, Fashion, Industry/Manufacturing and especially Medical. Augmented Reality Technologies is revolutionizing medical /
healthcare and is providing surgeons/ doctors with state of the art AR based Apps, Visual
Equipment’s for diagnosing patients. Considering the state of the art technologies adapted
by medical areas like Eye Surgery, Heart, General Medicine, Dentistry, Brain Surgery etc,
AR Technologies see a bright future in this area.
AR Technologies in Dentistry especially in Oral and Maxillofacial Surgery, Dental Implant, Orthodontics, Radiology, Clinical Applications and Education is widely adapted.
The focus of this chapter is to focus on wide range of technologies available for Dentistry
to assist Dental Practitioners for adaption for betterment of patient’s oral health.
5.1.1 Origin of Augmented Reality
Since the origin of mankind, human beings have done a lot in environment altering and
improvising. Early works surround regarding modifying and enhancing the surroundings
via physical objects addition in form of structures etc. Example, Humans cleared jungles,
modified rocks to sit on and make some tree shapes decorations to improvise everything.
In the later stages, they added information to the surrounding environment via images in
form of paintings on cave walls etc for educational purpose to indicate a location or story
telling etc.
With more progress, more tools and techniques were discovered for shaping the environment. But some changes were easy and some too hard to alter. With more progress
and integration of technology, new ideas became more popular, and ideas were represented
symbolically (Drawing) or Symbolically (Map). During this stage, the world was surrounded by lots of physical structures. Ideas were proposed in form of paintings, dance,
music, sculpture and many more. During this phase, the mapping technology was also
improvised.
During industrial age, rapid and significant improvements were seen in construction,
deconstruction and physical structures modification. But the rate at which the physical
entities were modified was very large i.e. it took months to years to modify something to
the physical structure. Changes to physical world remained in physical realm, i.e. any sort
of modification in environment was manifested with different physical entities occupying
space and weight. So, in order to make a piece of information available to specific physical
space, the best way is to create a physical model-based artefact which contains the entire
information. Example: If there is a requirement to present the Vehicle Maximum Speed
Augmenting Dental Care: A Current Perspective 53
required on the specific road, a physical poll containing a photo of speed limit would
be placed for effective user information. In the few years, it was decided to add certain
keyword-based sign information which gives more depth information to the drivers driving
on the road. The transformation keep on happening for years to add some Physical Street
based cameras for identifying the speed limit of the vehicle, add the speed sign with some
Computer based LED Screen.
With the advancements in Information technology, the information started representing
digitally. With the utilization of computers, tons of information can be stored, retrieved,
updated in small period of time and will occupy very little space. With utilization of IT
based Technologies, more powerful ways were generated to augment the real-environment.
With the power to store and update information with various devices like Smart Phones,
Tablets, Laptop computers connected to pervasive network, interconnected devices enabled
smart living for humans. With the passage of time, increase in ability of smart phones
power, cheap price, small size, simulations were possible and the word ”Virtual” evolved.
Example: Nowadays, lots of apps are available on smartphones, where you can play the
musical instrument as such like physical instrument.
3D Computer graphics generation in real-time enabled users to create virtual scenes
with as such replica like real-world physical environment. With the evolution of Virtual
Reality Systems, 3D Movies and Advanced Simulations, full rendering can be performed
of virtual objects. 3D TV’s and VR Gaming Headsets like KINECT, PS VR, the body
actions can be mediated via devices through joysticks, mice and buttons. The GPS Navigation devices have made it possible to have pinpoint location of anything anywhere in
the world. With GPS, one can locate anything from simple Coffee Shops, to popular sites
nearby by just typing keywords like ”coffee shops near me”. Each of these technologies
have taken mankind to step further and with digital enhancements, the users are able to
interact better with real world. In order for mankind to alter and improvise the world in
digital form, without any physical alteration, a plethora of ideas and technology innovations are required for significant change.
5.1.2 History of Augmented Reality
Augmented Reality has its origin way back in 1901 in a novel titled ”The Master Key” by
L. Frank Baum where he introduced the concept of Augmented Reality but the exact name
i.e. AR was not cited. He proposed the concept of Augmented Reality as a gift the central
character receives from a demon.
The following Table 5.1 lays a strong foundation highlighting the Origin of Augmented
Reality.
5.2 Augmented Reality Technology in Medical Technology
New invention and technology is proposed with prime objective Life Simplicity. Augmented reality has wide potential to play a significant role in Healthcare Industry. Technologies like Computer based AXIAL Tomography has provided a significant platform to
doctors to have deep insight of the human body. Telehealth facilitates doctors and patients
to have live communication even in remote areas. With Heads-up displays, vital information of the patient can be provided to doctors in matter of seconds. Emergency rooms and
ICU units are now equipped with AR technology for EMR and imaging.
54 Emerging Technologies for Health and Medicine
Table 5.1 The foundation highlighting the Origin of Augmented Reality
Augmented Reality (AR) and Virtual Reality (VR), nowadays are combined with Artificial Intelligence technology which is changing the entire face of healthcare industry.
According to Deloitte’s 2018 Technology, Media and Telecommunications Predictions Report, near to about 1 Billion smartphone users have created at least a single AR based content in 2018 and 300 million users are doing monthly and number can even increase to tens
of millions by end of this year.
Augmented Reality is defined as, Live Direct or Indirect view of a Physical, Realworld environment whose elements are termed as Live Direct or Indirect View of a Physical, Real-world environment whose elements are termed as ”Augmented” by computergenerated or real world extracted sensory input like sound, video, graphics or data based
on GPS”.
An AR system can be termed as system with following set of features:
Combines Real and Virtual Objects in real-time operational environment and performs live interaction with the end users.
Registers Virtual and Real time objects in reciprocal manner.
Augmenting Dental Care: A Current Perspective 55
In order to design and implement a AR system, the system should be fully integrated
with: virtual and real data sources, tracking sensors, image processing, object recognition
algorithms, feedback mechanisms with strong backbone support of Machine Learning,
Deep Learning and Mixed Learning.
Healthcare Industry is currently regarded as No: 1 industry in AR technology adoption
with more and more healthcare applications are developing, evolving and releasing with
passage of time. AR Technologies are integrated with wide range of medical applications
like Surgery, Anatomy, Dentistry, Education, Simulation, Chronic Diseases detection and
General Observation. According to EdTechReview, considering AR roots in healthcare
and education, the industry is expected to touch $659.98 million by 2018.
5.3 Existing Technologies in Medical / Healthcare Technology
AR technology provides a strong base to surgeons and other medical practitioners with
latest and all relevant information regarding patients. AR can itself be used very easily
by patients for self-education and quality treatment. VR [29] plays a significant role for
creating 3D simulations used by doctors and medical students for real-time diagnosing of
patients on chronic cases.
The most suitable examples for AR in medicine are: NuEyes- An AR based smart
hands-free electronic glasses designed for blind people built using ODG R-7 platform facilitating the people with low vision to recognize nearby things and easily perform routine
tasks. It can be wirelessly operated or via voice commands.
BrainPower- software technology designed by MIT revolutionized AR based devices
like Google Glass/ Samsung Gear into AI system to assist people in brain related issues
like Autism. Accuvein- smart AR projector to display all the patient’s veins on the skin in
real time. Microsoft Hololens- Smart AR device is providing surgeons to easily perform
spinal surgeries via Scopis platform.
In addition to the above, AR technology is also applied to other medical fields like Psychological disorder, rehabilitation, Prosthetics, Ultrasounds, Blood transfusion and many
more [6]. In the near future, AR with mix blend of Internet of Things (IoT) will facilitate 3D model development for accurate planning of surgeries and will train future doctors
anywhere and everywhere.
5.4 Augmenting Dental Care- AR Technologies assisting Dentists for Dental
Care
With the explosive growth of elderly population and growth in world economy, the concept
of Oral Health has increased at steady pace, and various dental and oral heath care issues
are becoming significantly important. The global market for dental equipment’s is expected
to touch $30 Billion U.S. dollars. In addition to this, WHO (World Health Organization)
statistics show that almost 60% of the school going children worldwide, 100% adults and
20% of the population ranging from 35-44 years have dental problem. With the increase in
number of old-age population, coupled with elderly people, the treatment rate is very low
[3-5].
Dental care also shares unique features with other medical specialist units but also has
distinct qualities. It is also significant field where AR technologies are applied successfully
and nowadays, dental treatment via AR assistance in diagnosing, education and surgeries
56 Emerging Technologies for Health and Medicine
is shining by leaps and bounds. In this section, AR technologies with respect to Oral and
Maxillofacial Surgeries, Dental Implant, Orthognathic Surgeries, Clinical Applications and
Dental Education will be elaborated.
5.4.1 Augmented Reality Technologies in Oral and Maxillofacial Surgery
Oral and Maxillofacial Surgery [7, 8] provides a strong base bridge between general medicine
and dentistry and is mostly concerned with the diagnosis and treatment of diseases effecting patient’s mouth, jaws, face and neck. The surgeon performs diagnosis and management
with regard to facial injuries, head and neck cancer, facial pain, impacted teeth, cysts and
jaws tumours as well as other issues concerning mouth ulcers and infections.
Figure 5.1 Oral and Maxillofacial Surgery- Before and After Results
5.4.1.1 Main Operations
A range of Oral and Maxillofacial surgical operations are carried out via Anaesthesia
or Conscious Sedation. The following are types of operations concerned with Dental Oral
and Maxillofacial Surgery:
Facial Injuries, retraction of complex craniofacial structures and other tissue injuries.
Orthognathic surgery for correcting facial disorders.
Pre-implant surgery like retaining facial or dental prostheses.
Impacted teeth removal.
In order to accurately perform Oral and Maxillofacial surgeries, understanding of complex anatomy of craniofacial structures with high precision is required. AR Technology
well fits for this purpose and nowadays, is highly recommended for performing Maxillofacial surgeries. In order to perform surgery, AR technology assists Dental surgeons via
AR Navigation and AR Guidance systems. AR navigation systems provides complete information of the patient via virtual scheme is mixed with real scene with 2D HMD (Head
Mounted Display) worn by surgeon. AR visualization systems, autostereoscopic 3D image
overlays are utilized for image and stereo tracking. AR guidance systems provides realtime intraoperative information and projects 3D images on laparoscopic images to mark
surgical incisions.
Augmenting Dental Care: A Current Perspective 57
5.4.1.2 AR Visualization Technology
Improvised Anatomical Visualization: Researchers from Shanghai Ninth People’s
Hospital and State Key Laboratory proposed a system for visualization improvement of
complex anatomical area via AR technology. The technology facilitates, scanning of patient skull using 3D CT and all images imported into Mimics CAD/CAM software resulting
in generation of 3D model of skull of patient. A Dental cast and occlusal splints is built
via acrylic resin.
Figure 5.2 3D Patient Skull Generation
On this technology, AR technology is applied for image orientation for live viewing of
surgery and usage of X-Ray gets eliminated.
AR based Guidance systems provides 3D representations of patient’s body in more
intuitive manner using HMD’s or eyepieces.
Microsoft HoloLens1: Mixed Reality smart glasses proposed by Microsoft for smart
guidance systems and work in efficient manner for Dental Surgeries.
HoloLens is connected to dynamic and well adjusted cushioned inner headband for
moving up and down. It is fully equipped with sensors related hardware which includes
cameras and processors. It is fully equipped with Accelerometer, Gyroscope and Magnetometer. It is fully equipped with IEEE 802.11ac Wi-Fi and Bluetooth 4.0 for connectivity.
Microsoft HoloLens is gaining worldwide attention by Dental Surgeons with a Project
titled ”HoloDentist” which combined Microsoft HoloLens and Dentistry. With HoloDentist based Mixed Reality, improvised communication can happen between dentist and patient and the device is able to create 3D model of the mouth and provides pinpoint 3D
visualization for surgery planning. It also provides service collaboration and remote support to the patients.
VUZIX Blade2: Vuzix blade provides smart AR technology in terms of Voice control,
Touchpad, Head Motion Sensors, Remote Support and is equipped with Wave-Guide Technology. It provides smart display with see-through viewing experience using Waveguide
optics and Cobra II display engine.
With VUZIX Blade smart AR Technology, dentists are able to map 3D model and visualization of the patient for carrying out facial treatments and other teeth surgeries.
1https://www.microsoft.com/en-us/hololens
2https://www.vuzix.com/
58 Emerging Technologies for Health and Medicine
5.4.2 Augmented Reality Technologies in Dental Implant Surgery
Dental Implant surgery is an effective procedure for replacing the tooth roots with metal,
screw like posts and replacing the damaged or missing teeth with artificial teeth with replica
of original teeth’s. Dental Implant surgery is tedious surgery and perfectly depends on the
nature of implant and jawbone condition.
Figure 5.3 Dental Implant Surgery via Screw fitted on Jawbone
5.4.2.1 Types of Dental Implants
There are two types of Dental Implants [9]:
Endosteal: This dental implant treatment is supported via screws, blades or cylinders
placed inside jawbone via precision surgery. This type of implant includes addition
of 1 or more than 1 prosthetic teeth.
Subperiosteal: Under special patient conditions, those having not proper jawbone
height, subperiosteal implant is recommended. Under this surgery, implants are placed
on jawbone top via metal framework in middle of gum to hold artificial tooth.
Considering Dental Implant- only five procedures are followed: Single Tooth Replacement; Multiple Teeth Replacement; Full Teeth Replacement; Sinus Augmentation and
Ridge Modification.
5.4.2.2 AR Technologies for Dental Implant Surgeries
Implant Positioning System based SDK’s: AR technology has supported dental implant surgery way back in the year 1995 via dental implant positioning systems. AR also
facilitates teleplanning and precision surgical navigation for dental replacement and placement.
AR ToolKit: an open source library for creating AR applications using video tracking
capabilities to determine real camera position and calibrate to square physical markers or
natural feature markers in real-time. It was designed and proposed by Hirokazu Kato and
was regarded as first AR based SDK system.
Features: AR Toolkit facilities tons of features via HMD displays for tracking real-time
dental patient via stereo camera, planer images detection, square marker generation and
Optical HMD’s for dentists to carry out implant surgeries.
Augmenting Dental Care: A Current Perspective 59
5.4.2.3 AR based Dental Implant Surgical Navigation Systems
AR Surgical Navigation systems were developed fully equipped with retinal imaging
display.
RPHMD: Yamaguchi et al. [11] proposed smart navigation AR system for dental implant by combining Retinal Projection HMD (RPHMD) and AR techniques for overlaying pre-operative simulation to provide real-time view to surgeon. The graphics
were represented by OpenGL library. Overall Image accuracy provides best results
for the surgery.
AR-SNS [12]: AR based Surgical Navigation systems (AR-SNS) was proposed by
Chen et al. It was designed and developed by using optical see-through HMD to assist
dental surgeons in safety and reliability in patient’s surgery. The system proposed
is highly smart and efficient enough for instrument calibration, registration, HMD
calibration and provides prevision 3D virtual model of complex anatomical structures
to doctor’s HMD. The device has pinpoint accuracy of 0.809 +/- 0.05 mm and 1.038o
+/- 0.0.5o in diagnosing patient’s health.
AQ-Navi Surgical Navigation System [13]3: was designed to improvise dental implant surgeries via precision guidance to dentists of surgery location via electrooptical technology. It is used to perform pre-planning surgical implantation procedure to identify and track the location of drill to be done during surgery. The system
provides add on features like accurate positioning, avoiding damages during complex
anatomy jaw structures of patients and enhanced assistance. It provides the dental
surgeons with 2D and 3D patient images with regard to anatomy, drill position and
dental implant. It makes use of IR tracking system composed of emitters, camera,
tracking data processor and dental HMD location.
Image Guided Implant Dentistry System (IGI)4: Image Guided Implant Dentistry System [14] is regarded as most advanced AR based system for dental implant surgeries
to make use of 3D Imaging and Motion tracking technology in combined manner. It
ensures better safety to patient’s health via CT Scan and Computerised Surgical Navigation system. It provides 3D model of patient’s anatomy structure and even assist
dental surgeons for precision drilling during surgeries. The system is fully integrated
with TRAX system which is advanced combination of hardware like: Camera, LED
array and also combines DentSim simulator for accurate tracking of the Drilling position.
5.4.3 Augmented Reality Technologies in Orthognathic Surgery
Orthognathic Surgery / Corrective Jaw Surgery [15] is regarded as combination of orthodontic surgery and general surgery for treating all sorts of jaw and dental abnormalities.
It is regarded as jaw corrective surgery whose primary goal is to straighten, realign the jaw
as well as correct all sorts of skeletal deformities in patient’s oral health. It is regarded as
sub-specialist branch of oral and maxillofacial surgery and the surgery revolves with improvising both functions and mouth appearance and sometimes breathing way correction
of the patient.
3https://www.taiwan-healthcare.org/
4https://image-navigation.com/igi/
60 Emerging Technologies for Health and Medicine
Figure 5.4 Orthognathic Surgery
Various conditions leading to Orthognathic Surgery are: Birth defects, Jaw pains, inefficient mouth gestures while breathing, Trauma, Protruding Jaw and Receding Lower Jaw
and chin.
The Treatment of Orthognathic Surgery involves four phases: Planning, Presurgical
Orthodontic Phase, Surgery, Post-Surgical Orthodontic treatment.
5.4.3.1 AR Technologies for Orthognathic Surgery
AR applications are most widely utilized till date in performing Dental Orthognathic
Surgeries as well as general Orthognathic surgeries. The First AR based Orthognathic
surgery was performed in 1997 by Wagner et al. in facial skeleton osteotomy via Head
Mounted Display (HMD). The technology aided surgeon with best visual information of
the patient during surgery.
5.4.3.2 AR Based Tracking Technologies
ManMos (Mandibular Motion Tracking System) [16, 17], a mixed reality-based system
for performing precision orthognathic surgery was proposed by Kanagawa Dental University, Kanagawa, Japan. The system makes use of dental cast and a computer-generated
3D maxillofacial model based on CT Scan DICOM data. The system is highly efficient in
order to synchronize the dental cast model movements via 3D model of patient.
5.4.3.3 AR based Applications for Orthognathic Surgery
Zhu et al. [18] conducted a feasibility study using ARToolKit for mandibular angle
oblique split osteotomy, using occlusal splint for AR registration. The study was
conducted on 15 patients and the virtual images of the mandible as well as cutting
edge plane both overlaid the mandible real-model. Under this study, the patients
undergone various treatments like tomographic scan and dental casts. Occlusal splint
was used by dental surgeon for marker fix and was tracked by ARToolKit.
Suenaga et al. [19] evaluated AR navigation system for providing markerless registration system using stereo vision in Orthognathic surgery. In this study, stereo camera
was utilized for performing all functions like tracking and markerless registration and
tomography consisted of 3D model of Jaw of patient. The pilot study resulted in precision detection of teeth incisal edges with error of transmission less than 1 mm. The
study concluded that 3D contour matching is best for Teeth information viewing even
with complex anatomies.
Augmenting Dental Care: A Current Perspective 61
Badiali et al. [20] proposed a novel localiser-free HMD for assisting dental surgeons
in Orthognathic as well as Maxillofacial surgery. The system proposed is to perform
accurate virtual planning overlay of the real-patient and proposed a method to determine the performance of waferless, AR-based Bone Repositioning. The method was
tested via Vitro Testing on live human skull of patient and study stated accurate detection of repositioned maxilla. The overall study demonstrated a mean error of 1.70
+/- 0.51 mm.
5.4.4 Augmented Reality Apps in Dental Applications
As compared to various AR based Navigation, Visualization and other HMD based displays, various AR based Mobile Applications are gaining ground and providing strong
base for dental surgeons to take patient’s treatment to next level. In this section of chapter,
various AR based Apps designed especially for assisting dental surgeons are enlisted:
Janus Health AR5: Janus Health AR app [21] makes use of machine learning technology to detect the patient’s mouth and replaces the patient’s mouth with 3D overlay
masked besides the lips. Apart from recognizing the mouth, the app is able to detect
almost 200 different types of smiles best fit for the patient. It also resizes the height,
width, thickness and curvature of the teeth of patient making the patient look like natural way. Overall the complete landscape of the patient’s mouth is depicted via this
app. The app gives live rendering dental modification of the patient’s teeth using front
camera of Phone or Tablet. The AR is highly efficient to simulate various teeth shades
ranging from OM1 to 3M4 on Vita 3D Master Shade.
Kapanu AR Engine6: Kapanu AR engine [22] is currently maintained and formulated
by ETH Computer Lab. The App works via 3D matching of the person’s mouth and
scan all set of patient’s teeth. The app can determine the exact teeth positions, shapes,
size and even spaces between the teeth and app gives a natural output on the screen
itself, what the patient should look like with complete dental surgery transformation.
The app is connected to strong information database comprising 3D models of natural teeth postures already implemented in dentistry. The app makes use of Machine
Learning approach to display the data and different options. The app gives almost
100% results with the patient’s teeth matching with natural smile and best mouth postures.
WHITEsmile App7: Another Dental AR based app for improvising cosmetic result
of teeth whitening with immediate effect. The app has high end machine learning
based algorithms for whitening teeth in real time and even on new or existing photos
on phone or tablet with high precision. The app has high end shade simulation with
automatic light environment detection on the image.
5.5 Augmented Reality in Dental Education
In addition to Virtual Reality, Augmented Reality is attracting interest with regard to Medical Education especially, Dental Education.
5www.getjanus.com/
6www.kapanu.com/
7www.whitesmile.de/whitesmile-app/
62 Emerging Technologies for Health and Medicine
By integrating AR based technology in real-education environment, new opportunities
will come to life. Currently, AR is providing deep roots in medical education especially
Laparoscopic surgery, Echocardiography and neurosurgery and slowly and steadily Dental
Education is also adapting AR technologies.
Figure 5.5 Dental Simulation
In Dental Education, pre-clinical training for dental students is a mix of theoretical
teaching and live patient exercise in laboratory which is quite costly, time consuming and
not highly efficient. Even with successful completion of pre-clinical training, the student
is not able to treat the patient with enough competent skills. New AR based technologies
address these problems and in recent years, various computer-oriented simulation tools and
systems are designed for assisting and developing pre-clinical dental interns with professional competencies with aid of: Intelligent Tutoring Systems, Dental Simulation, Virtual
Reality Technologies, Web 2.0 and even social networking tools. In recent years, Artificial Intelligence is also incorporated in varied technologies to improvise the quality of
teaching, live patient treatment and performing precise surgeries. These new technologies
provide comprehensive access to learning resources, quality interaction and cost reduction
in overall training.
5.6 Augmented Reality based Education Technologies for Dentistry
5.6.1 DentSim
DentSim8 [23] is regarded as industry leading AR based dental simulator, assisting students for improvising dental treatment skills. DentSim fully integrates with traditional lab
8www.dentsimlab.com/
Augmenting Dental Care: A Current Perspective 63
equipment’s and enable students to work on mannequins in real-time using computer aided
systems providing equivalent hands on real-time patient treatment environment. DentSim
is fully equipped with advanced cameras with GPS tracking capability providing students
with real-time 3D view of the work along with feedback of the operations performed.
Figure 5.6 DentSim Real Time Root Canal Treatment Simulation
DentSim provides the following unique features to dental students:
Knowledge acquisition using multimedia assistance with high end audio-visual content and high degree of interaction.
Enables students to work on personalized programs via digital tutor function.
2D knowledge is transferred into 3-D spatial work and 3D images can be analysed for
all sorts of errors.
Efficient quality control and real time feedback to the students and next generation
dental education.
5.6.2 The Virtual Dental Patient: System for Virtual Teeth Drilling
AIIA9 Laboratory Computer Vision and Image Processing Group, Department of Informatics, Aristotle University of Thessaloniki Greece, proposed Virtual Dental Patient: A
system for Virtual Teeth Drilling [24] to assist dental students to get fully acquainted with
teeth anatomy, teeth drilling equipment’s in addition to various real-time challenges in
9www.aiia.csd.auth.gr/
64 Emerging Technologies for Health and Medicine
handling patient’s teeth drilling. In addition to this, Virtual Dental patient is regarded as
efficient tool for assisting experienced dental surgeons for planning a real tooth drilling by
getting familiar with patient’s anatomy, landmarks identification, approach planning and
identifying prevision position of actual drilling activity.
The Virtual Dental Patient has following unique features:
A head/Oral cavity model is designed using 3D points on different head tissues using modelling techniques. A 3D surface model is designed using 1392 points and
2209 triangles using cryosections and CT data of patient using Visible Human Project
designed by National Institute of Health, USA. The complete model comprises of entire face, gums, palate, teeth, tongue, cheeks, lips, larynx and uvula. The model is
animated via MPEG-compatible facial animation player.
Virtual Tooth Drilling is performed using 3D structures representing drilling tools and
can enable the surgeon to learn almost any type of drilling. Four shapes i.e. Spherical, Cylindrical, Cylindrical-conical and conical are used using 3D mathematical
morphology using Phantom haptic device designed by SensAble Technologies. Tooth
drilling is performed using varied dental models stored in database constructed via
digitalization and post processing of teeth morphology.
5.6.3 Mobile AR Systems for Dental Morphology Learning
Juan et al. [25] proposed Mobile Augmented Reality System for Dental Morphology learning. The system is designed and developed using Unity 3D10 and Vuforia SDK11. Vuforia
makes use of computer vision techniques for recognizing and tracking various fiducial elements in real time like: Image Targets, Frame Markers, Multi-Image Targets, Cylinder
Targets, Virtual Buttons or Word Targets. The app uses mobile camera to track the image and capture the position and camera orientation relative to the center of the image
target. After capturing the image, the system transforms the image into Virtual Object.
The screen can be used to rotate, zoom in, zoom out the image. The system makes use of
AR technology for identifying: Triangular Ridge, Marginal Ridge, Buccal Cusps, Lingual
Cusps, Fosse and Grooves and Supplemental grooves. The App was tested using 38 Undergraduate, 6 Master Students and 11 Employees and study reported that understanding
the morphology structure was better in the students and results showed a whopping success
rare of 4.5/5 in overall evaluation.
5.6.4 Periosim
Haptic-Based 3D Virtual Reality Teaching and Training Simulator12. PerioSim [10, 26-
28], a VR (Virtual Reality) simulator was designed was developed at University of Illinois,
Chicago with join collaboration with Colleges of Dentistry and Engineering. The simulator
is highly efficient in simulating clinical periodontal procedures like periodontal probing,
detecting subgingival calculus using periodontal explorer and other subgingival topographies. It facilitates dental students to learn diagnosing and treating periodontal diseases
with aid of 3D virtual human mouth and tactile sensations via touching teeth surface, gingivae and calculus using precise virtual dental instruments.
10https://unity3d.com/
11https://www.vuforia.com/
12www.dentalhygienistsimulator.com/
Augmenting Dental Care: A Current Perspective 65
The Simulator has three types of Virtual Dental Instruments:
Periodontal Probe: It assists the students to measure pocket depth, determine the
health of tissue.
Scaler: Enables the students to feel virtual calculus on root surface. It aids in removing plaque and calculus from gum line.
Explorer: Overall observer to see, whether plaque removed completely or not.
5.7 Conclusion
In this Chapter, the complete history, Origin and utilization of Augmented Reality in Medical applications is stated. The chapter states in depth, utilization of Augmented Reality
in Dentistry focused towards various branches of dental treatments like Oral and Maxillofacial Surgery, Dental Implant Surgery, Orthognathic Surgery and Dental Education. In
addition to this, various AR based simulators available for educating dental students and
experienced dental surgeons are also enlightened. AR technology is expanding in other areas of Dental Care like Orthodontics, Endodontics via technological advancements. Augmented reality technology is enhancing its roots day-by-day by assisting dental surgeons
and students in different surgeries, Anatomy understanding and even Implants and these
days, AR technology is considered far more superior as compared to traditional dental
treatment methods. In the near future, with technologies like, Robotics, Mixed Reality,
3D Printing / 4D Printing, Machine Learning, Deep Learning, Haptics and even Internet
of Things/Everything, AR applications in dentistry are expected to become even more advanced as it can be observed today and will assist dental surgeons for precision treatment
to patients and build professional and skilled dental surgeons.
REFERENCES
1. Schmalstieg, D., & Hollerer, T. (2016). Augmented reality: principles and practice. AddisonWesley Professional.
2. Brohm, D., Domurath, N., Glanz-Chanos, V., & Grunert, K. G. (2017). Future trends of augmented reality. In Augmented reality for food marketers and consumers (pp. 1681-1685). Wageningen Academic Publishers.
3. Kwon, H. B., Park, Y. S., & Han, J. S. (2018). Augmented reality in dentistry: a current
perspective. Acta Odontologica Scandinavica, 1-7.
4. Huang, T. K., Yang, C. H., Hsieh, Y. H., Wang, J. C., & Hung, C. C. (2018). Augmented reality
(AR) and virtual reality (VR) applied in dentistry. The Kaohsiung journal of medical sciences,
34(4), 243-248.
5. Llena, C., Folguera, S., Forner, L., & RodrguezLozano, F. J. (2018). Implementation of augmented reality in operative dentistry learning. European Journal of Dental Education, 22(1),
e122-e130.
6. Huang, T. K., Yang, C. H., Hsieh, Y. H., Wang, J. C., & Hung, C. C. (2018). Augmented reality
(AR) and virtual reality (VR) applied in dentistry. The Kaohsiung journal of medical sciences,
34(4), 243-248.
66 Emerging Technologies for Health and Medicine
7. Chen, X., Xu, L., Sun, Y., & Politis, C. (2016). A review of computer-aided oral and maxillofacial surgery: planning, simulation and navigation. Expert review of medical devices, 13(11),
1043-1051.
8. https://www.rcseng.ac.uk/news-and-events/media-centre/media-background-briefings-andstatistics/oral-and-maxillofacial-surgery/ (Accessed on 10 May, 2018)
9. https://advancedsmiledentalcare.com/dental-implants-and-procedures/ (Accessed on May 10,
2018)
10. Rhienmora, P., Gajananan, K., Haddawy, P., Dailey, M. N., & Suebnukarn, S. (2010, November). Augmented reality haptics system for dental surgical skills training. In Proceedings of the
17th ACM Symposium on Virtual Reality Software and Technology (pp. 97-98). ACM.
11. Yamaguchi, S., Ohtani, T., Yatani, H., & Sohmura, T. (2009, July). Augmented reality system
for dental implant surgery. In International Conference on Virtual and Mixed Reality (pp. 633-
638). Springer, Berlin, Heidelberg.
12. Chen, X., Xu, L., Wang, Y., Wang, H., Wang, F., Zeng, X., ... & Egger, J. (2015). Development
of a surgical navigation system based on augmented reality using an optical see-through headmounted display. Journal of biomedical informatics, 55, 124-131.
13. https://www.taiwan-healthcare.org/biotech/biotech-product?vendorSysid=BhsProducts201612
08164204397824623 (Accessed on May 10, 2018)
14. https://image-navigation.com/igi/ (Accessed on May 10, 2018)
15. Steinhuser, E. W. (1996). Historical development of orthognathic surgery. Journal of craniomaxillo-facial surgery, 24(4), 195-204.
16. http://www.e-macro.ne.jp/en-macro/manmos mandibular motion tracking system.html (Accessed on May 10, 2018)
17. Fushima, K., & Kobayashi, M. (2016). Mixed-reality simulation for orthognathic surgery.
Maxillofacial plastic and reconstructive surgery, 38(1), 13.
18. Zhu, M., Chai, G., Zhang, Y., Ma, X., & Gan, J. (2011). Registration strategy using occlusal
splint based on augmented reality for mandibular angle oblique split osteotomy. Journal of
Craniofacial Surgery, 22(5), 1806-1809.
19. Suenaga, H., Tran, H. H., Liao, H., Masamune, K., Dohi, T., Hoshi, K., & Takato, T. (2015).
Vision-based markerless registration using stereo vision and an augmented reality surgical
navigation system: a pilot study. BMC medical imaging, 15(1), 51.
20. Badiali, G., Ferrari, V., Cutolo, F., Freschi, C., Caramella, D., Bianchi, A., & Marchetti, C.
(2014). Augmented reality as an aid in maxillofacial surgery: validation of a wearable system
allowing maxillary repositioning. Journal of Cranio-Maxillo-Facial Surgery, 42(8), 1970-1976.
21. http://getjanus.com/ (Accessed on May 10, 2018)
22. http://www.kapanu.com/ (Accessed on May 10, 2018)
23. https://image-navigation.com/home-page/dentsim/ (Accessed on May 12, 2018)
24. Marras, I., Papaleontiou, L., Nikolaidis, N., Lyroudia, K., & Pitas, I. (2006, July). Virtual dental patient: a system for virtual teeth drilling. In Multimedia and Expo, 2006 IEEE International
Conference on (pp. 665-668). IEEE.
25. Juan, M. C., Alexandrescu, L., Folguera, F., & Garcia, I. G. (2016). A Mobile Augmented
Reality system for the learning of dental morphology. Digital Education Review, (30), 234-
247.
26. Roy, E., Bakr, M. M., & George, R. (2017). The need for virtual reality simulators in dental
education: A review. The Saudi dental journal, 29(2), 41-47.
27. http://www.cvrl.cs.uic.edu/ stein/PeriosimUpdate08.htm (Accessed on May 12, 2018)
Augmenting Dental Care: A Current Perspective 67
28. Su Yin, M., Haddawy, P., Suebnukarn, S., Schultheis, H., & Rhienmora, P. (2017, March).
Use of Haptic Feedback to Train Correct Application of Force in Endodontic Surgery. In Proceedings of the 22nd International Conference on Intelligent User Interfaces (pp. 451-455).
ACM.
29. Nayyar, A., Mahapatra, B., Le, D., & Suseendran, G. (2018). Virtual Reality (VR) & Augmented Reality (AR) technologies for tourism and hospitality industry. International Journal
of Engineering & Technology, 7(2.21), 156-160.
69
Dac-Nhuong Le et al. (eds.), Emerging Technologies for Health and Medicine, (69–284) 
© 2018 Scrivener Publishing LLC
CHAPTER 6
REVIEW OF VIRTUAL REALITY
EVALUATION METHODS AND
PSYCHOPHYSIOLOGICAL
MEASUREMENT TOOLS
M.A. Munoz1, J.G. Tromp2, Cai Zhushun3
1 University of Granada, Spain
2 Duy Tan University, Vietnam
3 State University of New York, USA
Emails: mamuoz@ugr.es, jolanda.tromp@duytan.edu.vn, zcai@oswego.edu
Abstract
This chapter describes how scientific experiments can help to make informed design
choices for the Virtual Reality interface design and vice versa, Virtual Reality can help scientific research, such as psychotherapy and physiotherapy to improve medical treatment
and understand the functions of the brain better in response to virtual experiences that aim
to mimic the real-world experience, but can take place in the laboratory environment. The
chapter consists of an overview of the steps in the process of developing a VR setup, showing where and how evaluations take place. It describes the Human-Computer Interaction
design and evaluation methods, including psychophysiological measurements and tools,
when they can be used, and it discusses some of the factors that can jeopardize the quality
of experiments in VR
Keywords: Virtual Reality, Evaluation, Psychology, Neuroscience, Measurement tools.
70 Emerging Technologies for Health and Medicine
6.1 Science Can Help Inform Virtual Reality Development
To achieve VR systems that are highly usable and satisfactory, a user-centred approach is
employed in an iterative cycle of refinement of the design and evaluation of the new Virtual
Reality (VR) setup. A user-centred approach to design means making design choices with
respect and support for the needs of the actual end-users. Different design and evaluation
methods have been developed, with the end-user experience at the core. These methods
are relevant and useful at different stages of the development cycle. For instance, a user
task analysis and system task analysis start early on in the development process and will
form the basis for the evaluation criteria. The VR setup is created in an iterative cycle of
refinement of the design VR setup, via many smaller evaluation efforts, based on empirical measures of the usability of the design. This type of evaluation needs to take place
at strategic points throughout the development process. This type of evaluation is most
informative if it is done with real representative end-users if at all possible. And if it is
possible to observe the user while interacting with the VR setup in a realistic setting of use
that will also improve the quality of the observations of the user in action.
The process of VR Development follows the steps below, and keeps repeating in an
iterative manner, until the final configuration has been reached and/or time and finances
run out, see Figure 6.1. For more detailed descriptions of the framework for VR development, see [15] and for a more detailed description of the VR development process and the
appropriate evaluation methods during the VR development cycle, see [44].
Figure 6.1 The iterative process of VR Development
For VR researchers, psychology is a valuable tool to investigate the cognitive, behavioral and physiological changes during navigation in VR environments (VEs), and analyze
the information to inform the (re)design of user experience and user interface, to make their
Review of Virtual Reality Evaluation Methods 71
VEs more compelling and effective. In the last few years, psychophysiological methods
have started to be used to research several psychophysiological concepts that have been
observed in relation to VR exposure, such as Presence and Immersion.
The use of psychophysiological techniques allows us to directly capture, measure, analyze and store autonomic nervous system (ANS) activity recordings. It provides researchers and developers of VR systems access to the quantifiable and recordable experience of their desired end-user. Psychophysiology used in combination with other evaluation methods, will provide a complex, detailed account of both conscious and subconscious
processes, that can be quantified and compared in empirical studies, for example of fundamental topics for VR such as Presence and Immersion [1].
Psychophysiology as a research methodology was defined around 1964, when in the
Opening Editorial of Psychophysiology, Ax [2] offered a short description and the guidelines concerning the research of interest to psycho-physiologists. Albert Ax stated: ”Psychophysiology is a research area which extends observation of behavior to those covert
proceedings of the organism relevant to a psychic state or process under investigation
and which can be measured with minimal disturbance to the natural functions involved.
Psychophysiology provides a method for bringing both physiological and psychological
aspects of behavior into a single field of discourse by which truly organismic constructs
may be created”.
6.1.1 Objectives of Evaluations
An evaluation usually has a goal, a set of acceptance criteria. We are interested in the VR
user response to the interface or to the VR experience itself. We examine how participants
respond to different VE experiences or interactions with elements of the VE. We measure
how well they utilise all its functionality, many errors they make. We check to see whether
they are able to comprehend and use all the interface elements. In general and specific
terms, we are interested to know how and if the application meets its development goals.
We want to know if the application is usable and useful, whether it is a commercial VR
setup or an experimental VR setup, it has to work and it has to work well. See Figure 6.2
for a overview of the term ”usability” in relation to other similar terms and sources for
potential interface acceptance criteria.
Figure 6.2 Usability and other potential acceptance criteria, Nielsen’s Usability Diagram.
72 Emerging Technologies for Health and Medicine
The process of evaluation has the following recommended steps, see Figure 6.3. Specific care must be taken to protect the participants in VR experiments from harm, such
as simulator sickness, epileptic fits, and other effects of exposure to this new technology,
and it is recommended to adhere strictly to the guidelines for ethical research for VR, for
instance see [26].
Figure 6.3 The process of empirical evaluation
For empirical testing, one of the requirements is to assign participants to different conditions in a random fashion, so that we can rule out that any differences observed between
and within the groups in the different conditions, are caused by preferential assignment
of individuals to the different conditions of the experiment. There may also be more than
two conditions (A/B), necessary to address certain research questions, so that we may have
three (A/B/C), or four (A/B/C/D) groups i.e. experimental conditions, and some research
designs could call for even more. More information on Human-Computer Interaction design and evaluation methods can be found in [19, 30] and specifically for VR [15, 40, 43,
44]. The main evaluation goals for VE interface testing and measuring human response
are:
Navigation: Does the user know how to get around the VE and the approach and
orient to objects in the VE with ease and efficiency?
Recognition: Does the user recognize the objects, the other users (if available) and
the task interactions in the VE with ease?
Interaction: Does the user recognize and understand how to interact efficiently and
satisfactory with the interactive objects and other interactive elements in the VE?
Review of Virtual Reality Evaluation Methods 73
Side Effect and After Effect: Does the user experience any side effects or after effects
of the VE experience or the VE interface (hardware and software); these could be
desirable (such as after training in the VE) or undesirable (such as nausea caused by
the VE experience, leading to simulation sickness)?
There are a number of known factors which may influence the success of evaluations
[14]. Problems during the evaluation that can confound the outcome of the evaluation are:
Major bugs and gaps in task-flow: There may be a lack of fidelity which makes
functionality difficult to recognize for the user, or problems with the validity of the
VE itself, cause a break-down of the task-flow for the user.
Interface to the application: The interface to the VE may give problems to the user,
so that they are unable to get on with their actual task, the task inside the VE, or vice
versa, the task in the VE may be so complicated that no matter how well the UI is
designed the user will struggle.
Application not ready to test with end-users: The actual users may not be able to help
test prototypes because it may be hard for them to understand which functionality is
still being developed and which is ready, which may or may not match the expectations of the anticipated user population, or the user used for the tests are different from
the real end-users of the application. This can be particularly problematic if there are
major differences in the areas of aptitude and motivation.
Simulation sickness: Movement or interaction with(in) the VE causes users to feel
nausea and simulation sickness due to rapid movements outside of the users control
or due to the task asking for rapid head movements from the user.
To ensure that the VR systems has good usability, each task-action and sub-action that is
part of the total VR experience has to be designed and tested carefully with reference to the
intended users and their skills, knowledge and task-requirements. This can be facilitated
by developing task (and sub-task) analyses (TAs), that are recommended to be made at
each iterative stage of the development process and task that is being designed.
6.1.2 Test Often and Test Early
Different evaluation methods are appropriate at different points of the development cycle,
because some methods can be used before the application is ready enough to test with real
end-users. See table 1 for an overview of the methods and their associated parameters
for selection. In all design elicitation and evaluation methods it is advisable to use real
representative end-users, where possible. Sometimes a method can be adapted or tailored
to a specific development phase or used in multiple phases, it all depends on the available
resources. Some methods are more suitable to phases than others, for instance, some can be
used to perform early evaluation of the task flow and the actions and sub-actions, until all
steps of the task have been documented and assessed. This type of early evaluation method
uses a task analysis as input and does not require end-users, and it applies a Cognitive
Walkthrough (CW) and / or a Heuristic Evaluation (HE) to the steps of the task. The CW
and HE methods are further described in [19, 30]. Some methods can be used multiple
times during the different phases of the development process. Once a stable design has
been developed, empirical experiments can be done. As long as the application is not
stable when it is being used for a test with real end-users, you risk preparing everything for
a certain time, only to find out that the application has crashed beyond a quick repair.
74 Emerging Technologies for Health and Medicine
Table 6.1 Overview of Design and Evaluation methods and recommended time and setting for
using them
The most typical Human-Computer Interaction design and evaluation tools are briefly
described below. The numbers correspond to Table 6.1 and the overview of the design and
evaluations met, there are many descriptions of these methods online, so where possible
we have provided a link to a longer explanation. hods.
1. An interview is a conversation with a purpose. This process is for researchers to
collect user data, including their needs, wants, expectation, etc. There are three types
of interview: structured, semi-structured, and unstructured; with different types of
questions: closed-end questions, yes or no question, and open-end question. Each of
them is good for either comparing individuals or getting more insights, depends on
the need of researchers12.
1http://designresearchtechniques.com/casestudies/semi-structured-interviews/
2https://www.nngroup.com/articles/interviewing-users/
Review of Virtual Reality Evaluation Methods 75
2. Diary keeping is a method for researchers to collect temporal or longitudinal qualitative data from the end user in a natural context of the interaction. The data includes
but not limited to thoughts and experience of using systems3.
3. A survey is a commonly used tool for researchers to reach a wide range of users.
Researchers can include closed-end questions and open-end questions in the survey
to collect quantitative and qualitative data45.
4. Attitude & Opinion Questionnaire is designed for researchers to find out user’s attitude and opinions toward certain ideas throughout the research process. The questionnaire is usually built using Likert Scales, which contain multiple opinion statements
that can be used to measure attitudes and opinions when combined together6.
5. Sketching is a method that researchers use in the early design stages. Researchers use
simple tools such as pencil and paper to produce design sketches. The key is to strike
for quantity, not quality. Using this method, multiple design idea can be generated
within a short time, then share and discuss with other researchers and users7.
6. Focus Group is a method that requires 6-9 users having a discussion about their concerns about the design. Focus group helps researchers to get an autonomous reaction
of users and some group dynamic8.
7. Persona is a fictitious user that has the characteristics and needs of the specific user
group (data from user research). Researchers can use persona to create user empathy
among the research team910.
8. Storyboarding is a design tool that is made of sequential art which portrays the story
of a user using the design. It is a helpful tool for a researcher to gain user empathy by
walking in their shoes11.
9. Ethnographic Observation is a method to observe users in their life rather than in a lab.
This observation method can help researcher gain insights of users using the system
in a natural environment12.
10. Scenario Descriptions is a description of a user using a system. It helps researchers to
have a good understanding of users requirement13.
11. Hierarchical Task Analysis is a method to analyse users task-step and all the subtasks
necessary to complete a certain task. It can be used to analyse the interaction between
a user and a system in a objective way14.
3http://uxpamagazine.org/dear-diary-using-diaries-to-study-user-experience/
4http://uxpamagazine.org/writing-usable-survey-questions/
5https://www.nngroup.com/articles/qualitative-surveys/
6https://legacy.voteview.com/pdf/Likert 1932.pdf 7http://uxpamagazine.org/design like da vinci/ 8https://www.nngroup.com/articles/focus-groups/
9https://www.nngroup.com/courses/personas/
10http://uxpamagazine.org/current-customers/
11https://uxplanet.org/storyboarding-in-ux-design-b9d2e18e5fab
12https://www.experienceux.co.uk/faqs/what-is-ethnography-research/
13http://infodesign.com.au/usabilityresources/scenarios/
14https://www.uxmatters.com/mt/archives/2010/02/hierarchical-task-analysis.php
76 Emerging Technologies for Health and Medicine
12. Functionality Matrix is way to show a collection of main function in the system in a
prioritized manner15.
13. Paper prototype is a sketch on a paper that mimics digit representation. it is the fastest
and cheapest prototype a researcher can build. Not only an interaction tool, paper
prototype can also used as a tangible document that can includes notes for future
design16.
14. Scenario Building is a method that researchers used to think about possible future in
a systematic and creative manner17.
15. Task allocation needs to be done to create a system with good balance of user task
and system task. Methods for tasks allocations are context analysis, task analysis,
mandatory allocation, provisional allocation, and evaluation18.
16. Use Case Description is a written description of how a user finish certain task in a
system, begins with the user goal, and end with user’s goal is fulfill. Some use case
will be chosen by the project team as requirement of the system19.
17. Cognitive Walkthrough is a task-specific method for a researcher to test whether a
new user can complete a task in a giving system. It is very cost-efficiency compared
to other usability test20.
18. Heuristic Evaluation requires few expert evaluators to assess a system using accepted
evaluation principles. This method can help researchers diagnose system errors before
release21.
19. Controlled Testing is a widely used method by researchers in different fields. Researchers test the variables by controlling other confounding variables in an experiment22.
20. The main Side Effects & After Effects of virtual reality is cybersickness. Its symptoms
include but not limited to nausea, headaches, dizziness, fatigue, sweating and eye
strain. For measuring cybersickness, researchers can use physical measurement such
as heart rate, blink rate and electroencephalography, or subjective measurement using
Simulator Sickness Questionnaire23.
21. Transfer Effects to Real World: VR training is one of the main focus in VR development community. Thus it is essential to understand how skills learned in VR are
transferred to the real world environment24.
15http://www.scottburkett.com/process-improvement/jad-creating-a-functionality-matrix-107.html
16https://www.uxpin.com/studio/blog/paper-prototyping-the-practical-beginners-guide/
17http://www.pugetsoundnearshore.org/program documents/ps future appenda-i.pdf 18http://www.usabilitynet.org/tools/taskallocation.htm
19https://www.usability.gov/how-to-and-tools/methods/use-cases.html
20https://www.interaction-design.org/literature/article/how-to-conduct-a-cognitive-walkthrough
21https://www.interaction-design.org/literature/topics/heuristic-evaluation
22https://www.khanacademy.org/science/high-school-biology/hs-biology-foundations/hs-biology-and-thescientific-method/a/experiments-and-observations
23https://dl.acm.org/citation.cfm?id=2677780
24https://www.researchgate.net/publication/12516600 Training in virtual environments Transfer to real world
tasks and equivalence to real task training
Review of Virtual Reality Evaluation Methods 77
22. The Physiological Effects: Some specific tasks such as performing a surgery and
welding tasks needs complex and precise muscle moments. Thus it is important to
measure physiological effects of VR training. Researchers can use electromyography (EMG) as a measurement tool to get feedback from user’s muscles during VR
trainings25.
23. Psychological Effects: Multiple researches have shown that psychological Therapies
combined with VR technology is effective on reducing stress and treating psychological disorders. Researchers can use different measurement that already exists in the
field of psychology to measure the psychological effect of VR2627.
24. Health & Safety tests: Researchers has found VR technology can improve engagement of safety training and improve memorability of its content28.
25. Usability Questionnaires: the after scenario questionnaire (ASQ), Post-Study System
Usability Questionnaire (PSSUQ) and System Usability Scale (SUS) are some of the
standardized and ready to use questionnaires, that provide a quick benchmark tool,
for comparing usability scores of different designs (A/B testing).
ASQ: after scenario questionnaire is a 7-point-scale questionnaire to measure
user’s usability satisfaction towards a system. users can fill out the questionnaire
after they finish a task in a scenario29.
PSSUQ: the Post-Study System Usability Questionnaire is an questionnaire to
measure usability of a system in a scenario based study. It contains sixteen 7-
point scale questions and should be taken by users after they finish all the tasks in
a study30.
SUS: The System Usability Scale is a 5-point scale questionnaire for quantifying
usability of a system. This is a light-weight questionnaire with only 10 general
questions, therefore it can test a wide-variety of systems31.
6.1.3 Testing Options in the Early Pre-Prototype Phase
During the early stages of development of the VR setup, in many cases there will not be a
ready-to-use VE, to use for running tests. At this early point in the development process
it can be very helpful for the future activities of the design and the team, if simulations
are used to discuss and design the key features and interaction points. Once the design is
sufficiently clarified (in terms of what the system should do and what the user should do
and what the dialog is between them), a prototype (even if it is a series of sketches) can
be made that can be used with representative end-users. A user under guidance from the
experimenter, tries each step of the task-analysis and the experimenter makes notes about
25https://www.lincolnelectric.com/en-gb/equipment/training-equipment/Documents/Physiological and Cognitive
Effects of Virtual Reality Integrated Training 13July2011.pdf 26https://www.researchgate.net/publication/226028654 Virtual Reality Exposure Therapy for Anxiety Disorders
The State of the Art 27http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0187777
28https://ieeexplore.ieee.org/document/7817889/
29http://ehealth.uvic.ca/resources/tools/UsabilityBenchmarking/05a-2011.02.15-
ASQ and PSSUQ Questionnaires-no supplements-v3.0.pdf 30https://www.trymyui.com/blog/2017/02/24/sus-pssuq-website-usability-surveys/
31https://www.trymyui.com/blog/2017/02/24/sus-pssuq-website-usability-surveys/
78 Emerging Technologies for Health and Medicine
breakdowns in task flow that can be observed when the end-user tries to interact with the
prototype.
6.2 Virtual Reality Can Help Inform Psychology and Science
Virtual reality (VR) is currently being applied in many health care services and appears
poised to enter mainstream use-case scenarios for delivery of psychotherapy. Since the
early 1990s, when Hodges and colleagues (1995; 1993) reported on a project that used
virtual environments to provide acrophobic patients with fear-producing experiences of
heights in a safe situation, VR has been proposed as a new medium for psychological
therapy. The use of VR has been studied in others therapeutic conditions, including anxiety,
obesity, chronic pain, eating disorders and addictions [9, 18, 29, 32, 39].
VR technology, with its unique ability to simulate complex, real situations and contexts, offers to psychologies and clinicians unprecedented opportunities of treatment and
investigation with an exacting degree of control over relevant variables. The use of VR in
psychology research offers several advantages:
Controlled virtual environments allows naturalistic behaviors while physiological activity is monitored. This allows researchers to study physiological responses (peripheral and central) to situations which would simply not be possible to study out of
laboratory ’in the wild’.
VR environments allow researchers to manipulate widespread number of environment
variables, easily, economically and with high realistic level. The multimodal stimulus
inputs provide realistic stimulation at once, involving affective, cognitive, and behavior systems more fully than the simple stimuli as imagen, sounds or tasks, increasing
the potential to elicit psychological and behavioral responses.
VR also offers maximal control over stimuli. Virtual environments can present combinations of stimuli that are not found in the natural world and researchers can execute
changes in the environment that would not be possible physically. For instance, in order to treat phantom pain, VR is used to create a visual illusion of amputated limb
whereby the limb appears to be wholly intact and without pain in virtual environment
[13].
VR technology offers the potential to develop human performance testing environments that could supplement traditional neuropsychological procedures and could
conceivably improve accepted standards for psychometric reliability and validity of
the measurements. Neuropsychological assessment of persons with acquired brain
injury and/or neurological disorders serves a number of functions, including assisting in determining a diagnosis; provision of normative information on the status of
cognitive and functional abilities; assisting in producing guidelines for the design of
rehabilitative strategies; and creating data for the scientific understanding of brain
functioning through the examination of measurable sequelae due to different types of
brain damage or dysfunction [6, 45].
VR provides opportunities to enlarge the actual limits of neuropsychological rehabilitation providing valuable scenarios with common elements for the patients, training
them in rehabilitation of daily life activities and mental processes such as attention,
Review of Virtual Reality Evaluation Methods 79
memory, language skills, visual and auditory processing, spatial skills, executive functioning, logical reasoning and problem-solving [17, 31].
The peripheral devices used to create interactive simulations, can be used as rehabilitation instruments in the treatment of several diseases, such as ischemic stroke,
cerebral palsy or Parkinson’s disease [5, 34]. Several studies have shown that when
biofeedback is delivered through a display, sound, or haptic signal, it can serve as a
correctional mechanism for the patient and as a monitoring mechanism for the therapist. VR displays can integrate biofeedback notifications into simulations just as
games might use status bars, numerical displays, and written or spoken notifications.
Thus, VR sensors data can be used as a mechanism to improve dynamic balance,
functional mobility and strength for patients [1].
6.3 Types of Psychophysiological Measures and Tools
There are many types of measurements and tools for measuring psychophysiological signals. What follows is a description of some of these psychophysiological measures, measurements and measurement tools, including reference to how they have been used previously in other research and development settings.
6.3.1 Electrodermal Activity
Skin conductance response (SCR), or electrodermal response, is a physiological index of
activity of the sweat glands. Skin conductance is quantified by applying a weak electrical potential between two points of skin and measuring the resulting current flow between them. Skin conductance measurements comprise background tonic (skin conductance level: SCL) and rapid phasic component (Skin Conductance Responses: SCRs) that
result from sympathetic neuronal activity consequence of stimuli [3].
The recording of SCR has to be realize with silver-silver chloride electrodes to minimize
polarization which can affect the subject’s conductance. The electrodes are placed where
the concentration of eccrine sweat glands is the highest-the palmar surface of the hands or
fingers or the soles of the feet. The electrode jelly is the conductive medium between the
electrodes and the skin. Commercial electrode jellies can be used for SC recording, but
only if they have neutral pH [41].
SCR can be used as an objective index of emotional states, being used to examine implicit emotional responses that may occur without conscious awareness. Others researches
have shown that SCR is also a useful indicator of attentional processing perse, where salient
stimuli and resource demanding tasks evoke increased Electrodermal Activity (EDA) responses. In relation to VR, SCR have been found to correlate significantly with reported
presence and realism [2].
6.3.2 Cardiovascular activity
The most popular measures of cardiovascular activity include Heart Rate (HR), interbeat
interval (IBI), heart rate variability (HRV) and blood pressure (BP). The three first measurements are generated from electrical activity of the heart (electrocardiography: EKG)
when electric impulse spreads across the muscle of the heart. Last one, is a mechanical
response consequence of force originating in the pumping action of the heart, exerted by
the blood against the walls of the blood vessels.
80 Emerging Technologies for Health and Medicine
Figure 6.4 Electrode Placement to recording Galvanic Skin Response
The EKG recording has to be realize with silver-silver chloride electrodes together conductive jelly to facilitate the contact with skin. Although there is a standardized system
for electrode placement on the limbs and chest utilized for medical diagnosis, all that is
required for an EKG of quality is that two electrodes be placed on the skin fairly far apart.
Psychophysical recording uses standard limb leads, designated as follows [41]:
1. One electrode on each arm;
2. Right arm and left leg;
3. Left arm and left leg.
Changes in HR have been related with emotional activity. It has been used to differentiate between positive and negative emotional states, such as relaxing, stress, afraid or
angry. Dillon et al. [11] investigated the effects of content of a video clip (amusement,
sadness, neutral) over HR. The results showed that HR was a greater lowering of HR for
Amusement and Sadness material than for Neutral material. Likewise, virtual environments containing stressful versus non-stressful situations have showed that HR is significantly higher in the stressful environments [28]. Furthermore, HR has found to correlate
significantly with sense of presence and reported behavioural presence as measured by a
questionnaire [10, 16]. In general, it can be assumed that as the sense of presence in a VR
increases, the physiological responses to the environment will become increasingly similar to those exhibited in a similar real environment [22]. Thus, mora cardiac reactivity is
associated with high sensation of be there.
6.3.3 Muscular Activity: Facial Expressions
Electromyography (EMG) measures muscle activity by detecting surface voltages that occur when a muscle is contracted [41]. When used on the face, EMG has been used to
distinguish between positive and negative emotions. EMG activity over the eyebrow (cor-
Review of Virtual Reality Evaluation Methods 81
rugator muscle) region is lower and EMG activity over the cheek (zygomatic muscle) is
higher when a stimulus is evaluating as positive, as opposed to negative [4].
In order to register EMG activity, it uses surface electrodes with low impedance and
non-polarizing. Thus, commercial electrodes are either a combination of silver and silver
chloride or carefully chloride silver. There are several commercial electrode jellies which
can be used for EMG recording. The dimension of electrodes depends of muscle dimension. Small electrodes (about 4 mm) are recommend to facial recording, while higher to
arm or legs recording (8 mm). The electrodes of a pair will record the difference in electrical potential between them originating in nearby, and to a lesser degree, distant muscle
tissue. The two principal considerations to placement of the pair electrodes are [44], see
Figure 6.5.
1. Both electrodes should be only the same muscle or muscle group, and
2. The pair should, where possible, be on a line parallel with the muscle fibers.
The muscle activity is an index of the physical embodiment of various mental states,
such as emotions, stress, or fatigue. Facial EMG provides an index of internal emotional
state when participants are in emotional environments [16, 27]. Moreover, clear evidence
has recently emerged that facial EMG activity change in social situations or when the
subject is interacting with a virtual character [36]. The Also, EMG technique has been
applied in research on ergonomics and prosthetics. For example, EMG measurements
have been used to study the effect of posture on performance in work at a computer [33].
Wearable EMG clothes can register the activity of leg muscles during standing and walking
[42], and EMG can also be used as a control signal for prosthetic limbs or rehabilitation
[29].
Figure 6.5 Electrode Placement to recording facial expressions with EMG
6.3.4 Electrical brain activity: Electroencephalography
Electroencephalography (EEG), provides electric brain activity in an affordable and noninvasive way. In addition, EEG is relatively easy to set up, suitable for recording outside
82 Emerging Technologies for Health and Medicine
a laboratory setting and cheaper compared with metabolic techniques, such as functional
magnetic resonance imaging (fMRI) or Magnetoencephalography (MEG). Two types of
measurements can be realized:
(1) Spontaneous EEG, changes in synchronization of neural activation;
(2) Event-Related Potentials, signatures of brain activity that are time-locked to a specific
stimulus event, as the occurrence of incident in the virtual environment [41].
Today almost all EEG procedures use a variety of EEG helmets with up to 64 electrodes
built into the helmet, referred to the International 10-20 system (Jasper, 1958). The name
10-20 refers to the fact that electrodes in this system are placed at sites 10% and 20%
from four anatomical landmarks: the nasion (the bridge of the nose) to the inion (the bump
at the back of the head just above the neck) and the left to right on pre-auricular points
(depressions in front of the ears above the cheekbone) [41].
The EEG signal can be useful evaluating subject cognitive load and processing capacity
in ecologically valid settings, such as flight simulators [25], during simulated driving [23,
46], and in safety-critical monitoring [24]. In last few decades, EEG activity has been
used as an input for controlling a computer or other peripheral systems. Brain computer
interface (BCI) system extract specific features of brain activity and translate them into
commands that operate a device. Thus, a BCI system derives and utilizes control signals
that allow to subjects make a selection or engage with virtual environment, computer cursor
or a robotic wheelchair [7, 8].
6.4 Outcome of the Evaluation
The outcome of the evaluation will be a set of data, collected with the aim of answering a
more of less explicitly stated hypothesis. The data is collected and analysed and a detailed
report of documents the results. This report describes how well participants responded to
the different elements of the VE experience, whether they were able to use all the UX/UI
functionalities, and/or the goals of the experiment were reached.
Figure 6.6 The development cycle, using Rapid prototype <> test cycles by the Interaction Design
Foundation
Review of Virtual Reality Evaluation Methods 83
The experimental data and other information is then stored for future reference, dissemination to the scientific forum, added to the requirements specifications to inform the
design of the VE, and/or used to develop or refine, experimental empirical explorations of
user responses to VR experiences of interest to researchers. The process of developing a
VR setup is iterative and follows a cycle of continuous refinement and evaluations repeat
until a final version has been reached or a final understanding of the human response to
the VR experience has been reached. The final version is then ready for deployment. See
Figure 6.6 for an illustration from the Interaction Design Foundation32, showing the larger
framework within which the development cycle takes place.
6.5 Conclusions
Apart from being objective and quantifiable, psychophysiological measurement data tends
to be continuous, allowing for the assessment of characteristics that vary over time - such
as varying degrees of presence or levels of immersion. It can provide a rich source of
quantitative and qualitative data, benefiting from the opportunity and using a mixed design
for the tests.
A challenge for the psychophysiological data measures and analysis process can be
caused by the fact that it can be difficult to determine exactly what the detailed causal
effects are, i.e. what is being measured. This is further complicated by the fact that we
have not fully mapped out how our brains respond to VR and multiple factor will affect
our experiences of it. For instance, difficulties with finding the interface controls and being able to control their orientation in the Virtual or Augmented Reality world may occur
and the user may not be able to overcome them. This successful onboarding effect may
confound the measurement of the effects of the experimental conditions. It is generally
recommended to allow a new VR/AR user or new use VR/AR use scenario, or experiment,
to start with some interaction training to create opportunity for them to develop a sense
of self-identification with the virtual embodiment, avatar, or point-of-view and visual information from the VR/AR headset.” Another problem with data collection can be caused
by the novelty factor of the new technology, positively or negatively influencing people’s
opinions. Finally, the psychophysiological measurement tools my make it difficult or impossible to recreate a natural setting of use for the experiment, thus influencing the data
that is being collected. For this and obvious reasons, this means that rigorous empirical
designs of the experiments with statistical analyses of the data are essential.
REFERENCES
1. Bang, Y. S., Son, K. H., & Kim, H. J. (2016). Effects of virtual reality training using Nintendo
Wii and treadmill walking exercise on balance and walking for stroke patients. Journal of
physical therapy science, 28(11), 3112-3115.
2. Baren, J. van, & IJsselsteijn, W. (2004). Measuring Presence: A Guide to Current Measurement
Approaches. Deliverable of the OmniPres project IST-2001-39237.
3. Boucsein, W., Fowles, D. C., Grimnes, S., Ben-Shakhar, G., Roth, W. T., Dawson, M. E., . .
. Society for Psychophysiological Research Ad Hoc Committee on Electrodermal Measures.
32https://www.interaction-design.org/
84 Emerging Technologies for Health and Medicine
(2012). Publication recommendations for electrodermal measurements. Psychophysiology, 49,
10171034.
4. Cacioppo, J. T., Berntson, G. G., Larsen, J. T., Poehlmann, K. M., & Ito, T. A. (2000). The
psychophysiology of emotion. Handbook of emotions, 2, 173-191.
5. Camara Machado, F. R., Antunes, P. P., Souza, J. D. M., Santos, A. C. D., Levandowski, D.
C., & Oliveira, A. A. D. (2017). Motor improvement using motion sensing game devices for
cerebral palsy rehabilitation. Journal of motor behavior, 49(3), 273-280.
6. Canty, A. L., Fleming, J., Patterson, F., Green, H. J., Man, D., & Shum, D. H. (2014). Evaluation of a virtual reality prospective memory task for use with individuals with severe traumatic
brain injury. Neuropsychological Rehabilitation, 24(2), 238-265.
7. Chaudhary, U., Birbaumer, N., & Ramos-Murguialday, A. (2016). Braincomputer interfaces
for communication and rehabilitation. Nature Reviews Neurology, 12(9), 513.
8. Coogan, C. G., & He, B. (2018). Brain-computer interface control in a virtual reality environment and applications for the internet of things. IEEE Access, 6, 10840-10849.
9. Dascal, J., Reid, M., IsHak, W. W., Spiegel, B., Recacho, J., Rosen, B., & Danovitch, I. (2017).
Virtual Reality and Medical Inpatients: A Systematic Review of Randomized, Controlled Trials. Innovations in Clinical Neuroscience, 14(1-2), 1421.
10. Diemer, J., Alpers, G. W., Peperkorn, H. M., Shiban, Y., & Mhlberger, A. (2015). The impact
of perception and presence on emotional reactions: a review of research in virtual reality.
Frontiers in psychology, 6, 26.
11. Dillon, C., Keogh, E.,& Freeman, J. (2002). ’It’s been emotional’: Affect, physiology and
presence. In F.R. Gouveia, & F. Biocca (Eds). Proceedings of the 5th International Workshop
on Presence.
12. Dillon, C., Keogh, E., Freeman, J., & Davidoff, J. (2000, March). Aroused and immersed:
the psychophysiology of presence. In Proceedings of 3rd International Workshop on Presence,
Delft University of Technology, Delft, The Netherlands (pp. 27-28).
13. Dunn, J., Yeo, E., Moghaddampour, P., Chau, B., & Humbert, S. (2017). Virtual and augmented reality in the treatment of phantom limb pain: a literature review. NeuroRehabilitation,
40(4), 595-601.
14. Eastgate, R., (2001), The structured development of virtual environments: enhancing functionality and interactivity. PhD Thesis, University of Nottingham.
15. Eastgate, R.M, Wilson, J. R. & D’Cruz, M. (2015). Structured development of virtual environments. In K. Stanney (Ed.), Handbook of virtual environments: design, implementation and
applications, 2nd Edition, CRC Press, USA, pp.353-391.
16. Egan, D., Brennan, S., Barrett, J., Qiao, Y., Timmerer, C., & Murray, N. (2016, June). An
evaluation of Heart Rate and Electrodermal Activity as an objective QOE evaluation method
for immersive virtual reality environments. In Quality of Multimedia Experience (QOMEX),
2016 Eighth International Conference on (pp. 1-6). IEEE.
17. Garca-Betances, R. I., Jimnez-Mixco, V., Arredondo, M. T., & Cabrera-Umpirrez, M. F.
(2015). Using virtual reality for cognitive training of the elderly. American Journal of
Alzheimer’s Disease & Other Dementias, 30(1), 49-54.
18. Gutirrez-Maldonado, J., Wiederhold, B. K., & Riva, G. (2016). Future directions: how virtual reality can further improve the assessment and treatment of eating disorders and obesity.
Cyberpsychology, Behavior, and Social Networking, 19(2), 148-153.
19. Helander, M., Landauer, T., Prabhu, P., (eds.), (1997). Handbook of Human-Computer Interaction, 2nd ed., Elsevier, The Netherlands, pp. 705-715 and 717-731.
20. Hodges, L.F., Bolter, J., Mynatt, E., et al. (1993). Virtual environments research at the Georgia
Tech GVU Center. Presence, Teleoperators, and Virtual Environments 2:234243
Review of Virtual Reality Evaluation Methods 85
21. Hodges, L.F., Rothbaum, B.O., Kooper, R., et al. (1995). Virtual environments for treating the
fear of heights. IEEE Computer 28:2734.
22. IJsselsteijn (2004). Presence in Depth. Ph.D. Thesis. Eindhoven University of Technology
23. Khaliliardali, Z., Chavarriaga, R., Gheorghe, L. A., & del R Milln, J. (2015). Action prediction
based on anticipatory brain potentials during simulated driving. Journal of neural engineering,
12(6), 066006.
24. Kohani, M., Berman, J., Catacora, D., Kim, B., & Vaughn-Cooke, M. (2014, September). Evaluating Operator Performance for Patient Telemetry Monitoring Stations Using Virtual Reality.
In Proceedings of the Human Factors and Ergonomics Society Annual Meeting (Vol. 58, No.
1, pp. 2388-2392). Sage CA: Los Angeles, CA: SAGE Publications.
25. Kryger, M., Wester, B., Pohlmeyer, E. A., Rich, M., John, B., Beaty, J., ... & Tyler-Kabara,
E. C. (2017). Flight simulation using a Brain-Computer Interface: A pilot, pilot study. Experimental neurology, 287, 473-478.
26. Madary, M, Metzinger, T.K., (2016). Real Virtuality: A Code of Ethical Conduct. Recommendations for Good Scientific Practice and the Consumers of VR-Technology, Frontiers in
Robotics and AI, February 2016, https://doi.org/10.3389/frobt.2016.00003
27. Mandryk, R. L., Atkins, M. S., & Inkpen, K. M. (2006, April). A continuous and objective
evaluation of emotional experience with interactive play environments. In Proceedings of the
SIGCHI conference on Human Factors in computing systems (pp. 1027-1036). ACM.
28. Meehan, M., Insko, B., Whitton, M., & Brooks, F. P. (2001). Physiological measures of
presence in virtual environments. In Proceedings of 4th International Workshop on Presence.
Philadelphia, USA, 21-23 May, 2001.
29. Muoz, M. ., Idrissi, S., Snchez-Barrera, M. B., Fernndez-Santaella, M., & Vila, J. (2013).
Tobacco craving and eyeblink startle modulation using 3D immersive environments: A pilot
study. Psychology of Addictive Behaviors, 27(1), 243.
30. Nielsen, J, Mack, R.L, (1994). Usability inspection methods, John Wiley and Sons, New York,
NY.
31. Nolin, P., Stipanicic, A., Henry, M., Lachapelle, Y., Lussier-Desrochers, D., & Allain, P.
(2016). ClinicaVR: Classroom-CPT: A virtual reality tool for assessing attention and inhibition
in children and adolescents. Computers in Human Behavior, 59, 327-333.
32. Parsons, T. D., & Rizzo, A. A. (2008). Affective outcomes of virtual reality exposure therapy
for anxiety and specific phobias: A meta-analysis. Journal of behavior therapy and experimental psychiatry, 39(3), 250-261.
33. Pontonnier, C., Dumont, G., Samani, A., Madeleine, P., & Badawi, M. (2014). Designing
and evaluating a workstation in real and virtual environment: toward virtual reality based
ergonomic design sessions. Journal on Multimodal User Interfaces, 8(2), 199-208.
34. Powell, W., Rizzo, A., Sharkey, P., & Merrick, J. (2017). Virtual reality: recent advances in
virtual rehabilitation system design. Nova Science Publishers.
35. Preece, J., Rogers, Y., Sharp, H., (2002). Interaction Design: Beyond Human-Computer Interaction, John Wiley and Sons, Inc., USA, pp. 420-425.
36. Ravaja, N., Bente, G., Katsyri, J., Salminen, M., & Takala, T. (2016). Virtual character facial
expressions influence human brain and facial EMG activity in a decision-making game. IEEE
Transactions on Affective Computing.
37. Ravaja, N., Cowley, B., & Torniainen, J. (2016). A short review and primer on electromyography in human computer interaction applications. arXiv preprint arXiv:1608.08041.
38. Rodrguez-rbol, J., Ciria, L. F., Delgado-Rodrguez, R., Muoz, M. A., Calvillo-Mesa, G., &
Vila, J. (2013). Realidad virtual: una herramienta capaz de generar emociones. Anuario de
Psicologa Clnica y de la Salud Annuary of Clinical and Health Psychology.
86 Emerging Technologies for Health and Medicine
39. Rossell, F., Muoz, M. A., Duschek, S., & Montoya, P. (2015). Affective modulation of brain
and autonomic responses in patients with fibromyalgia. Psychosomatic medicine, 77(7), 721-
732.
40. Stanney, K., (2015). (Ed.), Handbook of virtual environments: Design, Implementation, and
Applications, 2nd Edition, CRC Press, USA.
41. Stern, R. M., Ray, W. J., & Quigley, K. S. (2001). Psychophysiological recording. Oxford
University Press, USA.
42. Tikkanen, O., Haakana, P., Pesola, A. J., Hkkinen, K., Rantalainen, T., Havu, M., ... & Finni,
T. (2013). Muscle activity and inactivity periods during normal daily life. PloS one, 8(1).
43. Tromp, J.G., Steed, A., Wilson, J., (2003). Systematic Usability Evaluation and Design Issues
for Collaborative Virtual Environments, in: Presence: Teleoperators and Virtual Environments,
Vol 12 (3), pp.241-267.
44. Tromp, J.G., Le, Chung, V., Nguyen, Tho, L. (2018). User-Centered Design and Evaluation
Methodology for Virtual Environments, in: Virtual Reality Section, Encyclopedia of Computer Graphics and Games, (eds. Debernardis, D, Papagiannakis, G, Thawonmas, R., Wu,
X., Lombardo, S., Joslin, Ch, Bostan, B., Nilsson, N.C., Mahmood, A.), SpringerLink, doi:
10.1007/978-3-319-08234-9 167-1).
45. Zanier, E. R., Zoerle, T., Di Lernia, D., & Riva, G. (2018). Virtual Reality for Traumatic Brain
Injury. Frontiers in Neurology, 9, 345.
46. Zhang, H., Chavarriaga, R., Khaliliardali, Z., Gheorghe, L., Iturrate, I., & d R Milln, J. (2015).
EEG-based decoding of error-related brain activity in a real-world driving task. Journal of
neural engineering, 12(6), 066028.
ARTIFICIAL INTELLIGENCE
TECHNOLOGIES AND
APPLICATIONS FOR HEALTH
AND MEDICINE
Part II
Dac-Nhuong Le et al. (eds.), Emerging Technologies for Health and Medicine, (87–284) 
© 2018 Scrivener Publishing LLC
89
Dac-Nhuong Le et al. (eds.), Emerging Technologies for Health and Medicine, (89–284) 
© 2018 Scrivener Publishing LLC
CHAPTER 7
STATE OF THE ART: ARTIFICIAL
INTELLIGENT TECHNOLOGIES FOR
MOBILE HEALTH OF STROKE
MONITORING AND REHABILITATION
ROBOTICS CONTROL
B.M. Elbagoury1, M.B.H.B. Shalhoub2, M.I. Roushdy1, Thomas Schrader3
1 Faculty of Computer and Information Sciences, Ain Shams University, Cairo, Egypt
2 Consultant of Information Technology at Ministry of Interior, Riyad, KS
3 University of Applied Sciences Brandenburg, D- 14770 Brandenburg, Germany
Emails: schrader@fh-brandenburg.de, miroushdy@hotmail.com, bassantai@yahoo.com,
shalhoub@live.com
Abstract Medical expert system development is used in the early detection of diseases
.And this project is a quantum civilized tremendous in the field of medicine being depends
very heavily at the application of technology advanced computer-based expert systems
and artificial intelligence systems and Systems retrieve data and images, as well as mobile
computing as it contributes to this service programming smart in the early detection of
stroke disease accurately and scientifically advanced from which the advancement aspect
of health of Saudi society to lift the suffering of the thousands of patients who suffer,
stroke diseases which contributes positively to the payment of health development and the
development and robot rehabilitation of all members of society until they are enjoying good
health and contribute effectively to the support and development of society in general.
The implementation of such a project would help in paying medical systems developed
Arabia to compete at the regional level and the world to keep up and keep pace with the
latest mechanisms therapy world and makes Saudi Arabia a model in the Arab region and
the Middle East . Below, we show for the most important applications outstanding which
provided by this pilot project. Building an expert system in the field of intelligent stroke
diagnosis to help doctors and patients all over the Kingdom. Expert systems in the field
of medical diagnostics remotely (Telemedicine) in order to take the doctors advice global
level Medical highly accurate. Building an intelligent system to track the status of the patient in dangerous situations by mobile telephone technology and wireless communication
systems in order to maintain the level of health in the Kingdom and also take advantage
90 Emerging Technologies for Health and Medicine
of the innovation research journal in the field Medical Informatics. Building a real-time
mobile computing for the state of emergency by using technology Medical Sensors like
EMG sensors. Develop a new innovative Rehabilitation Robotics system for PostStroke
treatment of patients.
Keywords: Mobile Health, Telemedicine Robot Rehabilitation, Case-based Reasoning
7.1 Introduction
Stroke and cardio vascular diseases have a high incidence in countries such as: Kingdom of
Saudi Arabia, Egypt and Germany, Romania, China and USA. Beside the early detection
of high-risk persons, their monitoring and the detection critical, deathtrap events, their
effective emergency management the rehabilitation process is difficult and cost intensive.
Stroke is an urgent case that may cause problems like weakness, numbness, vision problems, confusion, trouble walking or talking, dizziness and slurred speech. It may also cause
sudden death. It is a leading cause of death in the United States. For these reasons, brain
stroke is considered an emergency case as same as heart attack and needs to be treated
immediately before causing more problems.
Although stroke is a disease of the brain, it can affect the entire body. A common disability that results from stroke is complete paralysis on one side of the body, called hemiplegic. A related disability that is not as debilitating as paralysis is one-sided weakness or
hemi paresis. Many stroke patients experience pain in legs and hands. Therefore, patients’
case emergency for pre-stroke detection as well as post-stroke rehabilitation treatment is
very important for long time recovery and overall patient health management. Therefore,
in this project we three main targets, first is patient emergency and stroke early detection through mobile health technology and then second phase we aim to address patient
post-stroke rehabilitation through our new innovative design of rehabilitation robotics controller.
In first phase, we want to implement and develop a complete product through research
and development of Mobile health system, Mobile Health in remote medical systems has
opened up new opportunities in healthcare systems. Mobile Health is a steadily growing field in telemedicine and it combines recent developments in artificial intelligence and
cloud computing with telemedicine applications. For these reasons, brain stroke is considered an emergency case as same as heart attack and needs to be treated immediately before
causing more problems. In the recent research, what we witness is a high competition and
new revolution towards mobile health in general, especially in field of chronic illnesses and
emergency cases like heart attack and diabetics. However, today’s Mobile Health research
is still missing an intelligent remote diagnosis engine for patient emergency cases such as
Brain Stroke. Moreover, Remote patient monitoring and emergency cases need intelligent
algorithms to alert with better diagnostic decisions and fast response to patient care. This
research work proposes a Hybrid Intelligent remote diagnosis technique for Mobile Health
Application for Brain Stroke diagnosis.
Mobile Health in remote medical systems has opened up new opportunities in healthcare
systems. It combines recent developments in artificial intelligence and cloud computing
with telemedicine applications. This technology help patients manage their treatments
when attention from health workers is costly, unavailable, or difficult to obtain regularly.
AI Technologies for Mobile Health of Stroke Monitoring 91
In fact remote monitoring - which is seen as the technology with the highest financial
and social return on investment, given current healthcare challenges - is a focus for many
of the pilot projects.
Mobile Health for patient tracking supports the coordination and quality of care for
the benefits of rural communities including the urban poor, women, the elderly, and the
disabled. This would promote public health and prevent disease at the aggregate level.
Some stroke disorders affect the nerves (e.g. Stroke) and cause problems with thinking,
awareness, attention and lead to emotional problems. Stroke patients may have difficulty
controlling their emotions or may express inappropriate emotions. So that brain stroke is
considered an emergency case that needs to be treated immediately before causing more
problems.
In the first phase of the proposed research proposal aims to develop a new intelligent
mobile health applications based on new artificial intelligent technologies in the field of
brain stroke by proposing an intelligent mobile health application based on EMG sensor
which provides a significant source of information for identification of neuromuscular disorders.
In final (second) phase of the research, we want to develop a new innovative robotics
controller for patient’s rehabilitation. The rehabilitation points towards the intense and
repetitive movement assisted therapy that has shown significant beneficial impact on a
large segment of the patients. The availability of such training techniques, however, are
limited by:
The amount of costly therapist’s time they involve,
The ability of the therapist to provide controlled,
Quantifiable and repeatable assistance.
These limitations are quite important in Saudi Arabia. Rehabilitation robotics systems
are a very important problem, especially in the therapeutic domain of stroke patients. This
is due to:
The complexities of patients’ treatments procedures such as physiotherapy
Since Electromyography (EMG) detects muscle response during different actions, it
gives useful identification of the symptoms’ causes. Such disorders that can be identified by EMG are neuromuscular diseases, Nerve injury, and Muscle degeneration.
The dealing with Electromyography (EMG) signals provides significant source of information for identification of neuromuscular disorders.
A robot-assisted rehabilitation can provide quantifiable and repeatable assistance that
ensure consistency during the rehabilitation and
A robot-assisted rehabilitation is likely to be cost-efficient.
Rehabilitation robotics refers to the use of robotic devices (sometimes called ”rehabilitators”) that physically-interact with patients in order to assist in movement therapy.
Rehabilitation robotics is directed to improve mobility and independence in daily life of
patients. It uses specific ex-excises related to the therapeutic problem and patients practice
movements. The rehabilitation robotics controls this automatically. The pattern of movements follows a theoretical concept developed and disseminated by respected authorities.
However, now the proof of evidence for each concept is missing. Especially, no validated
92 Emerging Technologies for Health and Medicine
data to compare different therapeutic strategies are missed. The health economical demand
is to demonstrate the effectiveness of robotics and rehabilitative procedures [1].
Two important issues that the current robot-assisted rehabilitation systems do not address: they are limited by their inability to simultaneously assist both arm and hand movements (signal evaluation and robot steering is quite complicated using signals from arm,
hand or body (head, neck, shoulder). Current robot-assisted rehabilitation systems can
comprehensively alter with limits the task parameters based on patient’s feedback to impart effective therapy during the execution of the task in an automated manner.
Moreover, the third important problem of current robot-assisted rehabilitation systems
is intelligent robot control. Behavior control for an autonomous robot is a very complex
problem, especially in the rehabilitation and medical domains. This is due to the dynamics
of patients muscle movements and real-time EMG patient signal feedback.
7.2 Research Chapter Objectives
Stroke is an urgent case that may cause problems like weakness, numbness, vision problems, confusion, trouble walking or talking, dizziness and slurred speech. It is a leading
cause of death in the United States. For these reasons, brain stroke is considered an emergency case as same as heart attack and needs to be treated immediately before causing
more problems.
The main objective of the proposed research is to propose a Hybrid Intelligent remote
diagnosis Technique for Mobile Health Application for Brain Stroke diagnosis. Another
objective is monitoring human health conditions based on emerging wireless mobile technologies with wireless body sensor.
The research work focuses also on delivering better healthcare to patients, especially in
the case of home-based care of chronic illnesses.
On the other hand, our designed prototype investigates the implementation of the neural
network on mobile devices and tests different models for better accuracy of diagnosis and
patient emergency.
Integration of mobile technology and sensor in development of home alert system (mhealth
system) will greatly improve the lives of elderly by giving them safety and security and
preventing minor incidents from becoming life-threatening events.
7.3 Literature Review
7.3.1 Pervasive Computing and Mobile Health Technologies
Health monitoring is considered one of the main application areas for Pervasive computing. Mobile Health is the integration of mobile computing and health monitoring. It is
the application of mobile computing technologies for improving communication among
patients, physicians, and other health care workers [1]. Mobile Health applications are
receiving increased attention largely due to the global penetration of mobile technologies.
It is estimated that over 85% of the world’s population is now covered by a commercial
wireless signal, with over 5 billion mobile phone subscriptions [2].
Joseph John Oresko [3], proposed a real-time, accurate, context aware ST segment monitoring algorithm, based on PCA and a SVM classifier and applied on smartphones, for the
detection of ST elevation heart attacks. Feature extraction consists of heartbeat detection,
AI Technologies for Mobile Health of Stroke Monitoring 93
segmentation, down sampling, and PCA. The SVM then classifies the beat as normal or
ST elevated in real-time.
Qiang Fang [4], proposed an electrocardiogram signal monitoring and analysis system
utilizing the computation power of mobile devices. In order to ensure the data interoperability and support further data mining and data semantics, a new XML schema is designed
specifically for ECG data exchange and storage on mobile devices. Madhavi Pradhan [5],
proposed a model for detection of diabetes. Their proposed method uses a neural network
implementation of the fuzzy k-nearest neighbor algorithm for designing of classifier. The
system is to be run on smartphone to facilitate mobility to the user while the processing is
to be done on a server machine.
Oguz Karan [6], presented an ANN model applied on Smartphone to diagnose diabetes. In this study, three-layered Multilayer Perceptron (MLP) feedforward neural network architecture was used and trained with the error back propagation algorithm. The
back propagation training with generalized delta learning rule is an iterative gradient algorithm designed to minimize the root mean square error between the actual output of a
multilayered feed-forward neural network and a desired output.
Peter Pes [7], developped a Smartphone based decision support system (DSS) for the
management of type 1 diabetes in order to improve quality of life of subjects and reduce the
aforementioned secondary complications. The Smartphone platform implements a casebased reasoning DSS, which is an artificial intelligence technique to suggest an optimal
insulin dosage in a similar fashion as a human being would.
Jieun Kim [8], proposed a Case-Based Reasoning approach to match the user needs and
existing services, identify unmet opportunistic user needs, and retrieve similar services
with opportunity based on Apple Smartphone.
M.I. Ibrahimy [9], applied feed-forward ANN with back-propagation learning algorithm
for the classification of single channel EMG signal in the context of hand motion detection.
7.3.2 Rehabilitation Robotics for Stroke Patients
The use of robots for facilitating the motion in rehabilitation therapy to stroke patients
has been one of the fastest growing areas of research in recent years. The reason for this
growth is the potential to provide effective therapy at a low, acceptable cost. It is known
that by exercising the affected part, it could recover some degree of functionality [24, 29].
A Robot could be used for replicating the exercises provided by the therapist, but it also has
the potential to reproduce other regimes that would not be easily carried out by a human
being. Some of the robots with these abilities are the MIME System from VA Palo Alto
that allows the movement of the affected and the unaffected limbs [31], and the the ARM
and GENTLES [2] projects. On the other hand, a rehabilitation robotic system driven by
pneumatic swivel modules was presented in [26, 27]. This robot is intended to assist in
the treatment of stroke patients by applying the proprioceptive neuromuscular facilitation
method. Other examples of commercial robots for therapy are the InMotion Arm Robot,
based on the pioneering MIT-Manus [23], and the ARMEO [15] series system. Recently,
some works have been focused on gait and balance rehabilitation for stroke patients. They
are able to support patient’s body, while he or she maintain a nearly natural walk and can
concentrate on other activities. Within the group of gait rehabilitation, the walkaround
system helps to walk to people who have suffered from hemiplegia or other diseases that
require assistance in posture [34]. Other highly developed devices for rehabilitation and
gait balance are WHERE I and WHERE II. WHERE I is a mobile robot that assists with
gait, it contains one rotational degree of freedom arm manipulator that adjusts to different
94 Emerging Technologies for Health and Medicine
heights and sizes and supports the body. WHERE II is a mobile vehicle that consists of
four pneumatic bars that are adjusted to each side of the body [21]. There are commercial
robots for children called SAM and SAM-Y that help in gait rehabilitation.
7.4 Description of the Research Telemedicine Platform
The target of this project is the development of an intelligent hybrid rehabilitation robot
controller based on a Telemedical platform for a portable rehabilitation robot monitor system. The Telemedical platform allows to manage the monitoring of high-risk patients of
cardio-vascular diseases, detect critical events and control the rehabilitation process using
wireless sensors and robots. The proposed system consists of:
1. Various wireless sensors, used in an adaptable, scenario based setting.
2. A mobile processing unit for signal processing and feature extraction.
3. A mobile device as data transmitter controller.
4. A Robot controller unit for intelligent behavior control of the robot.
5. A robotic arm unit for interaction with the patient.
7.4.1 A State of the Art Telemedicine Robot Rehabilitation System
Stroke is a leading cause of disability in the world, and yet Robot-assisted and telemedicine
technology is currently available for individuals with stroke to practice and monitor rehabilitation therapy on their own. Telemedicine uses common technologies that provide
conduit for tele-consultation exchange between physicians, nurses and patients. The third
phase of our proposed product is to develop a hybrid rehabilitation robot controller and
a telemedicine in a portable rehabilitation robot monitor system with 3-D Exercise Machine for Upper Limb, coordination, range of motion and other relevant perceptual motor
activities. The aim of this study is to evaluate a device for robotic assisted upper extremity repetitive therapy; the robot will have four degrees of freedom at shoulder, elbow
and wrist; the robot EEG and EMG sensors feedback position and force information for
quantitative evaluation of task performance. It has the potential of providing a repetitive
automatic of supplementing therapy. The telemedicine system will consist of a Web-based
library of status tests and Single Board computer Monitor, and can be used with a variety
of input devices, including a feedback joystick, infrared emitter sensor Bar to integrated
therapy games Stepmania and Wii, assisting or resisting in movement. The system will
enable real-time, interactive integration of medical data, voice and video transmission in
the wireless Telemedicine environment.
Robot-assisted therapy refers to the use of robotic devices (sometimes called rehabilitators) that physically-interact with patients in order to assist in movement therapy [6, 7].
Virtual reality (VR) is an emerging and promising approach for task-oriented biofeedback
therapy [8, 9] Embedded telerehabilitation system used virtual reality and a pair of wireless
networked PCs. It is intended for rehabilitation of patients with hand, elbow, and shoulder
Figure 7.1. Shows the full system units, wireless telemedicine unit, signal processing
and feature extraction units, robot controller unit system, along with wireless sensors that
consist of EEG, ECG and EMG sensors along with telemedicine server. Mobile device and
robotic arm.
AI Technologies for Mobile Health of Stroke Monitoring 95
Figure 7.1 Intelligent Telemedicine Rehabilitation Robotic Architecture
This model reflects not only the intelligent robotic control as only one aspect of the
problem but also the monitoring of high-risk patients and covers the whole process of patients with cardio-vascular diseases and stroke. It also reflects the mobile signal processing
and feature extraction unit along with the Intelligent Behavior Controller of the Robotics
unit to alter real-time patients’ feedback to impart effective therapy during the execution
of the task in an automated manner.
Figure 7.2 Hierarchical Intelligent Behavior Control for Robot
Figure 7.2 shows the details description of the Intelligent Behavior controller of robotic
unit using case-based reasoning (CBR) and neural networks, which are recent and important Artificial Intelligence technologies. Also, due to the integration of mobile devices
96 Emerging Technologies for Health and Medicine
such as cell phones and tablet pc mobile network operators can offer an additional service
of monitoring and rehabilitation management. First consultations with Egyptian providers
showed their deep interests for such a telemedical management system including additional
values such as satisfaction of secure life data management, crisis intervention and rehabilitation improvement by individualization of the therapeutic interaction and intervention.
7.4.2 Wireless telemedicine module with robot
The increased availability, miniaturization, performance and enhanced data rates of future
mobile communication systems will have an impact and accelerate the deployment of mobile telemedicine system and services within the next decade. The expected convergence
of future wireless communication, wireless sensor networks and ubiquitous computing
technologies will enable the proliferation of such technologies around tel-rehabilitation
services with cost-effective, flexible and efficient ways. Wireless LAN (WLAN) is implemented as an extension to or as an alternative for wired LAN to make the communication
more flexible and powerful. We integrated wireless LAN interface between sensor network
and robot monitor.
7.4.3 Wireless intelligence sensor network extract user’s biofeedback signal
Many physiological processes can be monitored for biofeedback applications, and these
processes are very useful for rehabilitation services. Biofeedback is a means for gaining
control of our body processes to increase relaxation, relieve pain, and develop healthier,
more comfortable life patterns. Biofeedback is a broader category of methods. These methods use feedback of various physiological signals, such as EEG electroencephalographic
or brainwave, electrical activity of muscles (EMG), bladder tension, electrical activity of
the skin (EDA/GSR), or body temperature. These methods are applied to treatment or improvement of organism functions as reflected by these signals which can be detected by the
wearable health-monitoring device.
A wearable health-monitoring device using Body Area Network (BAN) usually requires
multiple wires connecting sensors with the processing unit, which can be integrated into
user’s clothes [10, 11]. This system organization is unsuitable for longer and continuous
monitoring, we integrated intelligent sensor into wireless body area network as a part of
telemetrically monitoring system. Intelligent wireless sensors perform data acquisition and
processing. Individual sensors monitor specific physiological signals (such as EEG, ECG,
EMG, and Galvanic Skin Response (GSR)) and communicate with transmitter microcontroller and wireless gateway. Wireless gateway can integrate the monitor into telemedical
system via a wireless network. Three channels of ECG, four channels of EMG, two GSR
and up to 16 channels of EEG monitoring create a bulk of wieldy wireless channel that can
significantly normal activity and expose user’s medical condition to assist rehabilitation.
7.5 A proposed intelligent adaptive behavior control to rehabilitation robotics
Behavior-based control [1] has become one of the most popular approaches to intelligent robotics control. The robot’s actions are determined by a set of reactive behaviors,
which map sensory input and state to actions. Despite of the behavior-control part, most of
robotics systems use classical behavior-control architectures. These classical architectures
AI Technologies for Mobile Health of Stroke Monitoring 97
can cover all sensory input states of complex environments and thus limits the robot ability
to adapt its behaviors in unknown situations. Recently, some AI techniques such as neural
networks, neural networks have been applied successfully to behavior-control of mobile
robots [4]. However, research on control of rehabilitation robots using AI is still in initial
stage [10].
Figure 7.2 An Intelligent Behavior Controller Software Architecture to Rehabilitation
Robotics. As shown This architecture presents an intelligent behavioral control model that
depends on case-based reasoning. It consists of a hierarchy of four levels, the first level
is to decide robot role. The second level is to decide which skill to execute. The third
level is to determine the behaviors of each skill and the fourth level is to adapt lower-level
behaviors as distance and angels of motions. We have designed this architecture before for
German team robot, humanoid soccer [1] and we want to apply it as the main intelligent
controller of rehabilitation robot because it shows successful results [2].
As shown, each level applies CBR cycle to control and adapt its behaviors.
The first two phases apply adaptation rules to adapt behaviors.
The last two phases apply the learning capabilities of NN to learn adaptation rules for
performing the main adaptation task.
Case-Based Reasoning (CBR) suggests a model of reasoning that depends on experiences and learning. CBR solves new cases by adapting solutions of retrieved cases. Recently, CBR is considered as one of the most important Artificial Intelligent (AI) techniques
used in many medical diagnostics tasks and robotics control.
Figure 7.3 Intelligent Behavior Control Algorithm
98 Emerging Technologies for Health and Medicine
Adaptation in CBR is a very difficult knowledge-intensive task, especially for Robot
control. This is due to the complexities of the robot kinematics, which may lead to uncertain control decisions. In this work, we will propose a new hybrid adaptation model
for behavior control of Rehabilitation Robot. It combines case-based reasoning and neural
networks (NN’s). The model consists of a hierarchy of four levels that simulates the behavior control model of a patient’s motions robot. Each level applies CBR cycle to control and
adapt its behaviors. The first two phases will apply adaptation rules to adapt behaviors,
while the last two phases will apply the learning capabilities of NN to learn adaptation
rules for performing the main adaptation task. The detailed Software Algorithm of the
Intelligent Behavior control is shown in Figure 7.3.
7.6 Materials and Methods
The telemedical platform covers the process of monitoring, signal processing, and management of telemedical care. The following Figure 7.4 shows the general process of signal
processing and feature extraction and interaction with the patient.
Figure 7.4 General process model for Telemedicine sensor data management
The clue is the distributed, level based sensor data evaluation process: the first level
includes the sensor nodes themselves with a basic but very fast signal processing. Aggregated data will be sent to the mobile unit/device as second level, this will take real-time
(EMG) data read through the mobile device which sends urgent event to the hospital server
as shown in Figure 7.5. The system can also respond by immediate recommendation and
sends patient data to responsible doctor or nurse. Moreover, the next processing step can be
done. The second and third level (server/cloud based signal processing) covers intelligent
data processing and decision support for interaction and robot control.
7.7 Conclusion Summary: Artificial Intelligence Technologies
First step in our system is Signal Acquisition phase. EMG wireless sensors include high
performance analog filters for signal acquisition, anti-aliasing and instrumentation noise
AI Technologies for Mobile Health of Stroke Monitoring 99
Figure 7.5 Mobile Patient Emergency for Stroke Patients to Nearest Hospital
management. Second step is Signal Pre-processing which means noise removal depending
on noise type by applying some typical filtering techniques like band-pass filter, band-stop
filter and then applying wavelet transform method. Third step is features extraction. This
step is divided into two phases. First of them is analyzing data of Brain Stroke based on
EMG sensors of muscles readings to enable extracting best features. Second phase is to
select significant features for efficient classification since it determines the success of the
pattern classification system. However, it is quite problematic to extract the best feature
parameters from the EMG signals that can reflect the unique feature of the signal to the motion command perfectly. Hence, multiple feature sets are used as input to the EMG signal
classification process. Some of the features are classified as time domain, frequency domain, time-frequency domain, and time-scale domain; these feature types are successfully
employed for EMG signal classification. The next step is signal classification phase. Artificial Intelligence techniques mainly based on machine learning have been proposed for
EMG signal classification. This technique is very useful for real-time application based on
EMG signal. Classification step in our system is divided into four phases. First of them
is to study and analyzing Neural Networks (NN) algorithms for EMG Data. Support Vector Machine (SVM) is a powerful learning method used in binary classification. The next
phase is to analyze Case-Based Reasoning Retrieval Algorithms in Medicine. Case-Based
Reasoning (CBR) suggests a model of reasoning that depends on experiences and learning.
CBR solves new cases by adapting solutions of retrieved cases. The four processes of CBR
Cycle [13] (Retrieve, Reuse, Revise, and Retain) describe the general tasks in a casebased
reasoner. They provide a global external view to what is happening in the system.
The proposed research system aims to study and apply Artificial Intelligence technologies, mobile devices, and cutting edge technologies of Cloud-Computing and take advantage of research achievements in image processing and information communication technologies. The project will create adaptive, collaborative, and innovative cloud computing
100 Emerging Technologies for Health and Medicine
and mobile application system in Health-Care and environments for Intelligent Information System in Health-Care. To successfully achieve the research program goals, a research
framework has been developed that consists of Six layers shown in the following figure.
Various research issues and application systems are proposed to be studied and be developed. This is shown in Figure 7.6.
Figure 7.6 Artificial Intelligence Technologies Components
The technology foundation of the research framework will consist of studying mobile
computing for stroke emergency diagnosis, intelligent case-based reasoning engine, cloud
computing hospital management engine, medical sensor processing for stroke diseases,
cloud computing artificial intelligence engine and cloud computing patient database engine.
REFERENCES
1. Shahriyar, R., Bari, M. F., Kundu, G., Ahamed, S. I., & Akbar, M. M. (2009, September). Intelligent mobile health monitoring system (IMHMS). In International Conference on Electronic
Healthcare (pp. 5-12). Springer, Berlin, Heidelberg.
2. Royal Tropical Institute: What is mHealth? [http://www.mhealthinfo.org/what-mhealth]
3. Oresko, J. J. (2010). Portable heart attack warning system by monitoring the ST segment via
smartphone electrocardiogram processing (Doctoral dissertation, University of Pittsburgh).
4. Webots robot simulator. http://www.cyberbotics.com/
5. Arduino 6 DOF Programmable Clamp Robot Arm Kit http://www.bizoner.com/arduino-6-dofprogrammable-clamp-robot-arm-kit-ready-to-use-p-238.html
6. Fang, Q., Sufi, F., & Cosic, I. (2008). A mobile device based ECG analysis system. In Data
Mining in Medical and Biological Research. InTech.
7. Pradhan, M., Kohale, K., Naikade, P., Pachore, A., & Palwe, E. (2012). Design of classifier
for detection of diabetes using neural network and fuzzy k-nearest neighbor algorithm. International Journal of Computational Engineering Research, 2(5), 1384-1387.
8. Karan, O., Bayraktar, C., Gumuskaya, H., & Karlik, B. (2012). Diagnosing diabetes using
neural networks on small mobile devices. Expert Systems with Applications, 39(1), 54-60.
AI Technologies for Mobile Health of Stroke Monitoring 101
9. Peter Pesl, Pau Herrero, Mobile-Based Architecture of a Decision Support System for Optimal
Insulin Dosing, Imperial Comprehensive Biomedical Research Centre, 2010.
10. Kim, J., Park, Y., & Lee, H. (2012, December). Using case-based reasoning to new service
development from user innovation community in mobile application services. In International
Conference on Innovation, Management and Technology (ICIMT 2012), Phuket, Thailand.
11. Qiang, C. Z., Yamamichi, M., Hausman, V., Altman, D., & Unit, I. S. (2011). Mobile applications for the health sector. Washington: World Bank, 2.
12. Kaur, G., Arora, A. S., & Jain, V. K. (2009). Multi-class support vector machine classifier in
EMG diagnosis. WSEAS Transactions on Signal Processing, 5(12), 379-389.
13. Farid, N., Elbagoury, B., Roushdy, M. O. H. A. M. E. D., & Salem, A. B. (2013). A Comparative Analysis for Support Vector Machines For Stroke Patients. Rec Adv Inf Sci, 71-76.
14. http://archive.ics.uci.edu/ml/datasets/
15. Roth-Berghofer, T., & Iglezakis, I. (2001). Six Steps in Case-Based Reasoning: Towards a
maintenance methodology for case-based reasoning systems. In In: Professionelles Wissensmanagement: Erfahrungen und Visionen includes the Proceedings of the 9th German Workshop on Case-Based Reasoning (GWCBR).
16. Kahn, J. G., Yang, J. S., & Kahn, J. S. (2010). Mobile health needs and opportunities in
developing countries. Health Affairs, 29(2), 252-258.
17. Kulek, J., Huptych, M., Chudek, V., Spilka, J., & Lhotsk, L. (2011, September). Data driven
approach to ECG signal quality assessment using multistep SVM classification. In Computing
in Cardiology, 2011 (pp. 453-455). IEEE.
18. Hu, S., Wei, H., Chen, Y., & Tan, J. (2012). A real-time cardiac arrhythmia classification
system with wearable sensor networks. Sensors, 12(9), 12844-12869.
19. Dragoni, M., Azzini, A., & Tettamanzi, A. G. B. (2012). A neuro-evolutionary approach to
electrocardiographic signal classification. In Italian Workshop on Artificial Life and Evolutionary Computation (WIVACE) (pp. 1-11). Universit degli Studi di Parma, Dipartimento di
Scienze Sociali.
20. Curran, K., Nichols, E., Xie, E., & Harper, R. (2010). An intensive insulinotherapy mobile
phone application built on artificial intelligence techniques. Journal of diabetes science and
technology, 4(1), 209-220.
21. http://crsouza.blogspot.com/2010/03/kernel-functions-for-machine-learning.html
22. Rekhi, N. S., Arora, A. S., Singh, S., & Singh, D. (2009, June). Multi-class SVM classification
of surface EMG signal for upper limb function. In Bioinformatics and Biomedical Engineering,
2009. ICBBE 2009. 3rd International Conference on (pp. 1-4). IEEE.
23. Khokhar, Z. O., Xiao, Z. G., & Menon, C. (2010). Surface EMG pattern recognition for realtime control of a wrist exoskeleton. Biomedical engineering online, 9(1), 41.
103
Dac-Nhuong Le et al. (eds.), Emerging Technologies for Health and Medicine, (103–284) 
© 2018 Scrivener Publishing LLC
CHAPTER 8
ARTIFICIAL INTELLIGENCE FOR SMART
CANCER DIAGNOSIS: A NEW
TELEMEDICINE SYSTEM BASED ON
FUZZY IMAGE SEGMENTATION
M.H.B. Shalhoub1, Naif M. Hassan Bin Shalhoub2, Bassant M. Elbagoury3,
Abdel-Badeeh M. Salem3
1 Consultant of Information Technology at Ministry of Interior, Riyad, KSA
2 Faculty of Medicine, King Abdel-Aziz University2, Jeddah, KSA
3 Faculty of Computer and Information Sciences, Ain Shams University, Cairo, Egypt
Emails: shalhoub@live.com, bassantai@yahoo.com, abmsalem@yahoo.com
Abstract Micro-calcifications appearing as collection of white spots on mammograms
show an early warning of breast cancer. Early detection is the key to improve breast cancer
prognosis. This paper presents a new intelligent telemedicine framework that has been developed to improve the detection of primary masses and micro calcifications of the disease.
Our main motivation is to provide remote services to radiologists and cancer patients. The
proposed telemedicine framework is based on Service-oriented technology, where it uses
in its application layer image compression by wavelet technique. Then image enhancement
is applied on the image to prepare it for the segmentation. Finally, image segmentation is
applied for the detection of the calcifications in the breast using Fuzzy C-Mean. The implementation of the system has shown a very good prototype result with integrated intelligent
techniques and it has been tested with 326 mammogram breast cancer images with overall
good results and this can serve as a real telemedicine platform for the cloud computing
industry in the future
Keywords: Telemedicine, Service-Oriented Architecture, Mammograms, Image Compression, Image Segmentation.
104 Emerging Technologies for Health and Medicine
8.1 Introduction
Telemedicine is a rapidly developing technology of clinical medicine, where medical information is transferred through interactive audiovisual media for the purpose of consulting
[1]. Telemedicine can also be used to conduct examinations and remote medical procedures. Opportunities and developments of telemedicine discussed in states [2]. Also, Anna
E.S. Klughammer [3] introduce improving breast cancer and cervical cancer screening in
developing countries using telemedicine. Elvira M. Zilliacus et al. [4] introduce telegenetics of Tele-health cancer.
The term telemedicine encompasses an array of services:
Specialist and primary care consultations may involve a patient seeing a health professional over a live
Video connection or it may use diagnostic images and/or video along with patient data
to a specialist for viewing later. This may be used for primary care or for specialist
referrals. Recent surveys have shown a rapid increase in the number of specialty and
subspecialty areas that have successfully used telemedicine. Major specialty areas
actively using telemedicine include: dermatology, ophthalmology, mental health, cardiology and pathology. According to reports and studies, almost 60 different medical
subspecialties have successfully used telemedicine.
Imaging services such as radiology (Teleradiology) continues to make the greatest
use of telemedicine with thousands of images read by remote providers. Digital images, sent to the specialist over broadband networks, are diagnosed with a report sent
back. Radiology, pathology and cardiology are all using telemedicine to provide such
services. It is estimated that over 400 hospitals in the United States alone outsource
some of their medical imaging services . Radiological images include (X-rays, CT,
MR, PET/CT, SPECT/CT, MG.. etc).
Remote patient monitoring uses devices to remotely collect and send data to a monitoring station for interpretation. Such home telehealth applications might include
using telemetry devices to capture a specific vital sign, such as blood glucose or heart
ECG or a more sophisticated device to capture a variety of indicators for homebound
patients. Such services can be used to supplement the use of visiting nurses.
Remote medical education and consumer information can include a number of activities including: continuing medical education credits for health professionals and
special medical education seminars for targeted groups in remote locations; the use of
call centers and Internet Web sites for consumers to obtain specialized health information and on-line discussion groups to provide peer-to-peer support.
In this chapter, we focus on the third type of Telemedicine, which is Teleradiology. We
propose an integrated teleradiology system, which is applied for breast cancer mammography images. This is because Breast cancer recent statistics shows that it is one of the
major causes of death among women. Moreover, Mammography is the main test used for
screening and early diagnosis, where the micro-calcifications appear in small clusters of
few pixels with relatively high intensity compared with their neighboring pixels [5]. Also,
S.Shaheb et el. [6] uses fuzzy logic for mammogram image segmentation.
In order to increase physicians’ diagnostic performance, many researchers and companies introduce Service-Oriented Architecture (SOA) [7], which is a flexible paradigm for
telemedicine phases development and computing.
Artificial Intelligence for Smart Cancer Diagnosis 105
In this paper, we introduce an integrated teleradiology system for breast cancer diagnosis. It is based on SOA and mammogram images compression, enhancement and fuzzy
C-mean mammogram segmentation. The coming sections explain each module in details.
The rest of this chapter is organized as follows, In section 2, we present a brief background and related work. We propose our system architecture in section 3 and in section 4
the telemedicine system modules are presented. Results and discussion are given in section
5. Finally, conclusion and future work are given in section 6.
8.2 Background and Related work
Breast cancer image segmentation is the process of partitions an image to several small
segments the main difficulties in image segmentation are, noise, bias field, partial volume
effect (a voxel contributes in multiple tissue types). R.Ramani et al. [13] presents A Survey
Of Current Image Segmentation Techniques For Detection Of Breast Cancer, where they
discuss:
8.2.1 De-noising methods
Decreases the noise: In image pre-processing techniques ar necessary in order to find the
orientation of the mammogram to remove the noise and to enhance the quality of the image[8].the pre-processing steps are very important in order to limit the search for abnormalities without undue influence from back ground of the mammograms. The main objective
of this process is to improve the quality of the image to make it ready to further processing
by removing the unrelated and surplus parts in the back ground of the mammograms [14].
1. Adaptive median filter: Adaptive median filter works on a rectangular region Pxy,
it changes the size of Pxy during the filtering operation depending on certain conditions
such as
Zmin = minimum pixel value in Pxy.
Zmax = maximum pixel value in Pxy.
Zmed = median pixel value in Pxy.
Pmax = maximum allowed size of Pxy.
Each output contains the median value in 3 by 3 neighborhoods around the corresponding pixel in the input images. The edges of the image however are replaced by zeros [15].
Adaptive median filter has been found to smooth the non repulsive noise from 2D signals without blurring edges and preserve image details. This is particularly suitable for
enhancing mammograms images.
2. Mean filter: The mean filter replaces each pixel by the average value of the intensities in its neighborhood.It can locally reduce the variance and is easy to implement
[16].
3. A Markov random field method: In this method spatial correlation information is
used to preserve fine details.in this method regularization of the noise estimation is performed. The updating of pixel value is done by iterated conditioned modes.
4. Wavelet methods: In frequency domain these method is used for de-noising and
preserving the signal application of wavelet based methods on mammography 4. Wavelet
methods In frequency domain these method is used for de-noising and preserving the signal
106 Emerging Technologies for Health and Medicine
application of wavelet based methods on mammography image makes the wavelet and
scaling coefficient biased.This problem can be solved by squaring mammograms images
by non central chi-square distribution method.
5. Median filtering: A median filter is a non linear filter is efficient in removing salt and
pepper noise median tends to preserve the sharpness of image edges while removing noise.
The various of median filter are i) centre-weighted median filter ii) weighted median filter
iii) max-median filter, the effect of increasing the size of the window in median filtering
noise is removed effectively.
6. Max-Min filter: Maximum and minimum filter attribute to each pixel in an image a
new value equal to the maximum or minimum value in a neighborhood around that pixel.
The neighborhood stands for the shape of the filter, maximum and minimum filters have
been used in contrast enhancement.
8.2.2 Image Segmentation Overview
The main objective of image segmentation is to extract various features of the images
which can be merged or split in order to build objects of interest on which analysis and
interpretation can be performed. Image segmentation refers to the process of partitioning
an image into groups of pixels which are homogeneous with respect to some criterion. The
result of segmentation is the splitting up of the image into connected areas. Thus segment
is concerned with dividing an image into meaningful regions. The image segmentation
techniques such as thresholding, region growing, statistics models, active control modes
and clustering have been used for image segmentation because of the complex intensity
distribution in medical images, thresholding becomes a difficult task and often fails [17].
1. Region growing segmentation: Region growing is an approach to image segmentation in which neighboring pixels are examined and added to a region class if no edges
are detected. This process is iterated for each boundary pixel in the region. If adjacent
regions are found, a region merging algorithms is used in which weak edges are dissolved
and strong edges are left intact. The region growing starts with a seed which is selected
in the centre of the tumor region. During the region growing phase, pixels in the neighbor
of seed are added to region based on homogeneity criteria thereby resulting in a connected
region.
2. K-Means clustering method: The k-means algorithms are an iterative technique
that is used to partition an image into kcluster. In statistics and machine learning, k-means
clustering is a method of cluster analysis which can to portions n observation into k cluster
in which each observation belongs to the cluster with the nearest mean [20-21]. The basic
algorithms is given below
1. Pick k cluster center’s either randomly or based on some heuristic.
2. Assign each pixel in the image to the cluster that minimum the distance between the
pixels cluster centre.
3. Re-compute the cluster center’s by averaging all of the pixels in the cluster. Repeat
last two steps until convergences are attained. The most common algorithm uses an
iterative refinement technique; due to this ambiguity it is often called the k-means
algorithms.
Artificial Intelligence for Smart Cancer Diagnosis 107
8.3 Proposed System Architecture
The proposed system is based on the basic SOA [7], which is shown in Figure 8.1 and it
can be divided into three levels:
Front-End layer: Which includes the Client interface and the network connection.
Application Layer: That includes the images processing which consists of image compression, image decompression and image segmentation. Also that layer contains the
feature extraction part as well as the CBR (Case-Based Reasoning) module.
Finally the Back-end layer: That contains the image database and the patients’ database.
The proposed system database consists of 326 mammogram images of different cases
of breast cancer with different diagnosis. They were obtained from the MIAS (Mammographic Image Analysis Society) [8] database is used because it has complete information about abnormalities of each mammographic image like class of lesion, location, size. We have selected those images which included micro-calcifications.
Figure 8.1 Basic Service-Oriented Architecture
SOA (Server Client Network) the move to service-oriented communication has changed
software development. Whether done with SOAP (Simple Object Access Protocol) or in
some other way, applications that interact through services have become the norm. For
Windows developers, this change was made possible by Windows Communication Foundation (WCF). In our proposed work, WCF is implemented primarily as a set of classes on
top of the. NET Framework’s Common Language Runtime (CLR). This lets .Net developers build service-oriented applications in a familiar way. As shown in Figure 8.2. We use
WCF service and configured it programmatically.
Figure 8.2 SOA Service Communication using WCF
108 Emerging Technologies for Health and Medicine
SOA implementation based on WCF. In order to develop the telemedicine SOA framework, a server-client connection is established using WCF because it supports serviceoriented cloud-computing development and also due to its inter-polarity with applications
that supports other technologies. Their main process connection is shown in Figure 8.3.
Figure 8.3 SOA implemented as WCF process and services
As shown, WCF allows creating clients that access services. Both the client and the service can run in a pretty much any Windows process- WCF doesn’t define a required host.
Wherever they run, client and services can interact via SOAP. The whole system is implemented by WCF console connection, Microsoft ASP.net interface and Matlab connection
for coding. Creating a WCF service. Every WCF service has three primary components:
A service class, implemented in C# as a CLR based language that implements one or more
methods. A host process in which the service runs and one or more endpoints that allow
clients to access the service. All communication with a WCF service happens via the service’s endpoints. An endpoint includes an address (URLs) that identify a machine and a
particular endpoint on that machine. It also includes a binding determining how this endpoint can be accessed. The binding determines what protocol combination can be used to
access this endpoint along with other things, such as whether the communication is reliable
and what security mechanisms can be used. Also, a contract name indicating which service
contract this WCF service class exposes via this endpoint.
Creating a WCF client is even more straightforward. In the simplest approach, all
that’s required is to create a local stand-in for the service, called a proxy, that’s connected
to a particular endpoint on the target service, and then invoke the service’s operations via
the proxy.
Security aspects of WCF exposing services on a network, even an internal network,
usually requires some kind of security. How can the service be certain of its client’s identity? WCF provide the core security functions of authentication, message integrity, message confidentiality and authorization. All of these depend fundamentally in the notion
of identity: who is this user ?. This can be done by directly invoking a WCF function.
Therefore, establishing an identity is an essential part of using WCF security.
Artificial Intelligence for Smart Cancer Diagnosis 109
8.4 Telemedicine System Modules
In this section, we are going to describe the telemedical system components and mammogram algorithms in details, as shown in Figure 8.4. It consists of three main modules,
Image compression, Image enhancement and Image Segmentation.
Figure 8.4 The Proposed Telemedicine System Modules
8.4.1 Image Compression
Mammogram images carry a lot of small features and details that are very important, they
are also inherently voluminous so an efficient data compression techniques are essential
for their archival and transmission. Image compression [9] is minimizing the size in bytes
of a graphics file without degrading the quality of the image. We found that a common
characteristic of most of images is that the neighboring pixels are correlated. Therefore
most important task is to find less correlated representation of image. After surveying
many algorithms for image compression, we have applied Discrete Wavelet Transform
technique [9]. Figure 8.5 shows the block diagram of image compression sub-modules.
Figure 8.5 Block Diagram of Image Compression Using Wavelet Technique
Image compression using Discrete Wavelet Transform (DWT) has emerged as a popular
technique for image coding applications; DWT has high decorrelation and energy compaction efficiency. One of the most important characteristics of DWT is multi-resolution
110 Emerging Technologies for Health and Medicine
decomposition. An image decomposed by wavelet transform can be reconstructed with desired resolution. When first level 2D DWT is applied to an image, it forms four transform
coefficients. The first letter corresponds to applying either low pass or high pas filter to
rows and the second letter refers to filter applied to columns, as shown in Figure 8.6.
Figure 8.6 Two level wavelet decomposition
A quantizes simply reduces the number of bits needed to store the transformed coefficients by reducing the precision of those values. Since this is a many to one mapping, it is
a lossy process and is the main source of compression in an encoder. In uniform quantization, quantization is performed on each individual coefficient. Among the various coding
algorithms, the Embedded Zero Tree Wavelet (EZW) coding and its improved version the
SPIHT has been very successful [10]. EZW is a progressive image compression algorithm,
i.e. at any moment, the quality if the displayed image is the best available for the number
of bits received up to that moment. Compared with JPEC the current standard for still
image compression, the EZW and the SPIHT are more efficient and reduce the blocking
artefact.
8.4.2 Image Enhancement and Region of Interest Segmentation
This section discusses image enhancement and Region of Interest (ROI) pre-processing
segmentation algorithm [11] techniques. Figure 8.7 shows the main flowchart modules.
Figure 8.7 Image Enhancement and ROI segmentation flowchart
Image enhancement techniques [12] are used to emphasize and sharpen image features for display and analysis. General methods of mammographic image enhancement
can be grouped into three classes: noise reduction, background removal, and contrast enhancement. Preprocessing steps include: a) noise removal, b) artifact suppression and
background separation (Thresholding and Contrast Enhancement), c) pectoral muscle segmentation (Seeded Region Growing).
Artificial Intelligence for Smart Cancer Diagnosis 111
Digitization Noise Removal. The first step we apply is Median filter for noise removal.
Digitization noises such as straight lines are filtered using a two-dimensional (2D) Median filtering approach in a 3-by-3 neighborhood connection. Each output pixel contains
the median value in the 3-by-3 neighborhood around the corresponding pixel in the input
images. The edges of the images however, are replaced by zeros (total absence or black
color). Median filtering has been found to be very powerful in removing noise and isolated
points from mammographic images without blurring edges. It is applied to remove the
high frequency components in the mammogram image. The merit of using median filter
is, it can remove the noise without disturbing the edges.
Algorithm . Median filters
BEGIN
Step1: Read the image from left to right.
Step 2: For each pixel get a 3X3 window with the pixel cantered in this window.
Step 3: Sort the values of the nine pixels that are in the window according to the gray level.
Step 4: Get the middle gray level value (median value) appearing in the sort
Step 5: Replace the pixel value with the new median value.
Step 6: Repeat the process over all pixels in image
END
Artifact Suppression and Background Separation. After applying the median filter algorithm, Radiopaque artifacts such as wedges and labels in mammograms images
are removed using thresholding and morphological operations. Figure 8.8 shows a mammogram image with a Radiopaque artifact present. Through manual inspection of the all
mammogram images acquired, a global threshold with a value T = 18 ( normalized value,
Tnorm = 0.0706) is found to be the most suitable threshold of transforming the grayscle
images into binary [0,1] format.
Figure 8.8 Results of Image Enhancement and Region of Interest Segmentation
After the grayscale mammorgram images are converted into binary, as shown in Figure
8.8. Morhological operations such as dilation, erosion, opening and closing are performed
on the binary images.
The algorithm of Suppression of Artifacts, labels and wedges:
112 Emerging Technologies for Health and Medicine
Algorithm . Suppression of Artifacts
BEGIN
Step 1. All objects present in the binary image in Figure 8.8 (threshold using, T = 18) are labelled
using the bwlabel function in MATLAB. The binary objects consist of the radiopaque artifacts and the
breast profile region as indicated in Figure 8.8.
Step 2. The ’Area’ (actual number of pixels in the region) of all objects (regions) in Figure 8 is
calculated using the regionprops function in MATLAB.
Step 3. Next, a morphological operation to reduce distoration and remove isolated pixels (ndividual
1’s surrounded by 0’s) is applied to the binary images using the bwmorph function in MATLAB with
parameter ’clean’.
Step 4. Another morphological operation is applied the binary images to smoothen visible noise
using the bwmorph function in MATLAB with the parameter ’majority’. This algorithm checks all pixels
in a binary image and sets a pixel to 1 if five or more pixels in a binary image and sets a pixel to 1 if five
or more pixels in its 3-by-3 neighbourhood are 1’s, otherwise, it sets the pixel to 0.
Step 5. The binary images are reoded using a flat, disk-shaped morphological structuring element
(STREL) using the MATLAB strel and imerode functions. The radius of the STREL object used is
R = 5.
Step 6. Next, the binary images are dilated using the same STREL object in Step6. Morphological
dilation is performed using the MATLAB imdilation function.
Step 7. The holes in the binary images are filled using the imfill function in MATLAB with the
parameter holes’. This algorithm fills all holes in the binary images, where a hole is defined as a set of
background pixels that cannot be reached by filling in the background from the edge of the image.
Step 8. The resulting binary image obtained from Step 8 is multiplied with the original mammogram image using the MATLAB immultiply function to form the final grayscale region growing (ROI)
segmented image.
END
Fuzzy C-mean Algorithm Segmentation: Traditional clustering approaches generate
partitions where each pattern belongs to one and only one cluster. The clusters in a hard
partition are disjoints.
The Fuzzy C-means algorithm, also known as fuzzy ISODATA, is one of the most
frequently used methods in pattern recognition. Fuzzy C-means (FCM) is a method of
clustering which allows one piece of data to belong to two or more clusters [13]. It is
based on the minimization of objective function to achieve a good classification. ’J’ is
a squared error clustering criterion, and solutions of minimization are least squared error
stationary point of ’J’ in equation (1).
J = 
C
j=1
n
i=1



Z(j) − Vj


 (8.1)
Where, 
Z(j) − Vj

 is the chosen distance measure between every point Z(j)
, and the
cluster, Vj . The value of this function is an indicator of the proximity of the n data points
to their cluster prototypes.
The algorithm is composed of the following steps:
Algorithm . Fuzzy C-mean Algorithm Segmentation
BEGIN
Step 1. Select K points into the space represented by objects that are being clustered. These
points represent initial group prototypes.
Step 2. Assign each object to the group that has the closet prototype.
Step 3. When all objects have been assigned, recalculate the positions of the K prototypes.
Step 4. Repeat second and third steps until the values of the prototypes no longer change. The
result is a separation of objects into groups, from which the metric to be minimized can be calculated.
END
Artificial Intelligence for Smart Cancer Diagnosis 113
Traditional clustering approaches generate partitions where each pattern belongs to one
and only one, cluster. Hence, the clusters in a hard partition are disjoints. Fuzzy clustering
extends this notion to associate each pattern to every cluster using a membership function.
Theorem FCM: if DikAi = 
Z(j) − Vj

 > 0, for every I, k,m > 1 and Z contains at
least C different patterns, then (U, V ) ∈ Mfmc × RC×N and Jfmc.
Following the previous equations of the FCM algorithm, given the data set Z, choose
the number of cluster, 1 ≤ c ≤ N, the weighting exponent m > 1, as well as the ending
tolerance δ > 0. The solution can be reached following the next steps:
BEGIN
Step 1. Provide an initial value to each prototype, Vi, i = 1, .., C. These values are generally given
in a random way.
Step 2. Calculate the distance between the pattern Zk and each prototype, Vi.
Step 3. Calculate the membership degrees of the matrix, U = [μiK], if DikAi > 0.
Step 4. Update the new values of the prototypes, Vi.
Step 5. Verify if the error is greater than δ. If this is true, go to the second step. Else, Stop.
END
8.5 Results and discussion
In this analysis, the first procedure is determining the seed regions. When dealing with
mammograms, it is known that pixels of tumor regions tend to have maximum allowable
digital value. Based on this information, morphological operators are used as Dilation
and Erosion to detect the possible clusters which contain masses. Image features are then
extracted to remove those clusters that belong to background and normal tissue as a first
cut. Features used here include cluster area and eccentricity. The Fuzzy C-means clustering
algorithm is used as a segmentation strategy to function as better classifier and aims to class
data into separate groups according to their characteristics. Figure 8.9 shows the resulted
image of clustering. As shown, after extracting the Region of Interest (ROI) and then
applying the morphological operators, the Fuzzy C-Mean algorithm cluster the image and
have successfully detected the breast cancer masses in mammograms.
Figure 8.9 Mammogram images while applying steps of Fuzzy C-Mean algorithm steps. (a)
Original image, (b) image with segmented ROI after applying the morphological operators, (c) The
resulted image after clustering
114 Emerging Technologies for Health and Medicine
8.6 Conclusion and Future Work
This paper proposed a new architecture of Telemedicine that can be introduce as an intelligent SOA for cloud-computing technology. The whole system can be divided into:
Service-oriented architecture, Server-Client network (WCF), image compression, image
enhancement and image segmentation. SOA main advantage is abstraction. Services are
autonomous, stateless and separate from the cross-cutting concerns of the implementation.
Also, we have used the Server-client network ( WCF ) which is one of the best techniques in
connecting a network since it unification of the original . NET Framework communication
technologies and explicit support for service-oriented development. Another strength of
our proposed telemedicine framework is that we apply a wavelet image compression algorithm to ease the transmission for the mammogram images through the network. Wavelet
technique is used since it is one of the most efficient algorithms in image compression
that compress the image with highest percentage of accuracy that might reach a lossless
compression. Moreover, image enhancement was used to prepare the image for segmentation through the removing of noise and unwanted objects from the image other than the
breast, also mathematical morphological operators used to clarify the details of the image
through some operations. Finally, image segmentation is used to detect microclacifications
in the breast using the Fuzzy C-Mean algorithm that depends on clustering the image for
the detection of microcalcifications which has higher dentistry that the surrounding tissue.
In our future work, we may pursue different areas of the research. The areas of research
that could be pursued include image compression, enhancement and segmentation and
mobile computing. Mobile computing technology can help the user capture the image
by the mobile camera and send it through the network to the server to be segmented and
diagnosed.
REFERENCES
1. Pruthi, S., Stange, K. J., Malagrino, G. D., Chawla, K. S., LaRusso, N. F., & Kaur, J. S. (2013,
January). Successful implementation of a telemedicine-based counseling program for high-risk
patients with breast cancer. In Mayo Clinic Proceedings (Vol. 88, No. 1, pp. 68-73). Elsevier.
2. Engan, K., Gulsrud, T. O., Fretheim, K. F., Iversen, B. F., & Eriksen, L. (2007). A Computer Aided Detection (CAD) System for Microcalcifications in Mammograms-MammoScan.
International Journal of Biological and Medical Sciences, 2(3).
3. Anna E. Schmaus-Klughammer, Improving Breast and Cervical Cancer Screening in Developing Countries Using Telemedicine, Klughammer GmbH, Ulrichsbergerstrasse 17, Deggendorf 94469, Germany
http://www.medetel.lu/download/2013/parallel sessions/abstract/day1/Improving Breast.pdf
4. Zilliacus, E. M., Meiser, B., Lobb, E. A., Kirk, J., Warwick, L., & Tucker, K. (2010). Women’s
experience of telehealth cancer genetic counseling. Journal of Genetic Counseling, 19(5), 463-
472.
5. Nagi, J., Kareem, S. A., Nagi, F., & Ahmed, S. K. (2010, November). Automated breast profile
segmentation for ROI detection using digital mammograms. In Biomedical Engineering and
Sciences (IECBES), 2010 IEEE EMBS Conference on (pp. 87-92). IEEE.
6. Basha, S. S., & Prasad, K. S. (2009). AUTOMATIC DETECTION OF BREAST CANCER
MASS IN MAMMOGRAMS USING MORPHOLOGICAL OPERATORS AND FUZZY C–
MEANS CLUSTERING. Journal of Theoretical & Applied Information Technology, 5(6).
Artificial Intelligence for Smart Cancer Diagnosis 115
7. Guilln, E., Ubaque, J., Ramirez, L., & Cardenas, Y. (2012). Telemedicine network implementation with SOA architecture: a case study. In Proceedings of the World Congress on Engineering
and Computer Science (Vol. 2012, pp. 24-27).
8. S. Bouyahia, S., Mbainaibeye, J., & Ellouze, N. (2009). Wavelet based microcalcifications
detection in digitized mammograms. ICGST-GVIP Journal, 8(5), 23-31.
9. Sethi, J., Mishra, S., Dash, P. P., Mishra, S. K., & Meher, S. (2011). Image compression using
wavelet packet tree. Saturn, 96, 96-2110.
10. Janaki, R. (2011). Still image compression by combining EZW encoding with Huffman encoder.
11. Nagi, J., Kareem, S. A., Nagi, F., & Ahmed, S. K. (2010, November). Automated breast profile
segmentation for ROI detection using digital mammograms. In Biomedical Engineering and
Sciences (IECBES), 2010 IEEE EMBS Conference on (pp. 87-92). IEEE.
12. Engan, K., Gulsrud, T. O., Fretheim, K. F., Iversen, B. F., & Eriksen, L. (2007). A Computer Aided Detection (CAD) System for Microcalcifications in Mammograms-MammoScan.
International Journal of Biological and Medical Sciences, 2(3).
13. Ramani, R., Suthanthiravanitha, D. S., & Valarmathy, S. (2012). A survey of current image
segmentation techniques for detection of breast cancer. International Journal of Engineering
Research and Applications (IJERA), 2(5), 1124-1129.
14. Ponraj, D. N., Jenifer, M. E., Poongodi, P., & Manoharan, J. S. (2011). A survey on the preprocessing techniques of mammogram for the detection of breast cancer. Journal of Emerging
Trends in Computing and Information Sciences, 2(12), 656-664.
15. Nagi, J., Kareem, S. A., Nagi, F., & Ahmed, S. K. (2010, November). Automated breast profile
segmentation for ROI detection using digital mammograms. In Biomedical Engineering and
Sciences (IECBES), 2010 IEEE EMBS Conference on (pp. 87-92). IEEE.
16. Cheng, H. D., Shan, J., Ju, W., Guo, Y., & Zhang, L. (2010). Automated breast cancer detection
and classification using ultrasound images: A survey. Pattern recognition, 43(1), 299-317.
17. Ramani, R., Suthanthiravanitha, D. S., & Valarmathy, S. (2012). A survey of current image
segmentation techniques for detection of breast cancer. International Journal of Engineering
Research and Applications (IJERA), 2(5), 1124-1129.
18. Kanungo, T., Mount, D. M., Netanyahu, N. S., Piatko, C. D., Silverman, R., & Wu, A. Y.
(2002). An efficient k-means clustering algorithm: Analysis and implementation. IEEE transactions on pattern analysis and machine intelligence, 24(7), 881-892.
19. Patel, B. C., & Sinha, G. R. (2010). An adaptive k-means clustering algorithm for breast image
segmentation. International Journal of Computer Applications, 10(4), 35-38.
117
Dac-Nhuong Le et al. (eds.), Emerging Technologies for Health and Medicine, (117–284) 
© 2018 Scrivener Publishing LLC
CHAPTER 9
MOBILE DOCTOR BRAIN AI APP:
ARTIFICIAL INTELLIGENCE FOR IOT
HEALTHCARE
Bassant M.Elbagoury1, Ahmed A.Bakr1, Mohamed Roushdy1, Thomas
Schrader2
1 Faculty of Computer and Information Sciences, Ain Shams University, Cairo, Egypt
2 University of Applied Sciences Brandenburg, D- 14770 Brandenburg, Germany
Emails: miroushdy@hotmail.com, schrader@fh-brandenburg.de, ahmed.adel-bakr@vodafone.com
Abstract Mobile Health is a steadily growing field in telemedicine and it combines
recent developments in artificial intelligence and cloud computing with telemedicine applications. Stroke is an urgent case that may cause problems like weakness, numbness,
vision problems, confusion, trouble walking and talking. It is a leading cause of death in
the United States. In the recent research, what we witness is a high competition and new
revolution towards mobile health in general, especially in fields of brain stroke, chronic
stroke illnesses and stroke emergency cases. However, today’s Mobile Health research
still missing an intelligent remote diagnosis engine for patient emergency cases such as
Stroke. Moreover, Remote patient monitoring and emergency cases need an intelligent
algorithms to alert with better diagnostic decisions and fast response to patient care. This
research work proposes a Hybrid Intelligent remote diagnosis technique for Mobile Health
Application for Brain Stroke diagnosis.
Keywords: Artificial Intelligence, Telemedicine, EMG signal Processing, Mobile Health,
Brain Stroke, Neural Networks
118 Emerging Technologies for Health and Medicine
9.1 Introduction
Health monitoring is considered one of the main application areas for Pervasive computing. Mobile Health is the integration of mobile computing and health monitoring. It is
the application of mobile computing technologies for improving communication among
patients, physicians, and other health care workers [1]. Mobile Health applications are
receiving increased attention largely due to the global penetration of mobile technologies.
It is estimated that over 85% of the world’s population is now covered by a commercial
wireless signal, with over 5 billion mobile phone subscriptions [2].
Joseph John Oresko proposes a real-time, accurate, context aware ST segment monitoring algorithm, based on PCA and a SVM classifier and applied on smartphones, for the
detection of ST elevation heart attacks [3]. Feature extraction consists of heartbeat detection, segmentation, down sampling, and PCA. The SVM then classifies the beat as normal
or ST elevated in real-time.
Qiang Fang proposes an electrocardiogram signal monitoring and analysis system utilizing the computation power of mobile devices [4]. In order to ensure the data interoperability and support further data mining and data semantics, a new XML schema is designed
specifically for ECG data exchange and storage on mobile devices.
Madhavi Pradhan proposes a model for detection of diabetes [5]. Their proposed method
uses a neural network implementation of the fuzzy k-nearest neighbor algorithm for designing of classifier. The system is to be run on Smartphone to facilitate mobility to the user
while the processing is to be done on a server machine.
Oguz Karan presents an ANN model applied on Smartphone to diagnose diabetes [6].
In this study, three-layered Multilayer Perceptron (MLP) feedforward neural network architecture was used and trained with the error back propagation algorithm. The back propagation training with generalized delta learning rule is an iterative gradient algorithm designed to minimize the root mean square error between the actual output of a multilayered
feed-forward neural network and a desired output.
Peter Pes develops a Smartphone based decision support system (DSS) for the management of type 1 diabetes in order to improve quality of life of subjects and reduce
the aforementioned secondary complications [7]. The Smartphone platform implements
a case-based reasoning DSS, Which is an artificial intelligence technique to suggest an
optimal insulin dosage in a similar fashion as a human being would.
Jieun Kim proposes Case-Based Reasoning approach to match the user needs and existing services, identify unmet opportunistic user needs, and retrieve similar services with
opportunity based on Apple Smartphone [8].
9.2 State of the Art
9.2.1 Mobile Doctor AI App for Stroke Emergency in Haij Crowd
Mobile Health in remote medical systems has opened up new opportunities in healthcare
systems. It combines recent developments in artificial intelligence and cloud computing
with telemedicine applications. This technology help patients manage their treatments
when attention from health workers is costly, unavailable, or difficult to obtain regularly.
In fact remote monitoring - which is seen as the technology with the highest financial
and social return on investment, given current healthcare challenges - is a focus for many
of the pilot projects.
Mobile Doctor Brain AI App 119
Mobile Health for patient tracking supports the coordination and quality of care for
the benefits of rural communities including the urban poor, women, the elderly, and the
disabled. This would promote public health and prevent disease at the aggregate level [9].
Some stroke diseases which affect the nerves (e.g. Stroke) and cause problems with
thinking, awareness, attention and lead to emotional problems. Stroke patients may have
difficulty controlling their emotions or may express inappropriate emotions. So that brain
stroke is considered an emergency case that needs to be treated immediately before causing more problems. The proposed work aims to make use of mobile health applications
and artificial intelligent techniques in the field of brain stroke by proposing an intelligent
mobile health application based on EMG sensor which provides a significant source of information for identification of neuromuscular disorders and transfer experience of expert
doctors through Artificial Intelligence technology Case-Based Reasoning.
9.2.2 Proposed Architecture
In the presented research, we propose a New Stroke EMG based Real-Time AI Mobile
HealthCare Solution as shown in Figure 9.1.
Figure 9.1 Mobile Doctor Brain AI App
In the coming figures, we illustrate the main research components for AI, Real-Time
EMG sensor processing, IoT embedded technologies as shown in Figures 9.2 and 9.3.
Stroke is an urgent case that may cause problems like weakness, numbness, vision problems, confusion, trouble walking or talking, dizziness and slurred speech. It is a leading
cause of death in the United States. For these reasons, brain stroke is considered an emergency case as same as heart attack and needs to be treated immediately before causing
more problems.
The main objective of the proposed research is to Propose a Hybrid Intelligent remote
diagnosis Technique for Mobile Health Application for Brain Stroke diagnosis.
Another objective is monitoring human health conditions based on emerging wireless
mobile technologies with wireless body sensor.
120 Emerging Technologies for Health and Medicine
Figure 9.2 Research Area 1: AI for Raspberry pi - system on chip
Figure 9.3 Research Area 2: AI Real-time EMG Human Motion Analysis
The research work focuses also on delivering better healthcare to patients, especially in
the case of home-based care of chronic illnesses.
On the other hand, our designed prototype investigates the implementation of the neural
network on mobile devices and tests different models for better accuracy of diagnosis and
patient emergency. Integration of mobile technology and sensor in development of home
alert system (M-Health system) will greatly improve the lives of elderly by giving them
safety and security and preventing minor incidents from becoming life-threatening events.
9.3 Proposed System Design
9.3.1 AI Telemedicine Platform and Proposed System Architecture
Health monitoring is necessary for classifying disorders early and identifying their treatments. Since Mobile Health (M-Health) Care is one of the most interesting applications
of mobile technology, it can be used to improve communications between patients and
physicians. For this, M-Health is defined as integration of mobile computing and health
monitoring. It enables the delivery of accurate medical information anytime and anywhere.
Mobile Doctor Brain AI App 121
The proposed system includes a Hybrid Intelligent Remote Diagnosis Technique for
M-Health Application in stroke diseases. This will focus on new trends of integrating
artificial intelligence methodologies as neural networks (NN’s) and case-based reasoning
(CBR) into mobile telemedicine solutions and cloud platform as shown in Figure 9.4. The
proposed AI Telemedicine platform covers the process of monitoring, signal processing,
and management of Intelligent telemedicine care. The following figure shows the general
process of signal processing and feature extraction and interaction with the patient.
Figure 9.4 General process model for Artificial Intelligence Telemedicine sensor data management
(Three Layers: Signal Processing, Mobile Data Aggregation with AI Engine an Cloud CBR Patients
Expert system)
The clue is the distributed, level based sensor data evaluation process: the first level
includes the sensor nodes themselves with a basic but very fast signal processing. Aggregated data will be sent to the mobile unit/device as second level, this will take real-time
(EMG) data read through the mobile device which sends urgent event to the hospital server.
The system can also respond by immediate recommendation and sends patient data to responsible doctor or nurse. Moreover, the next processing step can be done. The second
and third level (server/cloud based signal processing) covers intelligent data processing
and decision support for interaction and Telemedicine Management.
9.3.2 Wireless intelligence sensor network extract user’s biofeedback signal
Many physiological processes can be monitored for biofeedback applications, and these
processes are very useful for rehabilitation services. Biofeedback is a means for gaining
control of our body processes to increase relaxation, relieve pain, and develop healthier,
more comfortable life patterns. Biofeedback is a broader category of methods. These methods use feedback of various physiological signals, such as EEG electroencephalographic
or brainwave, electrical activity of muscles (EMG), bladder tension, electrical activity of
the skin (EDA/GSR), or body temperature. These methods are applied to treatment or improvement of organism functions as reflected by these signals which can be detected by the
wearable health-monitoring device.
122 Emerging Technologies for Health and Medicine
This system will take real-time (EMG) data and read through the mobile device which
sends urgent event to the hospital server as shown in Figure 9.5. The system can also
respond by immediate recommendation and sends patient data to responsible doctor or
nurse.
Figure 9.5 Patient Emergency Scenario for Stroke/Heart Attack Elderly and Expert Doctor
Recommendations
9.4 Proposed Artificial Intelligence Techniques for New AI IoT Health-Care
Solutions for Stroke Monitoring
9.4.1 Support vector machine (SVM)
Support Vector Machine (SVM) is a powerful learning method used in binary classification.
It is a supervised learning model with associated learning algorithms that analyze data
and recognize patterns. Its main task is to find the best hyper plane that can separate
data perfectly into its two classes. Recently, multi-class classification was achieved by
combining multiple binary SVMs. SVM architecture is shown in Figure 9.6.
The function of the hyper plane that classify training and testing data can be expressed
as following
f(x) = sign 
N
i=1
αiyik(xi, x) + b

(9.1)
Where N is the number of training instances, xi is the input of training instance and
yi is its corresponding class label, b is a bias, and K(xi, x) is the used kernel function
which maps the input vectors into an expanded feature space. And The coefficients αi are
obtained subject to two constraints given in the following two functions:
0 ≤ αi, i = 1, .., N (9.2)
Mobile Doctor Brain AI App 123
Figure 9.6 Architecture of support vector machine

N
i=1
αiyi = 0 (9.3)
SVM algorithm is probably the most widely used kernel learning algorithm. It achieves
relatively robust pattern recognition performance using well established concepts in optimization theory. Most common Kernel functions used in support vector machine classifier
include linear, polynomial, radial basis and quadratic kernels are listed below [10].
The Linear kernel: is the simplest kernel function. It is given by the common dot
product < xa, xb > plus an optional constant c. Kernel algorithms using a linear kernel
are often equivalent to their non-kernel counterparts. This kernel is only defined when the
data to be analyzed are vectors.
K(xa, xb) = xT
a xb + c (9.4)
Where xa, xb are objects from the dataset and c is an optional constant.
The Polynomial kernel: is a non-stationary kernel. It is well suited for problems where
all data is normalized.
K(xa, xb)=(αxT
a xb + c)
d (9.5)
Adjustable parameters are the slope (alpha α), the constant term c and the polynomial
degree d.
The Radial Basis Kernel function (RBF) is one of the most frequently used kernels in
practice. It is a decreasing function of the Euclidean distance between points, and therefore
has a relevant interpretation as a measure of similarity: the larger the kernel (xa, xb), the
closer the points xa and xb
K(xa, xb) = exp 
−xa − xb
2
2σ2

(9.6)
124 Emerging Technologies for Health and Medicine
The adjustable parameter sigma (σ) plays a major role in the performance of the kernel,
and should be carefully tuned to the problem at hand. xa − xb is the Euclidean distance
between the two objects x(a), xb.
The Rational Quadratic kernel is less computationally intensive than the Gaussian
kernel and can be used as an alternative when using the Gaussian becomes too expensive.
K(xa, xb)=1 − xa − xb
2
xa − xb
2 + c
(9.7)
We made a comparative work [11] on EMG physical action Data set from the machine learning repository (UCI) [12]. We investigates the usage of support vector machine
(SVM) classifiers with different kernel functions for identification of different hands and
legs, normal and auto aggressive actions from EMG data, and made a comparison between
classifications accuracies of each kernel function applied on different groups of actions. In
those experiments we used polynomial, quadratic and radial basis function with different
values of sigma which plays an important role in the classification accuracy.
The following table shows sample of published experimental results [11, 22]
Table 9.1 Sample of published experimental results
Accuracies obtained from applying RBF kernel function with different sigma values
(ex. on kneeing and pulling actions) are showed in Figure 9.7.
Figure 9.7 Classification accuracies for RBF kernel with different sigma values for Kneeing and
Pulling actions
Mobile Doctor Brain AI App 125
9.4.2 Case-based Reasoning
Case-Based Reasoning (CBR) suggests a model of reasoning that depends on experiences
and learning. CBR solves new cases by adapting solutions of retrieved cases. Recently,
CBR is considered as one of the most important Artificial Intelligent (AI) techniques used
in many medical diagnostics tasks. CBR has already been applied in a number of different
applications in medicine.
CBR Cycle [13]: The four processes Retrieve, Reuse, Revise, and Retain describe the
general tasks in a casebased reasoner. They provide a global external view to what is
happening in the system. The four tasks are decomposed into a hierarchy of CBR tasks the
system has to achieve.
1. Retrieval: An important step in the CBR cycle is the retrieval of previous cases that
can be used to solve the target problem. In this step the casebased reasoner retrieves
the most similar case or cases to the input case according to a predefined similarity
measure.
2. Reuse: in this step CBR reasoner evaluates retrieved cases in order to decide if the
solution retrieved is applicable to the problem.
3. Revise: Revising (adapting) the solution manually or automatically and validating
through feedback from the user.
4. Retain: Adding the confirmed solution with the problem, for future reuse, as a new
case in the database.
CBR Cycle is shown in Figure 9.8.
Figure 9.8 Case-Based Reasoning Cycle
126 Emerging Technologies for Health and Medicine
9.4.3 Particle Swarm Intelligence and ARX Model for Stroke Motion Estimation and Optimization
Particle Swarm Optimization (PSO) is population based stochastic optimization method
inspired by social behaviour of bird flocking [22]. PSO exploits a population of individuals to probe promising regions of the search space. In the context, the population is called
a swarm and the individuals are called particles. Each particle moves with an adaptable
velocity within the search space, and retains in its memory the best position it ever encountered. In the global variant of PSO the best position ever attained by all individuals of
swarms is communicated to all the particles.
Also In the statistical analysis of time series, auto-regressivemoving-average (ARMA)
models [23] are essential for real-time EMG data analysis [11] and their dynamical behavior. The used commercial EMG sensor is Shimmer [24] as shown in Figure 9.9.
Figure 9.9 EMG Commerical Shimmer Sensor
9.5 Conclusion
This chapter proposes a new stroke EMG based Real-Time AI Mobile HealthCare Solution.
The Hybrid Intelligent remote diagnosis technique for Mobile Health Application for Brain
Stroke diagnosis based on the support vector machine, case-based reasoning, and particle
swarm intelligence.
REFERENCES
1. Shahriyar, R., Bari, M. F., Kundu, G., Ahamed, S. I., & Akbar, M. M. (2009, September). Intelligent mobile health monitoring system (IMHMS). In International Conference on Electronic
Healthcare (pp. 5-12). Springer, Berlin, Heidelberg.
2. Royal Tropical Institute: What is mHealth? [http://www.mhealthinfo.org/what-mhealth]
3. Oresko, J. J. (2010). Portable heart attack warning system by monitoring the ST segment via
smartphone electrocardiogram processing (Doctoral dissertation, University of Pittsburgh).
4. Fang, Q., Sufi, F., & Cosic, I. (2008). A mobile device based ECG analysis system. In Data
Mining in Medical and Biological Research. InTech.
5. Pradhan, M., Kohale, K., Naikade, P., Pachore, A., & Palwe, E. (2012). Design of classifier
for detection of diabetes using neural network and fuzzy k-nearest neighbor algorithm. International Journal of Computational Engineering Research, 2(5), 1384-1387.
Mobile Doctor Brain AI App 127
6. Karan, O., Bayraktar, C., Gmkaya, H., & Karlk, B. (2012). Diagnosing diabetes using neural
networks on small mobile devices. Expert Systems with Applications, 39(1), 54-60.
7. Peter Pesl, Pau Herrero, Mobile-Based Architecture of a Decision Support System for Optimal
Insulin Dosing, Imperial Comprehensive Biomedical Research Centre, 2010.
8. Kim, J., Park, Y., & Lee, H. (2012, December). Using case-based reasoning to new service
development from user innovation community in mobile application services. In International
Conference on Innovation, Management and Technology (ICIMT 2012), Phuket, Thailand.
9. Qiang, C. Z., Yamamichi, M., Hausman, V., Altman, D., & Unit, I. S. (2011). Mobile applications for the health sector. Washington: World Bank, 2.
10. Kaur, G., Arora, A. S., & Jain, V. K. (2009). Multi-class support vector machine classifier in
EMG diagnosis. WSEAS Transactions on Signal Processing, 5(12), 379-389.
11. Farid, N., Elbagoury, B., Roushdy, M. O. H. A. M. E. D., & Salem, A. B. (2013). A Comparative Analysis for Support Vector Machines For Stroke Patients. Rec Adv Inf Sci, 71-76.
12. http://archive.ics.uci.edu/ml/datasets/
13. Roth-Berghofer, T., & Iglezakis, I. (2001). Six Steps in Case-Based Reasoning: Towards a
maintenance methodology for case-based reasoning systems. In In: Professionelles Wissensmanagement: Erfahrungen und Visionen (includes the Proceedings of the 9th German Workshop on Case-Based Reasoning (GWCBR.
14. Kahn, J. G., Yang, J. S., & Kahn, J. S. (2010). Mobile health needs and opportunities in
developing countries. Health Affairs, 29(2), 252-258.
15. Kulek, J., Huptych, M., Chudek, V., Spilka, J., & Lhotsk, L. (2011, September). Data driven
approach to ECG signal quality assessment using multistep SVM classification. In Computing
in Cardiology, 2011 (pp. 453-455). IEEE.
16. Hu, S., Wei, H., Chen, Y., & Tan, J. (2012). A real-time cardiac arrhythmia classification
system with wearable sensor networks. Sensors, 12(9), 12844-12869.
17. Dragoni, M., Azzini, A., & Tettamanzi, A. G. B. (2012). A neuro-evolutionary approach to
electrocardiographic signal classification. In Italian Workshop on Artificial Life and Evolutionary Computation (WIVACE) (pp. 1-11). Universit degli Studi di Parma, Dipartimento di
Scienze Sociali.
18. Curran K, Nichols E, Xie E, Harper R propose a solution in the form of an intelligent neural
network running on mobile devices, allowing people with diabetes access to it regardless of
their location.
19. http://crsouza.blogspot.com/2010/03/kernel-functions-for-machine-learning.html
20. Rekhi, N. S., Arora, A. S., Singh, S., & Singh, D. (2009, June). Multi-class SVM classification
of surface EMG signal for upper limb function. In Bioinformatics and Biomedical Engineering,
2009. ICBBE 2009. 3rd International Conference on (pp. 1-4). IEEE.
21. Khokhar, Z. O., Xiao, Z. G., & Menon, C. (2010). Surface EMG pattern recognition for realtime control of a wrist exoskeleton. Biomedical engineering online, 9(1), 41.
22. Elbagoury, B. M., & Vladareanu, L. (2016, December). A hybrid real-time EMG intelligent
rehabilitation robot motions control based on Kalman Filter, support vector machines and particle swarm optimization. In Software, Knowledge, Information Management & Applications
(SKIMA), 2016 10th International Conference on (pp. 439-444). IEEE.
23. http://zone.ni.com/reference/en-XX/help/372458C-01/lvsysidconcepts/modeldefinitionsarx/
24. http://www.shimmersensing.com/products/shimmer3-emg-sensor
129
Dac-Nhuong Le et al. (eds.), Emerging Technologies for Health and Medicine, (129–284) 
© 2018 Scrivener Publishing LLC
CHAPTER 10
AN ARTIFICIAL INTELLIGENCE
MOBILE-CLOUD COMPUTING TOOL:
EXPERIMENTAL RESULTS BASED ON
PHYSICIANS’ AND PATIENTS’ VIEWS OF
CANCER CARE BY FAMILY MEDICINE
M. Hassan Bin Shalhoub1, Mohammed H. Bin Shalhoub1, Mariam Marzouq Al-Otaibi3, Bassant M. Elbagoury4
1 Consultant of Information Technology at Ministry of Interior, Riyad, KSA
2 Faculty of Medicine, King Abdel-Aziz University, Jeddah, KSA
3 Consultant of Family Medicine, Faculty of Medicine, King Abdel-Aziz University, Jeddah, KSA
4 Faculty of Computer and Information Sciences, Ain Shams University, Cairo, Egypt
Emails: shalhoub@live.com, bassantai@yahoo.com
Abstract Intelligent Information Systems and Cloud computing in e-health sector such
as teleradiology [1, 2] in remote medical systems has opened up new opportunities in
healthcare systems. Artificial Intelligence and teleradiology are a steadily growing field in
telemedicine, and they combine recent developments in decision support systems (DSS)
and teleradiology images processing with Cloud-Computing e-health systems [3]. In the
market today, what we witness is a high competition and new revolution towards DSS and
Cloud Computing e-health (Telemedicine) sector in general. As, in last April 2011, Mobinil
made a protocol with IT company for health [4]. After one month, Vodafone made another
protocol with Ericsson for Mobile Health [5]. The possibilities of Artificial Intelligent
in Medicine to enhance the e-health services in the region along with cloud-computing
platform setting is going to be investigated by building new intelligent algorithms for remote diagnosis and then by evaluating their feasibility. The mission of this chapter is
to investigate recent Artificial Intelligence technologies that are to propose a novel integrated Intelligent Remote Diagnosis system for Information System for Cancer diseases
based on Cloud-Computing platform. This is to investigate artificial intelligent techniques,
namely case-based reasoning (CBR) and neural networks (NN). Also, advances in cloudcomputing and teleradiology data can be used for overall patient health management.
Keywords: Artificial Intelligence, Mobile-Cloud Computing, Intelligent Remote Diagnosis
130 Emerging Technologies for Health and Medicine
10.1 Introduction
Intelligent Cloud Computing Teleradiology [1, 2] in remote medical systems has opened up
new opportunities in healthcare systems. Mobile teleradiology is a steadily growing field
in telemedicine, and it combines recent developments in teleradiology images processing
and networking with telemedicine applications [3].
In the market today, what we witness is a high competition and new revolution towards
mobile health in general. As, in last April 2011, Mobinil made a protocol with IT company
for Egyptians health [4]. After one month, Vodafone made another protocol with Ericsson
for Mobile Health [5].
Since year 2000, we are international reviewers and researchers in the field of intelligent algorithms for cancer diagnosis and also medical imaging processing [6-9]. This is
why; we believe that today’s mobile health products still missing a real intelligent remote
diagnosis engine for cancer remote diagnosis. Moreover, Complexities of cancer domains
may lead to uncertain diagnosis decisions.
In this paper, we want to extend our experience to mobile teleradiology for cancer patients. Our main goal is to propose, a novel integrated Intelligent Remote Diagnosis system
for Mobile Teleradiology. It will combine two main intelligent techniques, namely casebased reasoning [CBR] [10-12] and Neural Networks [NN]. Also, advances in mobile
medical imaging retrieval as Context-Based Image Retrieval [CBIR] [13] and segmentation algorithms such as genetic algorithm [8], watershed algorithm, active contouring and
cellular automata Grow Cut method will be important parts in our research proposal. In
the coming parts of the proposal, we are going to state more clearly our main research
objective and describe each technique in more details.
10.2 Background and State-of-the-Art
Wireless transfer of radiology images to a portable computer was reported early in emergency medicine [29]. With the advance of digital cellular phones and worldwide digital
networks image transmission has been extended to major catastrophe sites and for purposes such as delivering information needed in post mortem recognition of bodies [29].
Wireless technology is identical to conventional teleradiology, with the obvious difference of using a radio frequency wireless network for the digital communication of radiographic images. There are four major disadvantages with wireless technologies: reliability,
cost, security and speed. The obvious major advantage of such technology is the ability
to view images virtually anywhere, from a hospital patient room to the quagmire of the
battlefield.
Utilization of ICT (information and communication technology) in health care has moved
from individual projects and services to a more comprehensive model. Instead of telemedicine,
terms like telehealth, eHealth, on-line health and most recently connected health (Microsoft
Corporation 2009) [29] have emerged.
eHealth is an umbrella term that gathers together ICT usage in health care (World Health
Organization 2010). It includes major infrastructure services and at the other end also
services targeted to individual citizens.
According to the EU eEurope action plan ”An information society for all” eHealth could
also seen as a derivative of eServices, in line with similar terms like e-Commerce or eGovernance (COM/2002/0263 final). The main political goal is to deliver more accessible
and better services to citizens [29].
An Artificial Intelligence Mobile-Cloud Computing Tool 131
In the future health care environment teleradiology has a strong role as one of the services given [29]. It is a means of delivering care, but not a medical discipline in itself. One
key issue is the well-established integration to other health information systems, especially
RIS. From the point of view of users, usability and reliable functionality are key issues
[29].
10.3 Development and Proposing a New Intelligent case-based Reasoning
Decision Engine for Cacer Diagnosis
This section illustrates a prototype hybrid Intelligent decision support system namely CancerC for cancer diagnosis, which is applied to thyroid cancer. Cancer-C is based on the case
based reasoning methodology.
The main aim of our research was to develop a new adaptation model, which uses
much less adaptation knowledge. The model combines transformational and hierarchical
adaptation techniques with ANN’s. The ANN’s are trained on transformational rules to
learn how to make adaptation to avoid training with retrieved patient cases, which may
have very similar features but completely different diagnoses. Also, to avoid the problem
faced with other medical CBR system that uses a large set of transformational rules. A
high performance rate of diagnosis is achieved at different ranges of similarities between
the new case and the retrieved case. The average of the similarity ranges is [40%-100%].
Figure 10.1 A hierarchical Case-Based Decision Support System for Cancer Diagnosis on three
levels of medical expert
132 Emerging Technologies for Health and Medicine
The model consists of a hierarchy of three phases, which simulates the expert doctors
reasoning phases for cancer diagnosis. CF’s are also added to reflect our expert doctors’
feelings of cancer suspicion. Figure 10.1 shows the architecture of the proposed CaseBased decision support architecture.
In our current research work, we want to implement this system on mobile devices to
act as a pre-diagnosis/diagnosis tool for Physicians and Patients. More experts will be
involved and more cases will be collected for better accuracy and reliability of diagnosis
decisions.
As shown a new patient case is diagnosed by using our proposed case-based decision
support model, Case diagnosis is performed in a top-down fashion using a hierarchy of
three phases, which simulates the expert doctor phases of cancer diagnosis, the Suspicionphase is for diagnosing cancer suspicion, the To-Be-Sure-phase is for diagnosing cancer
type and the Stage-phase is for diagnosing cancer stage. All the three phases are similar in
their structure but they are different in their inputs and outputs. Each phase uses a single
ANN. The final diagnosis of the new patient case is composed from the adapted sub-cases
diagnoses of the three phases, all of which are then evaluated by the expert doctor.
10.4 Experimental Results of The Proposed System
Expert doctors in the National Cancer Institute of Egypt supplied our system case-memory
with 820 real patient cases and a detailed analysis of thyroid cancer diseases. This is
besides other cancer resources from the Internet.
As explained by our expert doctors, a typical patient case consists of 44 features, which
are critical for the diagnosis. These features can be divided into groups of features.
Figure 10.2 Frame scheme of the 44 real medical features of thyroid cancer
The first group contains 18 features of the initial symptoms of the disease. The second
group contains 15 features of the lab-tests and scans results. The third group contains
11 stage features of the malignant disease, if exists. These groups appear to be mutually
An Artificial Intelligence Mobile-Cloud Computing Tool 133
exclusive, so we decompose each case in the case-memory into sub-cases, which are the
Symptoms-Sub-case, the Scans-Sub-case and the Stage-Sub-case. Figure 10.2. Shows a
frame description of the 44 thyroid cancer features.
Performance Measures: We have designed two performance measures for our decision
support system. They are diagnosis performance model and the adaptation performance
[6].
Diagnosis Model Performance: In our cross-validation test, 80 cases of the 220 definite cases are used for testing and the other 140 cases are stored in the case-memory to be
used for retrieval. Also, 300 cases of the 600 indefinite cases are used for testing, while
the other 300 cases are stored in the case-memory to be used for retrieval. That is, a total
of 380 cases are used for testing and a total of 440 cases are stored in the case-memory to
be used for retrieval. Table 10.1 shows the diagnosis performance (accuracy rate) of our
hybrid adaptation model.
The diagnosis performance at each phase is calculated as:
P haseAccuracy = T C
T T (10.1)
where, TC is the total number of test sub-cases diagnosed correctly and TT is the total
number of test sub-cases used for testing.
At our first experiments the model shows an overall high accuracy but this because all
features values are in binary.
Table 10.1, shows our algorithm performance. As shown, high retrieval performance
is achieved for each set of cases. This high accuracy is due to the following main factors,
which we fixed in our experiments: Usage of fixed features dataset that is well formatted
and Use of Nearest-Neighbour retrieval algorithm [11].
Table 10.1 Retrieval Accuracy of the CBIR
Accuracy Rate No. Of Test Cases Average Retrieval Accuracy
1. Benign 200 89%
2. Malignant 299 95%
10.5 Conclusion
This chapter is to investigate recent Artificial Intelligence technologies that are to propose a novel integrated Intelligent Remote Diagnosis system for Information System for
Cancer diseases based on Cloud-Computing platform. This is to investigate artificial intelligent techniques, namely case-based reasoning (CBR) and neural networks (NN). Also,
advances in cloud-computing and teleradiology data can be used for overall patient health
management.
REFERENCES
1. Mobile Teleradiology http://www.ncbi.nlm.nih.gov/pubmed/20824300
2. Mobile Teleradiology http://www.sciencedirect.com/science/article/pii/S053151310500316X
134 Emerging Technologies for Health and Medicine
3. Mobinil Mobile Health http://www.image-systems.biz/en/products/iq-mobility.html
4. Vodafone Mobile Health http://mhealth.vodafone.com/global/mhealth/home/index.jsp
5. Abdel-badeeh, M. S., & El Bagoury, B. M. (2003). A hybrid case-based adaptation model for
thyroid cancer diagnosis. In In ICEIS 2003, Proceedings of the 5th International Conference
on Enterprise Information Systems.
6. Im, K. H., & Park, S. C. (2007). Case-based reasoning and neural network based expert system
for personalization. Expert Systems with Applications, 32(1), 77-85.
7. Jele, ., Fevens, T., & Krzyak, A. (2008). Classification of breast cancer malignancy using cytological images of fine needle aspiration biopsies. International Journal of Applied Mathematics
and Computer Science, 18(1), 75-83.
8. Hrebie, M., Ste, P., Nieczkowski, T., & Obuchowicz, A. (2008). Segmentation of breast cancer fine needle biopsy cytological images. International Journal of Applied Mathematics and
Computer Science, 18(2), 159-170.
9. El Balaa, Z., & Traphner, R. (2003, April). Case-Based Decision Support and Experience
Management for Ultrasonogrphy. In Wissensmanagement (pp. 277-278).
10. El Balaa, Z., Strauss, A., Uziel, P., Maximini, K., & Traphoner, R. (2003, June). Fm-ultranet: a
decision support system using case-based reasoning, applied to ultrasonography. In Workshop
on CBR in the Health Sciences (Vol. 37, pp. 0-3).
11. Multidisciplinary Management of Cancers A Case-Based Approach
http://cancer.stanford.edu/documents/2010CME-CancerBrFinalp4.pdf
12. Liu, Y., Zhang, D., Lu, G., & Ma, W. Y. (2007). A survey of content-based image retrieval
with high-level semantics. Pattern recognition, 40(1), 262-282.
13. Antani, S. K., Long, L. R., & Thoma, G. R. (2004, September). Content-based image retrieval
for large biomedical image archives. In Medinfo (pp. 829-833).
14. Lehmann, T. M., Gld, M. O., Thies, C., Fischer, B., Spitzer, K., Keysers, D., ... & Wein, B.
B. (2004). Content-based image retrieval in medical applications. Methods of information in
medicine, 43(04), 354-361.
15. Obuchowicz, A., Hrebie, M., Nieczkowski, T., & Marciniak, A. (2008). Computational intelligence techniques in image segmentation for cytopathology. In Computational intelligence in
biomedicine and bioinformatics (pp. 169-199). Springer, Berlin, Heidelberg.
16. Hrebie, M., Korbicz, J., & Obuchowicz, A. (2007). Hough transform,(1+ 1) search strategy and
watershed algorithm in segmentation of cytological images. In Computer Recognition Systems
2 (pp. 550-557). Springer, Berlin, Heidelberg.
17. Plagianakos, V. P., Magoulas, G. D., & Vrahatis, M. N. (2006). Distributed computing methodology for training neural networks in an image-guided diagnostic application. Computer methods and programs in biomedicine, 81(3), 228-235.
18. Schmidt, R., & Gierl, L. (2000). Case-based reasoning for medical knowledge-based systems.
Studies in health technology and informatics, 720-725.
19. Hrebie, M., Ste, P., Nieczkowski, T., & Obuchowicz, A. (2008). Segmentation of breast cancer fine needle biopsy cytological images. International Journal of Applied Mathematics and
Computer Science, 18(2), 159-170.
20. Jeng, B. C., & Liang, T. P. (1995). Fuzzy indexing and retrieval in case-based systems. Expert
systems with applications, 8(1), 135-142.
21. Im, K. H., & Park, S. C. (2007). Case-based reasoning and neural network based expert system
for personalization. Expert Systems with Applications, 32(1), 77-85.
22. Aamodt, A., & Plaza, E. (1994). Case-based reasoning: Foundational issues, methodological
variations, and system approaches. AI communications, 7(1), 39-59.
An Artificial Intelligence Mobile-Cloud Computing Tool 135
23. Jurisica, I., & Glasgow, J. (1996, November). Case-based classification using similarity-based
retrieval. In Tools with Artificial Intelligence, 1996., Proceedings Eighth IEEE International
Conference on (pp. 410-419).
137
Dac-Nhuong Le et al. (eds.), Emerging Technologies for Health and Medicine, (137–284) 
© 2018 Scrivener Publishing LLC
CHAPTER 11
ADVANCED INTELLIGENT ROBOT
CONTROL INTERFACES FOR THE
VIRTUAL REALITY SIMULATION APPLIED
TO THE INDUCTION HARDENING
PROCESS
Gal Ionel-Alexandru1, Vladareanu Luige1,∗ and Shuang Cang2
1 Institute of Solid Mechanics of Romanian Academy, Romanian Academy, Bucharest, Romania
2 Northumbria University, Newcastle Business School, Faculty of Business and Law, UK
Emails: luigiv2007@yahoo.com.sg; luige.vladareanu@vipro.edu.ro; galexandru2003@yahoo.com
Abstract. The paper presents advanced intelligent control interfaces, for the virtual reality simulation of the robot mechanical structure Ro CIF VIP, using Unity3D, applied to
the induction hardening process. A mechanical structure with 5 degrees of freedom, designed to be used for induction hardening process of building metallic profiles is proposed.
All 5 joints of the proposed mechanical structure are prismatic, required to move a metallic
profile through the inducer. The simulation was achieved using Unity3D software which
provides the virtual environment needed for our purposes. To use the virtual simulation
of the structure, we have had to build software components to help us gain access to the
simulated components during the simulation. More components were added to implement
the user interface and also the management backbone of the entire simulation. To analyze
the experimental data, we have built our own system to save the joint values and the first 4
parameters of Interface class are values used for the advanced intelligent control interfaces.
The experimental data of the reference and actual values for two gripping prismatic joints
are also analyzed and presented graphically. All of the software components which are
used to interact with the mechanical system or its joints have been configured individually,
supplying environment or physical variables for each component. The results obtained lead
to the development of advanced intelligent control interfaces for the 5 DOF robot, Ro CIF
VIP, and its PID control law using this virtual reality simulation, and their testing before
building the actual robot.
Keywords: Virtual environment; Simulation; Unity3D; Induction hardening.
138 Emerging Technologies for Health and Medicine
11.1 Introduction
While metallic frame buildings are gaining popularity, the industrial process of building
these structures is continuously being improved. For this, an approach is to use induction
hardening for certain elements of the structure, to improve their strength and maximum
load. But induction hardening is not an easy process to use, because of several parameters
which need to be configured, one of these being the induction swipe speed. While other
researches focus on different alloys to be hardened [1] or on the preheating process [2]
before doing the induction hardening, other test the effect of high frequency induction on
different applications [3-5]. Porpandiselvi et. al [6] achieved a new induction hardening
process in which they use two different inverters to harden a required object with high and
low frequencies. But Neumeyer et. al [7] managed to simulate the induction hardening,
demonstrating how the profile temperature is time dependent and can be calculated using
finite element analysis, and optimize power and frequency for the hardening process.
What we want to achieve in the end, by using the proposed mechanical structure, is a
controlled system which can vary the velocity of the metallic profile while being subjected
to the induction hardening process. In this paper we present the mechanical structure
which will achieve the induction hardening, simulated using Unity3D. For this we have
had to build our own virtual environment and import the 3D structure, while developing
software modules to help with the simulation.
While Mahayudin et. al [8] is studying ways to visualize virtual environments by using
Unity3D, Ruzinor et. al [9-11] have researched the way to simulate big virtual working
environments, and Shin et. al [12] studies how to develop the 3D virtual environment and
how to navigate through it for simulations on navigation in virtual scenarios.
Even if the researches using the virtual environment Unity3D are at the beginning, Chen
et. al [13] have developed a virtual application for testing a 3D vehicle inside the Unity3D
software. Also, research on virtual environment provided by Unity3D was not limited to
terrestrial vehicles, but was also conducted on aerial UAVs [14].
Certain researchers have even used Unity3D to simulate through animation, testing of
real robots by connecting them with the virtual environment through specific interfaces
[15, 16], allowing the robot to experience different simulated scenarios.
11.2 Proposed Mechanical Structure
The proposed mechanical structure Ro CIF VIP [17] with 5 degrees of freedom is presented
in Figure 11.1. The structure is made to apply induction hardening treatment on metallic
profiles of length no more than 60cm. The structure can be divided into 4 main sections
according to their designation. The first section contains two prismatic joints. One joint
is for grabbing the metallic profile (top green part in Figure 11.1) when the user inserts
it inside the machine, and the other is for sliding it down through the inducer (top yellow
part in Figure 11.1). The second section is made out of the inducer and cooling system
(the inducer has magenta color in Figure 11.1). At this point this section has no degree
of freedom. The third section is similar to the first as it consists of two prismatic joints.
The first one will grab the metallic profile (bottom green component in Figure 11.1) as it
comes out of the inducer, and the second will slide the profile down through and out of the
inducer section (bottom yellow component in Figure 11.1). The fourth and final section is
the extraction system (blue part in Figure 11.1). This is made out of one prismatic joint and
Advanced Intelligent Robot Control Interfaces 139
it will take out of the mechanical system the metallic profile when the induction process
was completed.
Figure 11.1 Proposed mechanical structure of Ro CIF VIP with 5 DOF
11.3 Unit 3D Integration
For each prismatic joint described and presented in Figure 11.1, we have had to simulate
it in the virtual environment. This was achieved by adding prismatic joints to all 5 degrees
of freedom. Unity3D provides many tools for building a virtual simulation but it does
not provide a specific prismatic joint component. This is why we have had to add one,
ourselves.
The main advantage in working with Unity3D is that it provides the possibility to create new components, starting from existing ones or just starting from scratch. The other
advantage is working with C# which is an advanced programming language. With this we
have created several components of the simulation.
The component presented in Figure 11.2, is the Translation Joint class which implements a prismatic joint. We can see in this figure that the class has several fields of data
and methods to compute different parameters or methods that allow external objects to
access data information.
One important component of the Translation Joint class is the OnPositionChanged
Event. This event, which is present in Figure 11.2, will be triggered every time the joint
140 Emerging Technologies for Health and Medicine
Figure 11.2 Translation Joint Class diagram
will move. This will allow any components to be executed when the joint is moving, to
check if the position is close to a position constraint or to trigger a proximity sensor. With
this component we can easily simulate proximity sensors, or just to detect when the joint
is moving.
Figures 11.3 through Figures 11.5 presents the prismatic joint components added to
the simulated mechanical components. As one can see, every component is configured
differently, depending on each joint’s parameters. In each figure, the top field states the
name of the game object to which the joint component is attached. In this way, we can’t
make mistakes while configuring the component, because we always know which part is
being configured. Also, in these figures there are several other components which we have
not detailed, that contain the relative position of the game object (Transform component)
and the physical parameters like mass and density (Rigidbody component). All of these
components form just one small part of the entire structure, but when put together they
form the virtual simulation of the entire mechanical structure of the robot Ro CIF VIP [18,
19] with 5 DOF.
Comparing Figure 11.2 which contains the Translation Joint class diagram with Figures
11.3 to Figures 11.5 one can see that every parameter is present and configured. Thus, we
have computed and inserted the maximum velocity and maximum acceleration which the
joint PID controller will use to filter these values. The axis parameter configures the axis
on which the joint is moving. This means that we can tell the component the direction
of motion, which can be just one axis or a combination of all 3. Also the sign is very
important since it will provide the positive direction.
Two important parameters of the Translation Joint component are the Min and Max
position. These values define the motion limits of the joint in Cartesian coordinates, given
Advanced Intelligent Robot Control Interfaces 141
in the operational space. Based on these values we have defined the Ref Pos parameter
which is presented as a sliding controller in Figures 11.3 to Figures 11.5.
Figure 11.3 Top (a) and bottom (b) sliding prismatic joint
Figure 11.4 Top (a) and bottom (b) gripper prismatic joint
This parameter can define the target position as a percentage between the minimum and
maximum limit values. This means that we can set the reference as the end position, since
the maximum acceleration and velocity are limited, we can work with these parameters
142 Emerging Technologies for Health and Medicine
to define the Translation Joint motion to the desired position. Of course, we can disable
the limits using parameter UseLimits and then the joint has to work with operational space
reference points. This is required when a translation joint reference is given in Cartesian
coordinates.
Figure 11.5 Extraction prismatic joint
To control the prismatic joints of our robot, we have implemented a PID controller. The
control law will control the velocity of the joints, because we require a certain speed of the
metallic profile going through the inducer. For this, every Translation Joint component, has
3 parameters: P (proportional), I (integrative) and D (derivative). These 3 parameters were
configured for each joint individually. But because these particular joints have the same
task, they will behave the same, so the parameters are identical for each two components:
top and bottom sliding prismatic joints, top and bottom gripper joints. The extractor was
configured as the sliding joints. Using these parameters, the PID controller will control
each joint individually to achieve the desired reference velocity and position.
The second component called Interface is the GUI component of this virtual simulation,
and it is initialized using the root object within the simulation. This means that this component does not depend on the robot behavior within the simulation scene, and will even
be present if the robot is switched with another.
Figure 11.7 presents the class diagram of SimStarter object. Comparing with Figure
11.6, we identify the parameter responsible with linking the application with the robotic
structure of Ro CIF VIP, named cifPrefab. This component stores the link to the prefab file
that contains the entire robot structure from Figure 11.1.
Using this reference we can even add multiple robots in the same virtual environment.
Advanced Intelligent Robot Control Interfaces 143
Figure 11.6 Root component which starts the entire simulation
Figure 11.7 SimStarter class diagram
Figure 11.8 GUI Class which provides commands to the user
Figure 11.8 presents the Interface class diagram. This class has few variables, but has
several methods called when the user presses a button to do an action. With the tools that
Unity provides, we can build a good GUI in a short time that will fulfill our every need.
The first 4 parameters of Interface class are values required to use with the intelligent
interfaces. These parameters will take the user input and compute the robot parameters
144 Emerging Technologies for Health and Medicine
required for the CIF process. The next 4 parameters are values required for GUI to switch
between different states of the interface. This means that by pressing different buttons the
GUI will change its appearance.
Figure 11.9 and Figure 11.10 present two states of the GUI. The first one is when the
robot control section is shown, and the second when the entire structure of the GUI is
presented to the user to use the intelligent interfaces for computing the robot control parameters.
Figure 11.9 GUI required for controlling the virtual simulation
Figure 11.10 GUI used to compute CIF parameters using intelligent interfaces
The robot control buttons presented in Figure 11.9 control the CIF process in 3 stages.
The first stage is the Start Simulation. This sends to the robot the Ready command which
it interprets as a start button. At this point it will wait for the metallic profile to be inserted
into the machine to grab it and start the CIF process. The second button which is defined
as a second step is the Sim Top Contact button. This button simulates the metallic profile
presence within the top gripper component. Because we have designed the robot with a
presence sensor, we’ve had to add it into the virtual environment as well. After completing
Advanced Intelligent Robot Control Interfaces 145
the second step by pressing the required button, the robot starts the CIF process and slides
the metallic profile through the inducer. At the end, after the profile was extracted from the
structure, the user can press the third button as the last step of the simulation which is the
Return Extract button. On this action the metallic profile will reset its position at start and
the whole process can be started again from step 1.
As presented in Figure 11.8, the Interface class has several methods that are called when
a certain button is pressed from GUI, and are named suggestively.
Figure 11.11 GUI Class for navigating through the virtual environment
One important component during the simulation is the camera controller. This component will allow the user to navigate through the entire virtual environment, and observe the
simulation from every angle he sees fit.
Figure 11.12 Pan (a), Zoom (b), and Rotation (c) navigation with mouse
The designed class for this feature is presented in Figures 11.11. As one can see, it has
only one method called Update, in which all the inputs are tracked and has another five
fields as variables. These variables are enough to move (pan), rotate and zoom in/out the
camera within the virtual environment, achieving the navigation component of the virtual
simulation.
Figures 11.12 present how the navigation controller can be used within the virtual environment by using the mouse buttons and movement. The navigation controller was designed to be used entirely with a mouse as follows. To pan inside the environment the
user has to press the scroll wheel button and move the mouse. To zoom inside the virtual
environment and better see the robot, the user has to use the mouse scroll wheel. To rotate
around the robot, the user has to press the right mouse button and move the mouse until
146 Emerging Technologies for Health and Medicine
the desired angle is achieved. With these 3 operations the navigation inside the virtual
environment was fulfilled.
While the GUI and navigation components are needed for the human interaction with
the virtual environment, we still need some components to link everything together. These
components are SimManager component and Ro CIF VIP component. They are presented
in Figures 11.13 and Figures 11.15 as class diagrams.
Figure 11.13 SimManager class diagram required for accessing the simulated components from
within the app
The SimManager class is the backbone of the entire robot simulation. This is the place
that links all the components, and with its help, any other component can get a reference to
another, to use its data and properties. In this way, we can use the sensors attached on each
component, to give automatic commands to the joints. By being the central component of
the simulation, it has to be very well structured and clean. This is why there are no other
components other that the initialization of each joint and the reference to each section of
the mechanical system Ro CIF VIP.
Figures 11.13 also shows through arrows how the prismatic joint components inherit
their data type from Gripper and Slider class. This means that every parameter and method
developed inside the slider class is automatically present in the Translation Joint components and variables.
Figures 11.14 presents the structure and diagram of Slider class, which plays an important role in using the translation joints, because it facilitates setting new references and
getting the joint values when called through the SimManager class instance.
The joint parameter of Slider class connects the class instance to a Translation Joint
class instance, presented in figure 2. By doing this we can assign a certain instance of the
prismatic joint to configure and monitor.
The triggerPoints and speciaTriggerPoints defined in the Slider class will provide a
simulation for special sensor points in which the prismatic joints will have presence sensors
that can trigger an action. These two parameters hold the list of such points defined in the
operational space of each joint.
While SimManager is the central point of the simulation, Ro CIF VIP class is the one to
link every component found in SimManager to the actual object within the simulation. In
Advanced Intelligent Robot Control Interfaces 147
Figure 11.14 Slider class diagram inherited by each Translation Joint class
Unity, we can say there are two types of components or classes. The first type is the class
which inherits the MonoBehaviour class and can be present inside the simulation, having
parameters, like the TranslationJoint class. The second type is the type that does not inherit
the MonoBehaviour class. The second type class must have appropriate variables and must
be instantiated by a class that inherits MonoBehaviour directly or by a different class which
at some point was initialized by one.
Figure 11.15 Ro CIF VIP class component required to link the simulated objects with the manager
variables
148 Emerging Technologies for Health and Medicine
Ro CIF VIP class is of the first type, and has a reference for each prismatic joint, plus
the profile which is being moved. These references are assigned at design time, but can be
changed during the simulation if we can find the reference to the objects required by the
simulation.
To analyze the experimental data, we have built our own system to save the joint values.
The central point of this is the SaveToXML class, presented in figure 20 as a class diagram.
This class is responsible for recording the reference and real values of the monitored joint.
After recording the joint data, the user has the possibility to save it into XML files. The
recorded data will be saved into 3 fields: time since simulation start, joint reference value
at that particular time and the real joint value. To save the simulation data, the user has to
enable a trigger within the SaveToXML class, called SaveData. This is the same for saving
the recorded data to a file by using the SaveToFile trigger.
Figure 11.16 Save to XML experimental data, class diagram
Before recording any data, the user has to select the monitored joint. The joint reference
will be stored in the jointTranslate field which is of TranslationJoint class type. At this
point, to use the data export module, the user has to use the Unity3D project, as the GUI
for the save commands is not yet implemented.
11.4 Results
After the virtual environment and simulation were completed we have tested the simulated
CIF process. For this we have used constant reference values to send the prismatic joints
to their new targets using constant speeds. To analyze the simulation results we have saved
the joint data into XML files and then built graphs to better illustrate the joint motion
according to the received reference values.
Figures 11.17 present the experimental data for the prismatic joints used for sliding the
metallic profile through the inducer. As one can see, the reference starts at 0 values which
is the starting position for the joints and converts to values 1 representing 100% of the
translation motion. After the reference value is changed, the joint starts to move towards
the end position in a constant speed, and lowers it near the end point so that it will not hit
the limiter.
Advanced Intelligent Robot Control Interfaces 149
Figure 11.17 Saved data for Top Sliding (a) and Bottom Sliding (b) prismatic joint
Figures 11.18 present the experimental data for the two gripping prismatic joints, Top
Gripper and Bottom Gripper. For these two joints the reference values has two moments
when it changes. At first, the gripper will open, but each joint was built differently and the
top gripper opens for reference of 0 and the bottom one opens for reference of 1. What we
can see is that the time duration for closing and opening the grippers is around 1.5 seconds
for the whole length of the joint. This means that the gripper will act faster when it will
actually grip a metallic profile, since the distance for closing will be shorter.
Figure 11.18 Saved data for Top Gripper (a) and Bottom Gripper (b) prismatic joint
Figure 11.19 present the saved experimental data for the extractor joint. This joint will
extract the metallic profile from the robotic structure. Figure 11.19 shows the extraction
and homing motion of this prismatic joint. Starting from the second 64, the extractor
takes the metallic profile out of the structure and at second 70 the user presses the reset
command which returns the extractor using a homing motion. We can see that for homing,
the moving speed is doubled and that the joint velocity during both motions is constant up
until the target position when it will slow down to not hit the limiters.
150 Emerging Technologies for Health and Medicine
Figure 11.19 Saved data for Extractor prismatic joint
11.5 Conclusion
In this paper we have used the 3D model of a 5DOF mechanical structure (Ro CIF VIP)
designed for induction hardening of metallic profiles, and simulated it using Unity3D. To
achieve the simulation, we have had to build our own software components, at first to
simulate a prismatic joint and after that to use it inside the virtual simulation. To use the
virtual environment, we have built a user interface and a navigation component so that the
simulation can be controlled by any user. But, to use all of these components, we needed
a central point from where all of them could be called. This is why we have designed a
manager, to link every part of the simulation, and provide software reference to each initialized component. By using Unity3D software, we have created a virtual simulation of
the proposed mechanical structure with which we can test and simulate the induction hardening process, to detect structural anomalies and test future intelligent control methods.
Following the conducted simulations, we have concluded that the built virtual simulation
achieved its goal of testing the 5DOF robot, Ro CIF VIP, and its PID control law. Using
this virtual simulation, we will be able to add more advanced intelligent control interfaces
to test them before building the actual robot.
Acknowledgments
This work was developed with the support of Romanian MCI and MFE by Competitiveness Operational Programme (COP) / ”Pro-gramul Operaional Competitivitate” (POC),
TOP MetEco AMBI-ENT project, ID P 39 383, contract 107/09.09.2016 and the European Commission Marie Skodowska-Curie SMOOTH project (H2020-MSCA-RISE-2016-
734875).
REFERENCES
1. Chauhan S., Verma V., Prakash U., Tewari P. C., & Khanduja D. Studies on induction hardening of powder-metallurgy-processed FeCr/Mo alloys. International Journal of Minerals, Metallurgy, and Materials, 2017, 24(8), 918-925.
2. Li Z. C., & Ferguson B. L. Induction Hardening Process With Preheat to Eliminate
Cracking and Improve Quality of a Large Part With Various Wall Thickness. In ASME
Advanced Intelligent Robot Control Interfaces 151
2017 12th International Manufacturing Science and Engineering Conference collocated
with the JSME/ASME 2017 6th International Conference on Materials and Processing (pp.
V001T02A026-V001T02A026). (2017, June). American Society of Mechanical Engineers.
3. Habschied M., Dietrich S., Heussen D., & Schulze V. Performance and Properties of an Additive Manufactured Coil for Inductive Heat Treatment in the MHz Range. HTM Journal of Heat
Treatment and Materials, (2016) 71(5), 212-217.
4. Dede E. J., Jordn J., & Esteve V. The practical use of SiC devices in high power, high frequency inverters for industrial induction heating applications. In Power Electronics Conference
(SPEC), 2016, pp. 1-5. IEEE.
5. Phadungthin R., & Haema J., High frequency induction heating of full bridge resonant inverter
application. In Power Electronics, Electrical Drives, Automation and Motion (SPEEDAM),
2016 (pp. 1383-1387). IEEE.
6. Porpandiselvi S., & Vishwanathan N., Three-leg inverter configuration for simultaneous dualfrequency induction hardening with independent control. IET Power Electronics, 2015, 8(9),
1571-1582.
7. Neumeyer J., Groth C., Wibbeler J., & Hanke M. FE-Simulation of Induction Hardening of a
Calender Roll. HTM-Journal Of Heat Treatment And Materials, 2016, 71(1), 43-50.
8. Mahayudin M. H., & Mat R. C. Online 3D terrain visualisation using Unity 3D game engine:
A comparison of different contour intervals terrain data draped with UAV images. In IOP
Conference Series: Earth and Environmental Science, 2016, (Vol. 37, No. 1, p. 012002). IOP
Publishing.
9. Ruzinoor C.M., Shariff A.R.M, Zulkifli A.N, Mohd Rahim M.S, Mahayudin M.H. Web Based
3D Terrain Visualisation Using Game Engine. 5th International Conference on Computing and
Informatics (ICOCI 2015); 2015, Istanbul, Turkey.
10. Ruzinoor C.M., Zulkifli A.N., Nordin N., Mohd Yusof S.A. A Review on Technique in Managing Oil Palm Plantation towards a Digitilized Online 3D Application. Information Management
and Business Review; 2013, 5(11):547-52.
11. Ruzinoor C.M., Zulkifli A.N., Nordin N., Mohd Yusof S.A. Online 3D Oil Palm Plantation
Management Based on Game Engine A Conceptual Idea. Jurnal Teknologi; 2016, 78(2-2).
12. Shin I-S, Beirami M., Cho S-J, Yu Y-H. Development of 3D Terrain Visualisation for Navigation Simulation using a Unity 3D Development Tool. Journal of the Korean Society of Marine
Engineering; 2015, 39(5):570-6.
13. Chen K. C., Wang C. S., Shih H. Y., & Hsu K. S. Development and Application of the Unity
3D Vehicle Test. In Journal of Internet Technology, 2015, 16(5), 841-846.
14. Meng W., Hu Y., Lin J., Lin F., & Teo R. ROS+ unity: An efficient high-fidelity 3D multiUAV navigation and control simulator in GPS-denied environments. In Industrial Electronics
Society, IECON 2015 - 41st Annual Conference of the IEEE (pp. 002562-002567).
15. Bartneck, C., Soucy, M., Fleuret, K., & Sandoval, E. B. (2015, August). The robot engineMaking the unity 3D game engine work for HRI. In Robot and Human Interactive Communication
(RO-MAN), 2015 24th IEEE International Symposium on (pp. 431-437). IEEE.
16. Li, A., Zheng, X., & Wang, W. (2015, July). Motion Simulation of Hydraulic Support Based
on Unity 3D. In First International Conference on Information Sciences, Machinery, Materials
and Energy. Atlantis Press.
17. Vladareanu L., Iliescu M., Bruja A., Vladareanu V., Gal I.A., Melinte O., Mititelu E., Margean
A., Competitiveness Operational Programme (COP), European Project 2014-2020, Priority 1,
Action 1.2.1. D, Ecological and Sustainable Metal Buildings through Efficient Manufacturing
Technologies, TOP MetEco AMBIENT project, ID P 39 383 / My SMIS 105188.
152 Emerging Technologies for Health and Medicine
18. Vladareanu, L., Velea, L. M., Munteanu, R. I., Curaj, A., Cononovici, S., Sireteanu, T., ... &
Munteanu, M. S. (2009). Real time control method and device for robot in virtual projection.
patent no. EPO-09464001, 18, 2009.
19. Vladareanu, V., Dumitrache, I., Vladareanu, L., Sacala, I. S., Tont, G., & Moisescu, M. A.
(2015). Versatile intelligent portable robot control platform based on cyber physical systems
principles. Stud. Informat. Control, 24(4), 409-418.
153
Dac-Nhuong Le et al. (eds.), Emerging Technologies for Health and Medicine, (153–284) 
© 2018 Scrivener Publishing LLC
CHAPTER 12
ANALYSIS OF TELEMEDICINE
TECHNOLOGIES
Vikram Puri1, Jolanda G Tromp1, Noell C.L. Leroy2, Chung Le Van1,
Nhu Gia Nguyen1,
1 Duy Tan University, Da Nang, Vietnam
2 State university of Newyork, Oswego, USA
Emails: purivikram@duytan.edu.vn, jolanda.tromp@duytan.edu.vn, nleroy@oswego.edu,
levanchung@duytan.edu.vn, nguyengianhu@duytan.edu.vn
Abstract
Telemedicine, the use of novel combinations of Telecommunication technology and Information technology to provide clinical healthcare from a distance, is a promising new
technology to overcome distance barriers and to improve access to medical services to
geographically distant rural communities. Considering that more than half of the global
rural population is excluded from healthcare, telemedicine applications offer a beneficial
approach to improve the opportunity for timely healthcare, reduce the chances of disease
developing further, and reduce the cost of consultations, thereby making healthcare more
accessible for all. With the spreading adoption of telemedicine technology solutions, better
outcomes for health and recovery from sickness can be expected. Telemedicine is therefore
ultimately beneficial for rural or unreachable places where distance is still a barrier for access to the Health Services. Telemedicine technologies can be categorized based on usage.
This chapter describes the latest research and current market developments regarding these
technologies. Additionally, this chapter highlights the latest, state-of-the-art research findings from the development and deployment of these technologies. This chapter provides an
in-depth analysis of Telemedicine technologies in terms of current open issues for future
research and development.
Keywords: Telemedicine, Long-distance healthcare, Rural Healthcare, Primary Healthcare
154 Emerging Technologies for Health and Medicine
12.1 Introduction
Telemedicine [1] is a way to provide the medical information and services with the help
of telecommunication technologies. Telemedicine is a bidirectional audio-video communication between healthcare service provider and a patient in their place of residence.
Telemedicine is a exchange of electronics information from one location to other location
for the treatment or improvement of the patient status according to the words of American
Telemedicine Association (ATA) [2]. Telehealth care [3] [4] facilities provides a virtual
physical presence of medical doctors to the Patient’s home for the medical treatment and
monitoring doctor to the Patient’s Home for the medical treatment and monitoring of the
patient and their vital signs. Due to two way communication, diseases can be detected by
the medical doctors through the monitoring their vital signs and providing medical assessments through via Telehealth care system. From the last few decades [5, 6], telemedicine
technologies are increasing day by day and embedded into related technologies which have
already used in the hospitals, healthcare centers and patient home. Telemedicine technology is beneficial in many forms like exchange the medical information between doctors
to doctor, doctor to patient with the live interactive audio video sessions. It also helpful
to provide the medical consultation, health treatment suggestions according to the patient
analysis report. In additions, the services origin from the telemedicine technologies provide facility to access the health education system and it also supporting self-management
services via internet connected devices.
Figure 12.1 Telemedicine Technologies
12.2 Literature Review
Pacis, Subido, Bugtai [7] have reviewed the combination of Artificial Intelligence and
telemedicine technology for the endless way of development in the field of medicine. They
have faced a lot of challenges likes device and telemedicine programs are very expensive,
maintain a stable internet connection for rural or underdeveloped areas. Security level
between the connection is low and difficult to maintain a confidential call via internet or
satellite communication. Due to some reason and condition medical electronics device will
malfunctioned which directly impact on the patient monitoring. Fun Li, Wei Li [8], Guler,
Analysis of Telemedicine Technologies 155
Ubeyli [12] have reviewed and compiled the different types of the telemedicine technologies like Smart system for the specific diseases, elderly assistant technology etc. They have
discussed the following issues to implement the telemedicine technology, real time data
acquisition, system reliability, energy conservation, interference communication, and data
privacy and security. There are number of opportunities in the R&D for better development
of the telemedicine technologies. Parmar, Mackie, Varghese, Cooper [9] have evaluated
the benefits from the telemedicine technology, hindrance to adopt telemedicine and identify the research areas for the benefits of telemedicine technology. They also evaluate the
HCV (Hepatitis C Virus) treatment with the help of telemedicine technology. According to
the author studied, telemedicine technologies are less expensive but it need more number
of systematic cost analysis. Johansson, wild [10] have reviewed about the acceptance and
treatment outcomes regarding the telemedicine in the stroke management. Author studies
18 papers regarding the tele-strokes services which helps to better care as compare to traditional method. With the use of tele-stroke services, patients and teleservices provider got
higher level of satisfaction. From these study, researcher says that more research is needed
to explore the practical aspects of telemedicine technology. Bryant, Garnham, Tedmanson,
Diamandi [11] have evaluated the research areas on the tele-psychology and tele-education
and provide suggestion regarding the implementation of Information and Communication
technologies (ICT) by social workers and also provide the mental health services. With the
use of ICT based services, it creates a new way and shape for the social work education
and to enhance motivation for ICT experts for work with rural local peoples and to develop
more ICT based services for the remote community peoples. Navarro, Sanchez, Cegarra
[13] have examined an important bridge between telemedicine technology and eknowledge of the patients through a survey is done from 252 patients of Hospital in Home Unit
(HHU).This study helps to maintain a relationship between organizational learning and
patients e-knowledge with the help of telemedicine technologies.
The studies discussed in this section illustrate that how telemedicine is beneficial for the
improvement to the medical field. In this chapter , we will examine about various enabling
technologies regarding the telemedicine technology till present.
12.3 Architecture of Telemedicine Technologies
Telemedicine applications consist of three modes:
1. Save and Forward
2. Tele-meeting
3. Video-meeting.
Save and forward, stores a patient’s medical history and diagnosis report in a file with
the help of Electronics Medical Records (EMR). The EMR software sends the report to the
medical doctor for the advice. Telemedicine helps to reduce or bypass the typical problems
of waiting-times and travel time, when taking an appointment with a medical consultant.
However, this technique is not suitable in case of health problems that require immediate
treatment.
Tele-meeting [28-30] is a consultation between the different parties discussions regarding the patient health through audio via Internet. It is also suitable as medium for a discussion between the patients and doctor about the medical treatment. Video Conferencing
[31-33] have so far been found to be most beneficial and appropriate method for a long
156 Emerging Technologies for Health and Medicine
distance medical discussions. It used both audio and video and requires high bandwidth
and the cost of equipment can be high if there is using Radio Frequency (RF) Components
for the audio and video.
Figure 12.2 Telemedicine Architecture
12.4 Enabling Technologies for Telemedicine
12.4.1 Telehealth for Congestive Heart Failure
Currently, Heart Failure is become major problem and it is direct pathway to the unplanned
emergency hospitalization. After the hospitalization from the heart failure, Persons have
become more chance to re-admit again and it also decreases the rate of survival and quality of life [14]. The cost of caring is also very high for those patients who are suffering
from the heart failure. In 2000 [15], Peoples had spent approximately 905 million euros
in the hospitals for the treatment of heart failure. Telemedicine technology is an attractive solution for the reducing the rate of the hospitalization due to the Heart Failure [16]
.With the help of telemedicine technology, we can avoid many existing implementation
problems like improper health staff service model, improper alert management, improper
audit for service, improper patient selection, lack of proper team structure and carefully
re-design the telemedicine pathway to improve these implementation problems. Before
implementing the telemedicine technology, telemedicine services are faced multiple barrier with different following themes (see Table 12.1):
1. User’s related.
2. Health and Social Care organization related
3. Technology Related
4. Evidence / Economic Related.
Analysis of Telemedicine Technologies 157
Table 12.1 Multiple Barrier on different themes
12.4.2 Telemedicine for the Veterans
On the basis of larger development, a telehealth program is introduced by the Veteran
Health Administration (VHA) called Telehealth home care. This Program is embedded
with the home telemonitoring to maintain care regarding the management of Disease cure
technologies. A Survey is done with the use of this telehealth program. 17,025 patients
are participated which have more than 2 mental or physical illness from the diabetes to
depression [20]. Patients gets more satisfied level with the use of this program. 25 percent
and 19 percent peoples gets reduces bed day care and hospital admission respectively. In
2012, a programs is designed for the remote monitoring of veterans. It is more beneficial
for annual saving of $2000 per patient [21].These programs also qualifies 36 percent of the
patients for the long term home care. In Addition, new admissions in the hospitals are also
decreased by 38 percent in comparison with the last year records.
These Programs are the good example for the Telemedicine technology is beneficial and
promising solution to maintain and reduces the effect of disease illness.
12.4.3 Tele-ICU (Intensive Care Unit)
Tele-ICU is called as a system that provides the intensive and critical care to patient via
remote monitoring technology. Philips VISICU Platform [22] is based on the Tele-ICU
which consist of three parts 1. Electronics Critical System 2. Smart alerts 3. Camera system for individual patient room. In the Tele-ICU, there is a central platform called ”eCare
Manager” which contains an electronic display which helps to monitor and analyze the
medical data like blood pressure, oxygen level of the body, heart beat rate and body temperature and it also used machine and deep learning algorithm for pick up the emergency
level signs. In the smart alert system, it is used as decision tool that helps to remotely
monitor the changes in the patient’s condition and make an appropriate advice according
to the patient’s reports. Tele-ICU [23] is as bidirectional audio-video communication to
interact with the staff of ICU in- site as well as off- site and it is more beneficial when the
patient is critically in the emergency situation.
Many Hospitals in the developed countries have implemented the tele-ICU for providing
a better cure via critical care experts. Sentara Hospital [24] was the first hospital who had
implemented the tele-ICU in the year 2000 at USA. Tele-ICU has implemented on more
than 300 hospitals in the year of 2003 to 2013. Tele-ICU technology model depends on the
various key points:
158 Emerging Technologies for Health and Medicine
1. Number of patients accesses this service.
2. Patient acuity.
3. Mutual arrangement.
There are three types of general models which includes various combinations: In the
first model, Patient is monitored without any interruption is said as ”Continuous care
model”. Second model said as ”Schedule care model”, which is used for the periodic
consultation at predefined schedule time. In the third model, ”Reactive care model” helps
for visit or arrange meeting via virtualization.
12.4.4 Helping Patients Adhere to Medication Regimes
Millions of the peoples are suffering from the long-lasting diseases but they can properly
manage with the use of prescribed drugs. The Conclude data from the experiments shows
that those patients who can use technology for the treatment saves a lot of money as compared to non-technology usage patients.
Currently, the number of technology helps to make patient health better. Although,
these technologies have a different way of working but beneficial for the patients to remind
about medicines. Pill based software Alert system is a internet connected system which
use to alert the patients regarding their medication and in case of remote caregivers, this
system have ability to send the medical information via email.In the near future, pharmaceutical medicines are enabled with RFID tags or QR codes which is directly connected
to the Cloud System. If some person can scan this information via smartphone, he/she
gets the prescription regarding the medicine and also gives the information like Date of
Manufacturing , Date of expiry and the contents of salts exist in the medicine.
Additionally, Connected Health Center, a subdivision of healthcare partner are exploited
a trail to monitor the blood pressure and remind patients the medication via Wireless Pill
Bottles.The result outcomes from this survey are 68 percent peoples have improved their
medication with the use of Internet Connected devices and platforms. These technologies
are the straightforward examples of Internet connected technologies improve the healthcare
system , improving valuable results with low cost.
12.4.5 eReferral - reduces consultation time
eReferral [25] is a service model which helps for the integration of patient primary care and
speciality care. In 2005 , first program is established at the US hospital when waiting and
appointment time was increased from 2-3 months to more than 10 months. Currently this
program has established with more than 40 speciality services. From the reference of this
program, these type of service model covers major hospitals at San Francisco, England,
Norway , Netherland , Australia. After the each implementation, usage of telemedicines
services are improved and gets better results like short waiting time , reduce the crowd of
visited patients and improved the satisfaction of the patients to meet the speciality doctor.
Now a days, this application is famous and doubled the usage as compare to the past
decades. eReferral is an electronically bidirectional message in form of documents or PDF
files which can exchange the expertise information between patient and viewer. Naseriasl
, Adham , Janati [26] have reviewed the 4306 articles in the major research platforms like
PubMed, Google Scholar, Scopus, Springer, Science Direct , SID and Iran Docs. Only
27 articles are satisfied to their findings.They find 17 e-referral system which have helps
Analysis of Telemedicine Technologies 159
to improve the quality of communication between the doctor and patients , involve and
integrate the medical and health centers , helps to reduce the wait time. Sadasivam [27]
has integrating the e-referral system into the 137 regular clinical practical services which
includes learn lesson to quit the smoking,implementation cost. With the help of this implementation, 86% of medical and 25.3% of dental practices have used the ereferral system.
The result from these survey have showed that e-referral system with telemedicine technologies are beneficial to reduce the fee cost of patients, waiting time, and most important
thing satisfaction of the patient regarding their visit.
12.5 Conclusion
Presently, medical care are insufficient to fulfill the demand of health care providers and
mismatch communication between the expertise doctors and remotely health care providers.
Telemedicine technology provides a asynchronous bidirectional communication which can
create connected health care platform model between expertise doctors, service providers
and patients. This platform helps to improve the quality of medical services and to reduce
the expensive of medical care. It helps to involve or participate more patients and directly
concern with the expertise doctor.
In this chapter, we analyzed the research publications regarding the telemedicine and
telemedicine enabling technologies that are used by the medical healthcare providers and
presented the finding from the different telemedicine technologies based on the different
medical cares like Heart Failure, Tele-ICU. To prov ide a deeper insight, this chapter discussed a wide overview of telemedicine technologies. This overview does not provide
in depth analysis regarding the telemedicine technologies challenges like Security, C onnected internet to remote areas. Future research will need to overcome these challenges.
REFERENCES
1. Norris, A. C., & Norris, A. C. (2002). Essentials of telemedicine and telecare (p. 106). Chichester: Wiley.
2. American Telemedicine Association. (2013). What is telemedicine. Retrieved from
http://www. americantelemed. org/learn. (Accessed on 22 May 2018).
3. Kvedar, J., Coye, M. J., & Everett, W. (2014). Connected health: a review of technologies
and strategies to improve patient care with telemedicine and telehealth. Health Affairs, 33(2),
194-199..
4. Zhang, X. M., & Xu, C. (2012). A multimedia telemedicine system in internet of things. Proceedings of the Computer Science & Information Technology, 42, 180-187.
5. Lu, D., & Liu, T. (2011, December). The application of IOT in medical system. In IT in
Medicine and Education (ITME), 2011 International Symposium on (Vol. 1, pp. 272-275).
IEEE.
6. Al-Majeed, S. S., Al-Mejibli, I. S., & Karam, J. (2015, May). Home telehealth by internet
of things (IoT). In Electrical and computer engineering (CCECE), 2015 IEEE 28th Canadian
conference on (pp. 609-613). IEEE.
7. Pacis, D. M. M., Subido Jr, E. D., & Bugtai, N. T. (2018, February). Trends in telemedicine
utilizing artificial intelligence. In AIP Conference Proceedings (Vol. 1933, No. 1, p. 040009).
AIP Publishing.
160 Emerging Technologies for Health and Medicine
8. Li, K. F., & Li, W. (2011, October). A Survey on Home Telemedicine. In Broadband and Wireless Computing, Communication and Applications (BWCCA), 2011 International Conference
on (pp. 472-477). IEEE.
9. Parmar, P., Mackie, D., Varghese, S., & Cooper, C. (2014). Use of telemedicine technologies
in the management of infectious diseases: a review. Clinical Infectious Diseases, 60(7), 1084-
1094.
10. Johansson, T., & Wild, C. (2010). Telemedicine in acute stroke management: systematic review. International journal of technology assessment in health care, 26(2), 149-155.
11. Bryant, L., Garnham, B., Tedmanson, D., & Diamandi, S. (2018). Tele-social work and mental
health in rural and remote communities in Australia. International Social Work, 61(1), 143-
155.
12. Gler, N. F., & beyli, E. D. (2002). Theory and applications of telemedicine. Journal of Medical
Systems, 26(3), 199-220.
13. Cegarra-Navarro, J. G., Snchez, A. L. G., & Cegarra, J. L. M. (2012). Creating patient eknowledge for patients through telemedicine technologies. Knowledge Management Research
& Practice, 10(2), 153-163.
14. McCartney, M. (2012). Show us the evidence for telehealth. BMJ: British Medical Journal
(Online), 344.
15. Stewart, S., Jenkins, A., Buchan, S., McGuire, A., Capewell, S., & McMurray, J. J. (2002).
The current cost of heart failure to the National Health Service in the UK. European journal of
heart failure, 4(3), 361-371.
16. Bower, P., Cartwright, M., Hirani, S. P., Barlow, J., Hendy, J., Knapp, M., ... & Steventon,
A. (2011). A comprehensive evaluation of the impact of telemonitoring in patients with longterm conditions and social care needs: protocol for the whole systems demonstrator cluster
randomised trial. BMC health services research, 11(1), 184.
17. Crundall-Goode, A., & Goode, K. M. (2014). Using telehealth for heart failure: Barriers,
pitfalls and nursing service models. British Journal of Cardiac Nursing, 9(8), 396-406.
18. de Vries, A. E., van der Wal, M. H., Bedijn, W., de Jong, R. M., van Dijk, R. B., Hillege,
H. L., & Jaarsma, T. (2012). Follow-up and treatment of an unstable patient with heart failure
using telemonitoring and a computerised disease management system: A case report. European
Journal of Cardiovascular Nursing, 11(4), 432-438.
19. Domingo, M., Lupn, J., Gonzlez, B., Crespo, E., Lpez, R., Ramos, A., ... & Bayes-Genis, A.
(2012). Evaluation of a telemedicine system for heart failure patients: feasibility, acceptance
rate, satisfaction and changes in patient behavior: results from the CARME (CAtalan Remote
Management Evaluation) study. European Journal of Cardiovascular Nursing, 11(4), 410-418.
20. Darkins, A., Ryan, P., Kobb, R., Foster, L., Edmonson, E., Wakefield, B., & Lancaster, A. E.
(2008). Care Coordination/Home Telehealth: the systematic implementation of health informatics, home telehealth, and disease management to support the care of veteran patients with
chronic conditions. Telemedicine and e-Health, 14(10), 1118-1126.
21. Darkins, A. (2013). Scaling-up telemedicine: VA telehealth programs. Presentation at the
American Telemedicine Association Policy Summit, Washington, DC, 27-28.
22. http://www.himss.org/reducing-icu-length-stay-effect-tele-icu (accessed on May 28 , 2018)
23. Kumar, S., Merchant, S., & Reynolds, R. (2013). Tele-ICU: efficacy and cost-effectiveness
approach of remotely managing the critical care. The open medical informatics journal, 7, 24.
24. Chen, A. H., Murphy, E. J., & Yee Jr, H. F. (2013). eReferrala new model for integrated care.
New England Journal of Medicine, 368(26), 2450-2453.
25. Naseriasl, M., Adham, D., & Janati, A. (2015). E-referral solutions: successful experiences,
key features and challenges-a systematic review. Materia socio-medica, 27(3), 195.
Analysis of Telemedicine Technologies 161
26. Sadasivam, R. S., Hogan, T. P., Volkman, J. E., Smith, B. M., Coley, H. L., Williams, J. H., ... &
Allison, J. J. (2013). Implementing point of care e-referrals in 137 clinics to increase access to
a quit smoking internet system: the Quit-Primo and National Dental PBRN HI-QUIT Studies.
Translational behavioral medicine, 3(4), 370-378.
27. Zanjal, S. V., & Talmale, G. R. (2016). Medicine reminder and monitoring system for secure
health using IOT. Procedia Computer Science, 78, 471-476.
28. Zhang, X. M., & Li, J. (2011). Research on interoperability of Internet of Things’ gateway
oriented to telehealth and telemedicine. Energy Procedia, 13, 8276-8284.
29. Yang, G., Xie, L., Mntysalo, M., Zhou, X., Pang, Z., Da Xu, L., ... & Zheng, L. R. (2014). A
health-iot platform based on the integration of intelligent packaging, unobtrusive bio-sensor,
and intelligent medicine box. IEEE transactions on industrial informatics, 10(4), 2180-2191.
30. Guilln, E., Snchez, J., & Lpez, L. R. (2017). IoT Protocol Model on Healthcare Monitoring.
In VII Latin American Congress on Biomedical Engineering CLAIB 2016, Bucaramanga,
Santander, Colombia, October 26th-28th, 2016(pp. 193-196). Springer, Singapore.
31. Roine, R., Ohinmaa, A., & Hailey, D. (2001). Assessing telemedicine: a systematic review of
the literature. Canadian Medical Association Journal, 165(6), 765-771.
32. Krupinski, E. A. (2009). History of telemedicine: evolution, context, and transformation.
Telemedicine and e-Health, 15(8), 804-805.
33. Maheu, M., Whitten, P., & Allen, A. (2002). E-Health, Telehealth, and Telemedicine: a guide
to startup and success. John Wiley & Sons.
Part III
ROBOTICS TECHNOLOGIES
AND APPLICATIONS FOR
HEALTH AND MEDICINE
Dac-Nhuong Le et al. (eds.), Emerging Technologies for Health and Medicine, (163–284) 
© 2018 Scrivener Publishing LLC
165
Dac-Nhuong Le et al. (eds.), Emerging Technologies for Health and Medicine, (165–284) 
© 2018 Scrivener Publishing LLC
CHAPTER 13
CRITICAL POSITION USING
ENVIRONMENT MODEL APPLIED ON
WALKING ROBOTS
M. Migdalovici1, L. Vladareanu1, N. Pop1, H. Yu2,3, M. Iliescu1, V.
Vladareanu1, D. Baran4, G. Vladeanu1
1 Romanian Academy, Institute of Solid Mechanics of the Romanian Academy, Romanian
2 School of Computer Science and Network Security, Dongguan University of Technology, China
3 Faculty of Science and Technology, Bournemouth University, Poole BH125BB, UK
4 National Institute of Aerospace Research Elie Carafoli, Bucharest, Romanian
Emails: luige.vladareanu@vipro.edu.ro, migdal@imsar.bu.edu.ro, yuh@bournemouth.ac.uk, iliescumihaiela7@gmail.com, victor.vladareanu@vipro.edu.ro, nicpop@gmail.com
Abstract. The chapter presents the walking robots evolution models in correlation with
modeling of the environment in order to integration into the concept of virtual reality by
applying the virtual projection method. The environment’s mathematical model is defined
through the models of kinematics or dynamic systems for the general case of systems that
depend on parameters. In the first part of the chapter, some mathematical conditions that
imply the separation of stable regions from the free parameters domain of the system are
formulated. The property of separation between stable and unstable regions from the free
parameters domain of the system is deeply approached, being an important property of the
dynamic system evolution model that approaches the phenomenon from the environment.
In the second part an innovative method is developed on walking robot kinematics and
dynamic models with aspects exemplified on walking robot leg. The results lead to an
inverse method for identification of possible critical positions of the walking robot leg,
applied in the robot control in the virtual reality environment by the virtual projection
method.
Keywords: Environment’s model, Walking robot, Kinematics/dynamic model, Stability
regions.
166 Emerging Technologies for Health and Medicine
13.1 Introduction
The first part of the exposure is referred to mathematical modeling of the environment
where the walking robots evolution models are assumed.
The models of kinematics or dynamic system in the general case of systems that depend
on parameters assure, by its properties, the mathematical characterization of the environment.
Any system is expressed in terms of relevant parameters as geometrical parameters,
physical parameters (in particular mechanical parameters), possible chemical, biological,
economical, etc, [1-8, 13-26].
The important property of the dynamic system evolution models that approach the phenomenon from the environment is property of separation between stable and unstable regions from the free parameters domain of the system. This property is proposed that define
the environment’s mathematical model. The mathematical conditions on the linear dynamic system matrix components that assure the separation between stable and unstable
regions from the free parameters domain of the system are formulated.
In the second part of the exposure is described our walking robot evolution kinematics
model and corresponding dynamic model with application on particular case of walking
robot leg.
An inverse method for identification of possible critical positions of the walking robot
is established. The link between mathematical model of the dynamic system walking robot
and corresponding kinematics system mathematical model is emphasized.
The problem analyzed by kinematics walking robot model that can be analyzed as problem in dynamic walking robot model having similar results, is also underlined.
13.2 On the Environment’s Mathematical Model
The mathematical property that one can remark on all dynamic systems models from the
literature, which approaches the environment phenomena, is separation property between
stable and unstable regions on the system free parameters domain [4-7]. We have formulated, for the first time, the sufficient conditions needed for the functions that defined
the dynamic system, linear or non linear, which assure the separation between stable and
unstable regions on the free parameters domain [6, 7].
The real matrix, which defines the linear dynamic system or ”first approximation” of
the nonlinear dynamic system in general case that depends on parameters, is denoted by
A and assumed as matrix from Rn×n, n ∈ N. Below is discussed on QA algorithm for
Hessenberg form of the real matrix A.
Let matrix
A ∈ Rn×n
be with real elements aij , i = 1..n, j = 1..n. The matrix A is considered for beginning
that has the distinct eigenvalues, real or complex. The matrix A is in Hessenberg form if
their elements
aij = 0
for
2 < i ≤ n, j < i − 1
. Any real matrix A can be substituted by a similar matrix in Hessenberg form, because
they have the same eigenvalues, which facilitates studies of the stability.
Critical Position using Environment Model Applied 167
The QR algorithm is formulated in hypothesis of matrix A in Hessenberg form to assure
that the complex eigenvalues α±iβ, if there exists, to appear in real final Schur form of the
matrix A, calling the real matrix of two order 
αβ
−βα
situated for each distinct complex
conjugate eigenvalues on the diagonal of the Schur form of the matrix and for each distinct
real eigenvalue identified also on the diagonal of the Schur form of the matrix A, similar
with initial matrix. The similar Schur form of the matrix A is justified in [7, 11, 12].
The matrices Qk, k = 1, 2.. are orthogonal and the matrices Rk are invertible and upper
triangular. The matrices Ak, Ak+1, k = 1, 2... are similar and in Hessenberg form.
The QR algorithm convergence of matrix A to Schur form of the matrix, where real
matrix A is in Hessenberg form, is analyzed by Parlet [11].
The matrix A − λI, where the value λ is real or complex, is a matrix in Hessenberg
form if the matrix A is in Hessenberg form. The value λ is named ”the shift of origin” for
the matrix. The shift of origin assures the transposition of the matrix with real components
that describe the dynamic system in complex domain using suitable complex value λ.
The QR algorithm applied to matrix A using the shift of origin is defined by the relations
[11]:
Qs(As − ksI) = Rs, As+1 =
= RsQT
s + ksI = QsAsQT
s , s = 1, 2, ... (13.1)
The initial matrix A of the system is denoted in the QR algorithm by A1 and assumed
in Hessenberg form, ks is ”shift of origin”, Qs is orthogonal matrix, As, s ≥ 2 is in
Hessenberg form, and matrix Rs is in upper triangular form.
The shift of origin, using initial value λ sufficient close to one matrix eigenvalue, real or
complex, assures acceleration of the convergence in algorithm to the similar diagonal form
of the matrix. This is also an important motivation to use algorithm by shift of origin.
The matrix A with distinct eigenvalues is similar with the corresponding matrix in Hessenberg form and algorithm through shift of origin facilitates the convergence of the initial
matrix to similar diagonal form of the matrix.
The above analysis is established in hypothesis that all eigenvalues of the real matrix
are distinct. For extension of the analysis in the case of real matrix multiple eigenvalues,
calling to the results from matrix theory reminded below.
Hirsch, Smale and Devaney have verified on the linear normed space L(Rn) of matrices
that the set of matrices with distinct eigenvalues from space L(Rn) is an open and dense
set in this space [1].
The above quality creates the possibilities to motivate transmission of some properties,
which can arise from the stability analysis on linear or on ”first approximation” dynamic
systems, from the real matrices set with distinct eigenvalues to the real matrices set that
include multiple eigenvalues.
Some Liapunov theorems on linear or nonlinear local stability are reminded below.
Theorem 1. Let the linear dynamic system be defined by the differential equation of the
form dy
dt = Ay(t), y(t)=(y1(t), ..., yn(t))T , A = (aij ), i = 1..n, j = 1..n, the symbol T
signifying transposition of the matrix and where the values aij are assumed constants. If
the real part of all eigenvalues of the matrix A is strictly negative then the solution of the
differential equation is asymptotic stable in origin. If the real part at least one eigenvalue
of the matrix A is strictly positive then the solution of the differential equation is unstable
in origin. If the real part of the eigenvalues of the matrix A is strictly negative with the
exception of at least one eigenvalue that has null real part then the stability of the dynamic
system in origin is unknown (possible stable or unstable).
168 Emerging Technologies for Health and Medicine
The function f(x) is assumed dependent of variable x = (x1, ..., xn)T in the following,
having value of function in the form . The functions f(x)=(f1(x), ..., fn(x))T are
considered that can be developed in series as below:
fi(x) = fi(0) + n
j=1 (∂fi(x)/∂xj )
	
	
	
x=0
xj
+n
j=1
n
k=1 

∂2fi(x)

∂xj∂xk
	
	
	
x=0
xjxk + ..., i = 1, ..., n
(13.2)
Without loss of the generality can consider fi(0) = 0, i = 1, ..n and using the notations
aij = ∂fi(x)/∂xj |
x=0, i, j = 1, ..n for the first order derivatives we can formulate the
differential equation:
dx
dt = [aij ] x + g(x);i, j = 1, ..., n (13.3)
The linear system of ”first approximation” deduced from (13.3) is of the form:
dx
dt = [aij ] x;i, j = 1, ..., n (13.4)
The following Liapunov theorems are also mentioned:
Theorem 2. The evolution of non linear dynamic system (13.3) is asymptotic stable
in origin if the real part of all eigenvalues of the matrix A = [aij ], i, j = 1..n is strictly
negative.
Theorem 3. The evolution of the non linear dynamic system (13.3) is unstable in origin
if the real part of at least one eigenvalue of the matrix A = [aij ], i, j = 1..n is strictly
positive.
The important result performed by Halanay and Rsvan on nonlinear dynamic system is
reminded below [2].
Theorem 4. Let the dynamic system be defined by the equation dx
dt = Ax + g(x). The
real matrix A, of dimension n×n, is assumed that is compounded from constant elements,
the variable is x = (x1, ..., xn)T of dimension n, the function x(t) ≡ 0 is a solution of
the equation, the function g(x) is assumed continuous and with the property that for each
γ > 0 there is δ(γ) > 0such that if |x| < δ(γ) then |g(x)| < γ |x|. It is also assumed that
the matrix A has the property that all roots λi, i = 1, .., n of the characteristic polynomial
have the real part strictly negative such that Realλi ≤ −2α < 0,,i = 1, ..n.
Then there is δ0 > 0, β ≥ 1 such that for each is true the inequality:
|x(t;t0, x0)| ≤ βe−α(t−t0)/2 |x0| , t ≥ t0 (13.5)
Remark: If the function g(x) and the matrix A that intervene in theorem 4 have the imposed properties then we observe that stability in origin implies stability in neighborhood
of origin and thus implies stable region separation in origin neighborhood.
The following theorem on the QR algorithm is described because can help us to verify
conditions imposed by theorem of separation exposed in the next [7].
Theorem 5. If the components of the matrix A are continuous on piecewise and the
sequence of Hessenberg form matrices As, s = 1, 2, ... from algorithm that started with
the matrix is uniform convergent to the Schur form of the matrix A then the eigenvalues of
the matrix A are continuous on piecewise.
The extended conditions on the matrix of functions that define the autonomous linear
dynamic system or ”first approximation” of nonlinear dynamic system that allow the separation of the stable regions on the parameters domain are mentioned below in our theorem
Critical Position using Environment Model Applied 169
on separation. Some results performed in matrix theory are used for deduction of this
theorem [9-12].
Theorem on separation: If the autonomous linear dynamic system, defined by the
matrix , has the continuous on piecewise components of the matrix as functions on dynamic
system free parameters and is assured that the eigenvalue functions of the matrix are also
continuous on piecewise, then these conditions allow the separation between stable and
unstable regions of the dynamic system in the parameters domain.
Remark: For the kinematics models of systems that depend on parameters, the property
of separation is formulated on existence and in-existence regions in the domain on free
parameters. This case is analyzed through our application on walking robot leg model.
13.3 Physical and Mathematical Models of The Walking Robot Leg
In the following we firstly describe our physical and mathematical model on the walking
robot leg kinematics, with physical model of the robot leg defined by a ”pivot point” attached to the robot body and two components BtP and PQ jointed in point P denoted
”knee joint” of the leg and the point Q, denoted ”base point” of the leg, which are moved
in vertical plane, as shown in Figure 13.1.
Figure 13.1 Physical model of the walking robot leg; Bt and OE are center of circle respectively
ellipse arc trajectories; P1, PA, P, PB are knee joint positions in leg cyclic evolution; Q1, QA, Q, QB
are base point positions in leg cyclic evolution; P∗
A,P∗
B, Q∗
A, Q∗
B are critical positions
The point P, in the case of fixed pivot point, describes a circle arc route in a cycled
evolution of the robot leg and the base point Q is assumed that describes a close route
compounded from the superior ellipse arc QBQA with semi axes length a, b and with
point OE center of the ellipse, and closure of the leg evolution cycle by horizontal segment
QBQA traversed by the base point Q.
The orthogonal system of coordinates and parameters signification are identified from
the coordinates on the figure points: Bt(a, h), OE(a, b1), P(xP ,yP),Q(xQ,yQ),, QA(xA, 0),
QB(xB, 0).
170 Emerging Technologies for Health and Medicine
The points P∗
A and P∗
B define the extremities of the maximal domain on circle arc where
the knee joint P is moving because in these points, geometrical identified by the property
that the segments P∗
AQ∗
A and P∗
BQ∗
B are normally on the ellipse arc, the direction of movement is changed. The positions P∗
A and P∗
B of the knee joint, identified by us on the
particular case of Figure 13.1, are named by us the critical points from the leg evolution.
In other cases of robot leg with fixed pivot point, defined by the values of the geometrical
parameters or in cases where the pivot point is moved in the walking robot evolution, is
important to search the possible existence of the knee joint critical positions where the direction of movement is changed and where the speed of the knee joint must to be zero for
the continuous evolution of the knee joint [8].
The mathematical model deduced from the physical model, suggested by the particular
case represented in Figure 13.1, is defined through two formulas described by the equations:
(x − a)2 + (yP − h)2 − a2 = 0.;
(x − a)2/a2 + (yQ − b1)2/b2 = 1. (13.6)
Between the parameters’ values, are assumed conditions a>b>b1 > 0; 2a>h
where a and b are semiaxes of the ellipse.
Explicit functions generated by (12.6), are:
yP = h ∓ (2ax − x2)1/2;
yQ = ±b/a(2ax − x2)1/2 + b1
(13.7)
Let P(xP , yP ) and Q(xQ, yQ) be points on the circle arc respectively on the ellipse arc
for one leg position from the evolution.
The condition on the distance PQ for a position of the walking robot leg, namely the
relation (xP − xQ)2 + (yP − yQ)2 − a2 = 0, is imposed.
The uniform linear evolution of the variable x between 0 and 2a, excepting a neighborhood around possible critical points, in the case of fixed pivot point of the leg, is assumed as
below, where the constant speed v0 and initial condition x0 are selected using compatible
values:
x(t) = v0t + x0 (13.8)
One cycle for the robot leg evolution can start from the position of base point QB,
traversing the superior ellipse arc up to the point QA, using the evolution law (13.8), and
returns by the linear uniform displacement on the horizontal axle, in the point QB, excepting a selected neighborhood around each position Q∗
A, Q∗
B, where is defined a proper
evolution.
The mathematical model of the walking robot leg, near of the critical point P∗
A, which
have abscise denoted xP A∗ , using physical model from the Figure 13.1 and abscise xP
convergent to abscise xP A∗ , imposed by abscise xQ evolution assumed decreasing convergent to abscise xQA∗ with xQ ∈ (xQA∗ , 2a
3 ), is described through below formulas:
yP = h + (2axP − x2
P)1/2;
yQ = b/a(2axQ − x2
Q)1/2 + b1;
xQ(t) = −v0t + 2a/3
(xP − xQ)2 + (yP − yQ)2 − a2 = 0
(13.9)
The value of abscise x(t), in our hypothesis, respect the condition xP A∗ < x(t) that
implies xP A∗ < −v0t+ 2a/3 such that v0 < 2a/3−xP A∗ . The corresponding expression
Critical Position using Environment Model Applied 171
of yP (t) is as follows:
yP(t) = h + (2axP(t) − (xP(t))2)
1/2 (13.10)
The value of time parameter, in assumption that xQ(t) is decreasing convergent to abscise xQA∗ , for which is verified the equation d(yp(t))
dt = 0, using admissible value of
parameter v0, is of interest for us because identifies critical position, if it exists, of knee
joint P.
We remark that the domain of the parameters’ values x, yP, yQ, h, v0, x0, t with fixed
values of positive parameters a, b, b1, in this analyzed case, for which described evolution
exists, is an interval for one free parameter. The domain of existence coincides, in these
formulated cases, with the domain of stability, such that we can affirm that there is a separation between stable (existence) and unstable (inexistence) regions of the free parameters
values of the robot leg kinematics model.
The intuitive analysis permits to conclude that the separation is true and for robot leg
with uniform distributed mass on the leg in the dynamic model. The following judgment
on kinematics and dynamic models of walking robot leg, describe their link. The dynamic
model is assumed that consists from two pipes of mass decreasing convergent to zero. The
contribution of pipe mass for leg dynamic model is negligible in the case of mass sufficient
close of zero. One can affirm that our kinematics model of leg is a limit case of dynamic
model. Evolution of the leg dynamic model can be described using supplementary our
study on Bernoulli Euler beam model with some analytical results described in the next
[8]. The problem that arises in kinematics walking robot model can be transferred as
problem in dynamic walking robot model with similar solution. The problem of critical
positions, analyzed in our two dimensional case of walking robot leg model, having some
specific problems in three dimensional case analyzed in follows capitol, is an example.
13.4 On Critical Positions of 3D Walking Robots
Figure 13.2 Physical model of three dimensional walking robot leg
172 Emerging Technologies for Health and Medicine
A three dimensional (3D) leg evolution physical model for one leg from multi-legged
walking robot is described in Figure 13.2 using three orthogonal coordinate system denoted
xyz.
In the critical point is necessary to be assured zero value of the leg base point speed
to respect natural continuous evolution of the leg. We remark that the property of walking robot stability (existence) position is also maintained in a neighborhood for one such
position.
One 3D leg model is compounded from superior component BtQt defined by the extremities points Bt, Qt jointed in pivot point Bt attached to the body of the robot and
inferior component QtPt that consist from ”knee joint” Qt and base point Pt.
For the length of components BtQt and QtPt has assumed a constant value.
The base point Pt is moving on the ellipse arc between points PI and PF , in vertical
plane, orthogonal on axis Oy, using uniform accelerated displacement on the horizontal direction up to the median point PM, on the ellipse arc, and symmetric displacement assured
up to the final point PF . The joint point Bt attached to the body of the robot is moving
together with point Pt, having linear route parallel to the axis Ox, using uniform displacement between initial point BI up to median point BM and symmetric displacement assured
between the median point BM up to final point BF .
The trajectory of the ”knee joint” point Qt, unique identified in the vertical plane, defined by the points Bt, Qt, Pt in each time t, having defined the distance between its, with
the length of segment BtPt dependent on time t, is studied for possible critical points identification, similar as in two dimensional cases described above. The following formulas,
using geometric data from Figure 13.2 and corresponding physical data are described. The
trajectory of the point Qt, in three dimensional of system coordinates is orthogonal projected on the plane xOz using the projected point Qt0 from this plane. The critical position
of the knee joint Qt, in its three-dimensional evolution, if there exists, is identified using
critical position on the plane projected trajectory of the point Qt0 where the direction of
movement must to be changed. The point Qt0 is projection of the point Qt on the side
QBtQP t, which is parallel with the axis Ox, and where the triangles QtQBtQP t, Q0B0P0
have the sides respectively parallel.
The uniform accelerated displacement of the point denoted P, on the horizontal line,
from the point PI up to middle of the segment PIPF , is described by the relation:
xP (t) = aP t
2/2 (13.11)
The parameter aP represents constant acceleration in the horizontal direction of the
point denoted P.
The uniform displacement of the pivot point Bt, on parallel line with axis Ox, from the
point BI up to middle of the segment BIBF , is described by the relation:
xBt(t) = vBt (13.12)
The parameter vB is constant speed in displacement, on parallel line with axis Ox, of
the point Bt.
The parameter L is introduced for correlating the data from the three-dimensional walking robot leg mathematical model. The following lengths are defined BtQt=2L, QtPt =
3L, BtB0 = 1.5L, PtPt0 = 1.5L.
The walking robot having six legs is assumed, so that to close the cycle of evolution,
so that the following relation arises PIPF = 6BIBF. This relation is justified by the
hypothesis on the separation and successive displacement of each leg in cycling evolution.
Critical Position using Environment Model Applied 173
The time for simultaneously arriving in the middle of the trajectories BIBF , PIPF , by
point Bt respectively Pt, is denoted tM. The following relations are true:
vBtM=d/12, vB=d/tM/12;
aP t
2
M/2 = d/2, aP = d/t2
M
(13.13)
The coordinates for the points Bt, Pt are identified in function of the time as below:
xBt = xBI + vBt, yBt = 0., zBt = 1.5L
xPt = aPt
2/2, yPt = 1.5L,
zPt = (b/a)(a2 − (xPt − d/2)2)1/2 − b1
(13.14)
In the relations described above are considered 0 ≤ t<tM and a, b that represent
ellipse semiaxes where 0 < b1 < b. Coordinate z is imposed negative for the ellipse
centres from the Figure 11.2 so that zPt ∈ [0, b − b1).
The length of the segment BtPt is evaluated from:
BtPt = ((xPt − xBt)
2 + (yPt − yBt)
2 + (zPt − zBt)
2)
1
2 (13.15)
The angles in the vertical triangle BtQtPt are evaluated, using the lengths of the triangle
sides, by the relations of type:
cos((QtBtPt)) =(BtQt)2 + (BtPt)2 − (QtPt)2
2BtQt × BtPt (13.16)
The angles values of the vertical triangle BtQtPt, from the physical model shown on
Figure 11.2, permits to evaluate all angles or sides of the physical model. The critical point
position where is changed the direction of movement referred to projected point Qt0 is
searched in the interval [0, tM) of time parameter.
The mathematical model referred to three dimensional legs evolution of multi-legged
walking robot, proposed above, permits to identify the critical position, if they exist, for
knee joint point, using specialized computer program.
13.5 Mathematical model of beam without damping
Firstly, we analyze the extended Bernoulli - Euler dynamic beam model without damping,
necessary for studying the model of beam with damping. This model is suitable for beams
with length more large than section, as in our case studied. The following equation of beam
is considered:
mL
∂2wi(x, t)
∂t2 = −kiwi(x, t) + T ∂2wi(x, t)
∂x2 − EI ∂4wi(x, t)
∂x4 + q(x, t) (13.17)
The equation (12.7) describes the behaviour of the beam, excited by the force q, applied
transversal on the beam, acting in the point of abscissa x, at the time t.
We denote by mL the mass unit length of the beam, by EI the bending rigidity of the
beam, by T the tension in the beam, by wi(x, t) corresponding vertical displacement of
the beam for vibration mode of order i, by k the rigidity of the beam as a coefficient of
beam transverse displacement, and by L the length of the beam. The case of free vibrations
(q(x, t)=0, excepting initial conditions) is needed to be studied.
174 Emerging Technologies for Health and Medicine
The stabilized free transverse vibration of the beam without damping is searched in
standing wave form:
wr(x, t) = wr(x) sin (ωrt + ϕr), r = 1, 2, ... (13.18)
In formula (12.8), the notations signify: ωr = 2πfr is the circular frequency, fr is
the resonance frequency of the beam in free vibrations without damping, and ϕr is the
phase angle between the initial impulse and displacement. Below appear the following
dimensionless notations (Nowacki, 1961):
α2 = T L2
EI , δr =


α2/2 + 

α4/4 + β4
r
1/2
1/2
,
εr =


−α2/2 + 

α4/4 + β4
r
1/2
1/2 (13.19)
In the above expressions, we use the notation β4
r = mL(ω2
r − kr)L4/EI and assume
the condition ω2
r − kr ≥ 0. For all r = 1, 2, ... in case of rigidity kr not negligible. The
equation of free vibrations for undamped beam is described by (12.6) where q(x,)=0,
excepting needed initial conditions.
The following equation is deduced from equation of free vibrations, using formula
(12.7):
(mLω2
i − ki)wi(x) = −T d2wi(x)
dx2 + EI d4wi(x)
dx4 , i = 1, 2, ... (13.20)
The solutions of equation (11.20), which respect some boundary conditions by identify
its coefficients, are searched in the form described below.
wi(x) = c1i sin (εi
x
L) + c2i cos (εi
x
L) + c3ish (δi
x
L) + c4ich (δi
x
L), i = 1, 2, ... (13.21)
The particular case analyzed here is the simple supported undamped beam. That means
simple supported extremity at both extremities of the beam.
The coefficients c1i, ..., c4i are searched so that the following analytical conditions to be
respected:
wi(0) = 0, d2wi(x)
dx2
	
	
	
x=0
= 0,
wi(L)=0, d2wi(x)
dx2
	
	
	
x=L
= 0. (13.22)
The equation of natural frequencies for undamped beam, in the particular case analyzed,
is simply described by:
sin εi = 0 (13.23)
The vibration mode of undamped beam, using the particular case of beam boundary
conditions, is as follows:
wi(x) = c1i sin (εi
x
L) (13.24)
The coefficient c1i is independent of x.
The transcendental equation (12.13), for any resonant frequency of undamped oscillated
system (identified by theoretical and experimental way) can be used to obtain the bending
rigidity EI of the beam, in specified boundary conditions.
Critical Position using Environment Model Applied 175
13.6 Mathematical Model of Beam with Viscous Damping
The equilibrium equation in this case of free vibrations is:
mL
∂2wi
∂t2 = −kiwi − cV
i
∂wi
∂t + T ∂2wi
∂x2 − EI ∂4wi
∂x4 ;i = 1, 2, ..., (13.25)
One searches, in the equation (12.15), the solution as:
wi(x, t) = Xi(x)Ti(t) (13.26)
The Xi(x), which verifies (12.10), defines a vibrating mode of the beam undamped free
vibration of order i.
The equation deduced from (12.15) for Ti(t) is as below:
d2Ti(t)
dt2 + 2cV L
i
dTi(t)
dt + ω2
i Ti(t)=0.,
cV L
i = cV
i /mL/2, i = 1, 2, ..., (13.27)
The free vibrations of the beam, having denoted values of initial conditions (wi(xo, 0) =
Doi, w˙ i(xo, 0) = 0.) are deduced as:
wi(x, t) = Xi(x) Doi
Xi(xo) e−cV L
i t
cV L
i
ωV
i
sin ωVt
i + cos ωVt
i

,i = 1, 2, ...;
cV L
i = cV
i /mL/2, 

ωV
i
2 = ω2
i − 

cV L
i
2
> 0.
(13.28)
In this case, the objective function for identification of one parameter, by selection of
other suitable parameters values, is of the type below:
f = 
i,x,t
wcof
i {wi
c
(x, t)− wi
exp(x, t)}
2 (13.29)
The theoretical value wi
c(x, t) and experimental value wi
exp(x, t) of the displacements
are used in (12.18), theoretically calculated or experimentally measured, in some points
of abscissa denoted x, for some moments of time and some frequencies, in the domain of
interest. The weights wcof
i = 1
(wiexp(x,t))2 assure the dimensionless objective function.
13.7 Conclusion
The property of separation in the free parameters domain on the dynamic systems that
approach the phenomena on the environment permits us to characterize the environment
mathematical model. This notion includes dynamic and kinematics models. The link
between dynamic and kinematics walking robot model applied on walking robot leg model
and our considerations on environment’s mathematical modeling, on stability theory of the
dynamic systems or on kinematics systems existence regions that depend on parameters is
not exhausted by our analysis but is opened a promising and attractive way of research.
Acknowledgment
This work was developed with the support by Romanian Academy, European Commission
Marie Sklodowska-Curie SMOOTH project (H2020-MSCA-RISE-2016-734875/2016-2020
176 Emerging Technologies for Health and Medicine
and ”Joint Laboratory of Intelligent Rehabilitation Robot” collaborative research agreement between Romanian Academy by IMSAR, RO and Yanshan University, CN,
Project KY201501009/2016.
REFERENCES
1. Hirsch, M. W., Smale, S., & Devaney, R. L. (2012). Differential equations, dynamical systems,
and an introduction to chaos. Academic press. https://doi.org/10.1016/c2009-0-61160-0.
2. Halanay, A., & Rasvan, V. (2012). Applications of Liapunov methods in stability (Vol. 245).
Springer Science & Business Media. http://dx.doi.org/10.1007/978-94011-1600-8
3. Teodorescu, P., Stanescu, N. D., & Pandrea, N. (2013). Numerical analysis with applications
in mechanics and engineering. John Wiley & Sons. http://dx.doi.org/10.1002/9781118614563
4. Migdalovici, M., Baran, D., & Vldeanu, G. (2014). On the Dynamical Systems Stability Control and Applications. In Applied Mechanics and Materials (Vol. 555, pp. 361-368). Trans Tech
Publications. http://dx.doi.org/10.4028/www.Scientic.net/amm.555.361
5. Migdalovici, M., Vldreanu, L., Baran, D., Vldeanu, G., & Radulescu, M. (2015). Stability Analysis of the Walking Robots Motion. Procedia computer science, 65, 233-240.
https://doi.org/10.1016/j.procs.2015.09.117
6. Migdalovici, M., Baran, D., & Vladeanu, G. (2016). Stability control of linear and nonlinear dynamic systems. International Journal of Acoustics and Vibration, 21(4), 440-445.
https://doi.org/10.20855/IJAV.2016.21.4438
7. Migdalovici M, Cononovici SB, Vldreanu L, Secrieru G, Vldreanu V, Pop N, Vldeanu A,
Baran D, Vldeanu G and Feng Y (2017). On environment mathematical model and on improved stable evolution in these hypotheses, Romanian Journal of Technical Sciences Applied
Mechanics, 62(2), pp.119-134. http://www.academiaromana.ro/RJTS-AM.htm
8. Migdalovici, M., Sireteanu, T., & Videa, E. M. (2010). Control of vibration of transmission
lines. International Journal of Acoustics and Vibration, 15(2), 65. https://doi.org/10.20855/
IJAV. 2010.15.2259
9. Francis JGF. QR transformation, The Computer Journal, 4, First part, pp: 265-
271, (1961), https://doi.org/10.1093/comjnl/4.3.265, Second part, pp: 332-345, (1962),
https://doi.org/10.1093/ comjnl/4.4.332
10. Wilkinson, J. H. (1965). Convergence of the LR, QR, and related algorithms. The Computer
Journal, 8(1), 77-84. https://doi.org /10.1093/comjnl /8.1.77
11. Parlett, B. (1968). Global convergence of the basic QR algorithm on Hessenberg matrices.
Mathematics of Computation, 22(104), 803-817. https://doi.org/10.1090/s0025-5718-1968-
0247759-4.
12. Wilkinson JH, Martin RS, and Peters G (1970). Handbook Series Linear Algebra. The
QR Algorithm for Real Hessenberg Matrices. Numerische Mathematik 14: 219-231
https://doi.org/10.1007/978-3-662-39778-7 25.
13. Vladareanu L, Tont G, Ion I, Munteanu M S, Mitroi D (2010). Walking robots dynamic control
systems on an uneven terrain, Advances in Electrical and Computer Engineering, 10(2), 146-
153. https://doi.org/10.4316/aece.2010.02026
14. Vladareanu, V., Tont, G., Vladareanu, L., & Smarandache, F. (2013). The navigation of mobile
robots in non-stationary and non-structured environments. International Journal of Advanced
Mechatronic Systems, 5(4), 232-242. https://doi.org/10.1504/ijamechs.2013.057663
Critical Position using Environment Model Applied 177
15. Pop, N., Vladareanu, L., Popescu, I. N., Ghi, C., Gal, A., Cang, S., ... & Deng, M. (2014).
A numerical dynamic behaviour model for 3D contact problems with friction. Computational
Materials Science, 94, 285-291. https://doi.org/10.1016/j.commatsci.2014.05.072
16. Vladareanu, L., Melinte, O., Bruja, A., Wang, H., Wang, X., Cang, S., ... & Xie, X. L.
(2014, July). Haptic interfaces for the rescue walking robots motion in the disaster areas.
In Control (CONTROL), 2014 UKACC International Conference on (pp. 498-503). IEEE.
https://doi.org/10.1109/CONTROL.2014.6915189
17. N Pop, L Vladareanu, IN Popescu, C Ghi, A Gal, S Cang, H Yu, V Bratu, M Deng, A numerical
dynamic behaviour model for 3D contact problems with friction, Computational Materials
Science 94, 2014, 285-291
18. Luige Vladareanu, Daniel Mitroi, Radu Ioan Munteanu, Shuang Chang, Hongnian Yu, Hongbo
Wang, Victor Vladareanu, Radu Adrian Munteanu. Hou z.g., Octavian Melinte, Radu Adrian
Munteanu , Zeng-Guang Hou , Xiaojie Wang , Guibin Bia , Yongfei Feng and Eugen Albu
(2016). Improved Performance of Haptic Robot Through the VIPRO Platform, Acta Electrotehnica, 57. https://doi.org/10.1109/icamechs.2016.7813482.
19. Pop, N., Cioban, H., & Horvat-Marc, A. (2011). Finite element method used in contact problems with dry friction. Computational Materials Science, 50(4), 1283-1285.
https://doi.org/10.1016/j.commatsci.2010.03.018.
20. Feng, Y., Wang, H., Lu, T., Vladareanuv, V., Li, Q., & Zhao, C. (2016). Teaching training
method of a lower limb rehabilitation robot. International Journal of Advanced Robotic Systems, 13(2), 57. https://doi.org/10.1109/coase.2013.6653944.
21. Pop, N. (2008). An algorithm for solving nonsmooth variational inequalities arising in frictional quasistatic contact problems. Carpathian Journal of Mathematics, 110-119.
22. Vladareanu, V., Schiopu, P., & Deng, M. (2013). Robots extension control using fuzzy smoothing. In Advanced Mechatronic Systems (ICAMechS), 2013 International Conference on (pp.
511-516). IEEE.
23. Gal, I. A., Vladareanu, L., Yu, H., Wang, H., & Deng, M. (2015, August). Advanced intelligent walking robot control through sliding motion control and bond graphs methods. In
Advanced Mechatronic Systems (ICAMechS), 2015 International Conference on (pp. 36-41).
IEEE. https://doi.org/10.1109/ICAMechS.2015.7287125
24. Gal, A. I., Vladareanu, L., & Munteanu, R. I. (2015). Sliding motion control with bond graph
modeling applied on a robot leg. Rev. Roum. Sci. Techn.lectrotechn. et nerg, 60(2), 215-224.
https://doi.org/10.1109/icit.2006.372347
25. Vladareanu, V., Munteanu, R. I., Mumtaz, A., Smarandache, F., & Vladareanu, L. (2015). The
optimization of intelligent control interfaces using Versatile Intelligent Portable Robot Platform. Procedia Computer Science, 65, 225-232. https://doi.org/10.1016/j.procs.2015.09.115.
26. Melinte, O., Vladareanu, L., Munteanu, R. A., & Yu, H. (2015). Haptic interfaces for compensating dynamics of rescue walking robots. Procedia Computer Science, 65, 218-224.
https://doi.org/10.1016/j.procs.2015.09.114
179
Dac-Nhuong Le et al. (eds.), Emerging Technologies for Health and Medicine, (179–284) 
© 2018 Scrivener Publishing LLC
CHAPTER 14
THE WALKING ROBOT EQUILIBRIUM
RECOVERY APPLIED ON THE NAO
ROBOT
N. Pop1, L. Vladareanu1, H. Wang2, M. Ungureanu3, M. Migdalovici1,
V. Vladareanu1, Y. Feng2, M. Lin2, E. P. Mastan4 and I. El Emary5
1 Romanian Academy, Institute of Solid Mechanics, Bucharest, Romanian
2 Yanshan University, Qinhuangdao, 066004, China
3 Technical University of Cluj-Napoca - North University Centre of Baia Mare
4 High School Nemeth Laszlo Baia Mare, Romania
5 King Abdulaziz Universitry Jeddah, Computer Science and Systems, Saudi Arabia
Emails: luige.vladareanu@vipro.edu.ro , luigiv2007@yahoo.com.sg, nicpop@gmail.com,
hongbo w@ysu.edu.cn, omary57@hotmail.com, yf feng@126.com
Abstract. The chapter equilibrium recovery of the walking robot applied on the NAO
robot using strategies for balancing in the sagittal plane, in the presence of external disturbances is analyzed, by comparing the feedback time to the equilibrium position for the
same disturbance the case of lower control effort. The case of the high values chosen for
R, in the case of higher control effort, together with the case of the low values chosen of
the R are presented. Equilibrium and maintaining the balance of the biped walking robots
play an important role in robot’s operation. The model of a double linear pendulum inverted under-actuated, with one passive and one active joint are studied in modeling the
robot’s balance. The proposed strategy of balance has a goal to move the disturbed system
to the desired equilibrium state. The results lead to a biped walking model equipped with
actuator that provides a torque at the hip. or/and at the ankle. The case studies by these
strategies is validated by Webots and is applied on NAO robot.
Keywords: Walking robots; Balance; Under-Actuated System; Control law.
180 Emerging Technologies for Health and Medicine
14.1 Introduction
The support area of the robot, is either the foot surface in case of one supporting leg or the
minimum convex area containing both foot surfaces in case both feet are on the ground.
These are referred to as single and double support phases, respectively.
The most used models and strategies for balancing of the biped walking robots are:
CoG (or CoM) balancing which has goal to maintain the projection of the center of
gravity (or center of mass) on the ground, inside of the foot support area. This strategy
is used for static walk or only for slow walking speeds;
ZMP (zero momentum point) balancing which has goal to maintain the point inside
the surface defined by BoS (boundary of support, this is the polygon described by the
robot’s foots). This strategy is used for dynamic walking. ZMP is a point where the
horizontal moments are zero. In conclusion, if point P is inside the surface defined
by BoS, all moments and forces exerted by the body on the ankle are compensated,
which necessarily leads to a dynamic balance [1, 2, 10-12].
Combined strategy applies to both ankle and hip strategy to protect the system against
external disturbances.
We specify that for the dynamic walking robots CoG (or CoM) can be outside of the
BoS, but the ZMP, cannot. This chapter uses a two-link inverted pendulum model in the
sagittal plane, with actuator at the hip joint - see Figure 14.1.
Figure 14.1 Two-link inverted pendulum model in the sagittal plane
14.2 The Choice of the Model
For analyzing the balance control for a biped walking model, can be expressed as one or
multi-dimensional inverted pendulum chain. Thus the walking biped can be modeled with
The Walking Robot Equilibrium Recovery 181
one-dimensional inverted pendulum [3-5], in this case the system will be described only
by-one variable: the angle of the ankle joint.
This model is not sufficient to completely explain balance properties, even for standing
balance. In many studies is used the double inverted pendulum model [3-5, 13-15]. This
model is not sufficient to completely explain balance properties, even for standing balance.
In many studies is used the double inverted pendulum model [6, 7]. For best approximation of the human body, the biped walking robot can be modeled with multi-dimensional
inverted pendulum chain that allows the study of the responses for complex perturbations
[16, 17].
We chose a simple model which can give as much information as possible to the balance
recovery strategy of a biped walking robot. The two rigid links of the model are: one
established by both legs and the other including the head, arms and torso. We chose to
study, the model of a double inverted pendulum under-actuated, with one passive and one
active joint, who can approach balance in the case of a single phase, i.e. in case of one
supporting leg [8-9, 18-20]. This model also assumes that both legs move together at all
times, there-fore are modeled as a single link.
14.3 Mathematical Modeling of Two-Link Biped Walking Robot
The equations of motion for a two-link inverted pendulum were derived using the NewtonEuler equation and linearized by employing Taylor series expansion, evaluated around the
equilibrium point x = [1.57, 0, 0, 0], which is vertical position and zero angular speeds, as
follows:
M

q¨1
q¨2

= −F

q˙1
q˙2

− G

q1
q2

+ BN

τ1
τ2

(14.1)
Where the matrices can be written as follows,
M =

J1 + m1l
2
c1 + m2l
2
1 + 2m2l1lc2 + J2 + m2l
2
c2 J2 + m2l1lc2 + m2l
2
c2
J2 + m2l1lc2 + m2l
2
c2 J2 + m2l
2
c2

(14.2)
F =

f1 0
0 f2

(14.3)
with a linearized friction model:
f1 = c1
α
2 + v1 and f2 = c2
α
2 + v2 (14.4)
obtained from the following nonlinear model:
Fi = cisgn( ˙qi) + viq˙i ≈ cith(αq˙i) + viq˙i (14.5)
where ci and vi are Coulomb and viscous friction coefficients at the ankle joint (i = 1)
and at the hip joint (i = 2), respectively. The function sgn(.) is approximate with the
hyperbolic tangent function th(α.), α = 50. Also,
G =

m1glc1 + m2glc2 + m2l1g m2glc2
m2glc2 m2glc2

(14.6)
182 Emerging Technologies for Health and Medicine
BN =

0 0
0 1 
(14.7)
resulting in only one actuator at the hip joint. Where m1, m2, l1 and l2 are the equivalent
mass and length of each link, leg and torso, respectively, lc1 and lc2 are mass centers
relative to the lower joint, J1 and J2 are the moments of inertia about the CoM of the
corresponding link (around pitch axis), q1 and q2 are the ankle and hip joint angles and τ1
and τ2 are the ankle joint torque and hip joint torque, respectively (in our case τ1 = 0 ).
After linearization, it is known that coriolis and centrifugal terms obtained in the original non linear equations have been eliminated and do not contribute to the simplified
model.
14.4 Linear Control Design
For formulating the feedback control model, we defined the state vector, x, of joint kinematics referenced as, where x = [q1 q2 q˙1 q˙2]
T are the ankle and hip joint angles, and q˙1
and q˙2 represent angular velocities, respectively. The state model is determined by,
x˙ = Ax + Bu (14.8)
where matrix A encapsulates the dynamic properties of the system that exist due to the
particular chosen state and B determines the input function. The variable u is the control
input and the system output, or response function,
y(t) = Cx(t) = 

1000 
x(t) (14.9)
The system (12.1), representing two ordinary differential equations of the second order,
is equivalet with system (12.8), which represents four ordinary equations of the first order.
This equations are linearized around the equilibrium state, resulting in a simplified model
which is only valid within a close vicinity to this point. This assumptions, for this control
technique, implies instability for large deviations from the desired position.
After we defined the state space model, it is important to determine appropriate control
input, which will guarantee stability and convergence of the system to the desired position.
For this purpose, state feedback is employed, which requires gain tuning to get convergence
at the desired position under constraints imposed by actuator and joint limitations.
If we note the matrices:
M−1 =

m11
m21
m12
m22 
(14.10)
M−1G =

mg11 mg12
mg21 mg22 
, (14.11)
M−1F =

mf1 0
0 mf2

(14.12)
and
The Walking Robot Equilibrium Recovery 183
M−1BN =

0 m12
0 m22 
(14.13)
from (14.1), the matrix A and B, from the system (12.8), gets:
A =
⎡
⎢
⎢
⎢
⎢
⎣
0 0 10
0 0 01
−mg11 −mg12 −mf1 0
−mg21 −mg22 0 −mf2
⎤
⎥
⎥
⎥
⎥
⎦
(14.14)
B =
⎡
⎢
⎢
⎢
⎢
⎣
0
0
0
0
0
0
m12
m22
⎤
⎥
⎥
⎥
⎥
⎦
, (14.15)
Bu =
⎡
⎢
⎢
⎢
⎢
⎣
0
0
0
0
0
0
m12
m22
⎤
⎥
⎥
⎥
⎥
⎦

τ1
τ2
0
0
0
0
0
0

=
⎡
⎢
⎢
⎢
⎢
⎣
0 000
0 000
m12τ2 000
m22τ2 000
⎤
⎥
⎥
⎥
⎥
⎦
(14.16)
and vector x˙ =[˙q1 q˙2 q¨1 q¨2]
T .
In the case studied, the ankle joint being stabilized, the hip joint is required to maintain
its desired position q2d = 0. Optimality in this situation is determined by employing the
linear quadratic regulator, described in the next section.
14.4.1 Linear Quadratic Regulator
To determine the optimal trajectory in regaining the equilibrium position an optimal feedback controller needs to be designed. Optimality [4] has been defined in terms of a
quadratic cost function as follows
Jlqr = 1
2
∞
0
[x(t)
T Qx(t) + u(t)
T Ru(t)]dt (14.17)
where xT Qx is the state cost with weight Q = QT > 0, and uT Ru is called the control
cost with weight R = RT > 0. The value of Q and R are randomly chosen until the output
of the system does not get the desired value.
The linear feedback matrix u, is defined as:
u(t) = −Klqrx(t) (14.18)
The Klqr matrix is responsible for defining optimality in the linear quadratic regulator
and is obtained by solving the Riccati equation, given below:
AT P + P A + Q − PBR−1BT P = 0 (14.19)
184 Emerging Technologies for Health and Medicine
The solution of this equation is P, called the optimal matrix, used in determining the
gain matrix:
Klqr = R−1BT P (14.20)
The feedback control input u, the joint torque, was assumed to be generated by full-state
feed-back in the following form:
u = [τ1 τ2]
T = −Klqrx(t) (14.21)
and,
Klqr =

k11
k21
k12
k22
k13
k23
k14
k24 
(14.22)
By varying the gain matrix Q and R, the penalty error of the state x and the control effort
u is controlled. The gain matrix used in experimentation are,
Q = I4x4, R = (10e − 12)I2x2 (14.23)
where a higher penalty is applied on the control effort as compared to the state.
These gains can be determined by keeping in mind joint motor limitations in providing
the control effort in terms of torque.
This approach is proven to be much faster compared to traditional pole placement technique, while the desired balance of priorities between the state and control effort can be
regulated much easily. The system (12.8) will be equivalent with system:
x˙ = (A − BKlqr)x(t). (14.24)
14.4.2 Numerical esults using MATLAB
In order to verify the correctness of the proposed model, the simulations are obtained with
the parameters given in Table 14.1 for the NAO robot.
Table 14.1 Parameters of the NAO robot
Model Parameter Units Label Value
Mass Kg m1 2.228
m2 2.118
Length m l1 0.27
l2 0.27
Center of the mass m lc1 0.135
lc2 0.135
Inertia Kg m2 J1 0.000192
J2 0.00000833
Coulomb friction Nm c1 0.1
c2 0.2
Viscous friction Ns v1 -2.78
v2 -23.5
The numerical results justify the mathematical model, when the model is under disturbance.
R
The Walking Robot Equilibrium Recovery 185
The LQR controller can be tuned in MATLAB when the values for Q and R matrices for
state space model are specified, and the state-feedback gain matrix is obtained as follows:
Klqr =

0
−344.14
0
−84.68
0
−40.52
0
24.99 
(14.25)
For an initial disturbance of approximately 1.2 degrees (0.02 radians) to the ankle and
1.7 degrees (0.03 radians) to the hip, results are shown in Figure 14.2, Figure 14.3, Figure
14.4, and Figure 14.5.
Figure 14.2 Stabilization is done in 18 seconds, with a disturbance to the ankle and the hip of
x0 = [0.02, 0.03, 0, 0] and a low R value
Figure 14.3 Stabilization is done in 35 seconds, with a disturbance to the ankle and the hip of
x0 = [0.02, 0.03, 0, 0] and a high R value
Although, for small values of R, (rii = 10−12), a rapid return to the equilibrium position
was obtained, it was through a very high effort for the actuator, as shown in Figure 14.2
and Figure 14.4.
186 Emerging Technologies for Health and Medicine
Figure 14.4 Stabilization is done in 18 seconds, with a disturbance to the ankle and the hip of
x0 = [−0.02, 0.03, 0, 0] and a low R value
Figure 14.5 Stabilization is done in 60 seconds, with a disturbance to the ankle and the hip of
x0 = [−0.02, 0.03, 0, 0] and a high R value
In case of high values for the elements of the matrix R, (rii = 1.), the actuator control
effort is lower, but the stabilization is reached in a longer time and after several oscillations,
as seen in Figure 14.3 and Figure 14.5.
Also, in the case of a disturbance only at the level of the hip, in Figure 14.6 and Figure
14.7, it can be noticed that the stabilization time is twice as high, with lower control effort,
meaning higher R values chosen.
In this paper, we compared the return times to the equilibrium position for the same
disturbance, when the NAO robot has the mass centers in the middle of the links or when
the robot has the mass centers placed in the ratio (li − lci)/lci = 1.618, i = 1, 2, (i.e. the
golden section). It has been found that, for the NAO robot case, a low height robot, the
stabilization time is approximately equal.
The Walking Robot Equilibrium Recovery 187
Figure 14.6 Results for disturbance only to the hip x0 = [0, 0.03, 0, 0] with high R values,
stabilization is done in 40 seconds.
Figure 14.7 Results for disturbance only to the hip x0 = [0, 0.03, 0, 0] with lower R values,
stabilization is done in 20 seconds.
The possible justification is that in the case when the mass centers are placed in the
middle of the links and in the case when the robot has the mass centers placed in the
golden section (lc1 = 0.135, lc1 = 0.103), are approximately equal, and that the spectral
radius of the matrices of the two dynamic systems are almost equal (0.16 or 0.1).
14.5 Results and Discussion
The linear quadratic regulator is used as a faster means of convergence when the hip joint
is close to the desired state. The control strategy formulated defines torque for the hip joint,
this can be accompanied with a simple PD controller at the ankle, τ1 = KP 1(1.57 − q1) −
KD1q˙1.
188 Emerging Technologies for Health and Medicine
This algorithm completely removes any torque provided to the ankle and evaluates performance of various controllers under such conditions.
14.6 Conclusions
The analyzed state space model has been tested in several cases, under the same disturbance
applied to the system, and the return times to the equilibrium position have been compared.
The numerical result, established in the case of lower control effort, implies a higher R
value and in the case of higher control effort, a low R value will be used. If the R gain
value is increased to make a lower control effort, the stabilization is achieved after a few
oscillations.
The obtained results were quantified by decreasing the spectral radius of the matrix,
which means increasing stability of the biped walking robot.
Acknowledgment
This work was developed with the support by Romanian Academy, European Commission
Marie Sklodowska-Curie SMOOTH project (H2020-MSCA-RISE-2016-734875/2016-2020)
and ”Joint Laboratory of Intelligent Rehabilitation Robot” collaborative research agreement between Romanian Academy by IMSAR, RO and Yanshan University, CN. Project
KY201501009/2016.
REFERENCES
1. Vukobratovic, M., & Juricic, D. (1969). Contribution to the synthesis of biped gait. IEEE
Transactions on Biomedical Engineering, (1), 1-6.
2. Vukobratovic, M. (1973). How to control artificial anthropomorphic systems. IEEE Transactions on Systems, Man, and Cybernetics, (5), 497-507.
3. Winter, D. A., Patla, A. E., Rietdyk, S., & Ishac, M. G. (2001). Ankle muscle stiffness in the
control of balance during quiet standing. Journal of neurophysiology, 85(6), 2630-2633.
4. Peterka, R. J. (2002). Sensorimotor integration in human postural control. Journal of neurophysiology, 88(3), 1097-1118.
5. Micheau, P., Kron, A., & Bourassa, P. (2003). Evaluation of the lambda model for human
postural control during ankle strategy. Biological cybernetics, 89(3), 227-236.
6. Martin, L., Cahout, V., Ferry, M., & Fouque, F. (2006). Optimization model predictions for
postural coordination modes. Journal of biomechanics, 39(1), 170-176.
7. Park, S., Horak, F. B., & Kuo, A. D. (2004). Postural feedback responses scale with biomechanical constraints in human standing. Experimental brain research, 154(4), 417-427.
8. Ahmed, S. M., Chew, C. M., & Tian, B. (2013). Standing posture modeling and control for
a humanoid robot. In Intelligent Robots and Systems (IROS), 2013 IEEE/RSJ International
Conference on (pp. 4152-4157). IEEE.
9. MONKOVA, K., MONKA, P., & HRICOVA, R. (2017). Two Approaches to Modal Analysis
of the Flange Produced by DMLS Technology. DEStech Transactions on Engineering and
Technology Research, (tmcm).
The Walking Robot Equilibrium Recovery 189
10. Vladareanu, L., Melinte, O., Bruja, A., Wang, H., Wang, X., Cang, S., ... & Xie, X. L. (2014).
Haptic interfaces for the rescue walking robots motion in the disaster areas. In Control (CONTROL), 2014 UKACC International Conference on (pp. 498-503). IEEE. DOI: 10.1109/CONTROL. 2014.6915189, ISBN 978-1-4799-2518-6
11. Pop, N., Vladareanu, L., Popescu, I. N., Ghi, C., Gal, A., Cang, S., ... & Deng, M. (2014).
A numerical dynamic behaviour model for 3D contact problems with friction. Computational
Materials Science, 94, 285-291.
12. Luige Vladareanu, Daniel Mitroi, Radu Ioan Munteanu, Shuang Chang, Hongnian Yu, Hongbo
Wang, Victor Vladareanu, Radu Adrian Munteanu. Hou z.g., Octavian Melinte, Radu Adrian
Munteanu , Zeng-Guang Hou , Xiaojie Wang , Guibin Bia , Yongfei Feng and Eugen Albu
(2016), Improved Performance of Haptic Robot Through the VIPRO Platform, Acta Electrotehnica, vol. 57, ISSN 2344-5637.
13. Pop, N., Cioban, H., & Horvat-Marc, A. (2011). Finite element method used in contact problems with dry friction. Computational Materials Science, 50(4), 1283-1285.
14. Feng, Y., Wang, H., Lu, T., Vladareanuv, V., Li, Q., & Zhao, C. (2016). Teaching training
method of a lower limb rehabilitation robot. International Journal of Advanced Robotic Systems, 13(2), 57.
15. Pop, N. (2008). An algorithm for solving nonsmooth variational inequalities arising in frictional quasistatic contact problems. Carpathian Journal of Mathematics, 110-119.
16. Vladareanu, V., Schiopu, P., & Deng, M. (2013, September). Robots extension control using
fuzzy smoothing. In Advanced Mechatronic Systems (ICAMechS), 2013 International Conference on (pp. 511-516). IEEE.
17. Gal, I. A., Vladareanu, L., Yu, H., Wang, H., & Deng, M. (2015, August). Advanced intelligent
walking robot control through sliding motion control and bond graphs methods. In Advanced
Mechatronic Systems (ICAMechS), 2015 International Conference on (pp. 36-41). IEEE.
18. Gal, A. I., Vladareanu, L., & Munteanu, R. I. (2015). Sliding motion control with bond graph
modeling applied on a robot leg. Rev. Roum. Sci. Techn.-lectrotechn. et nerg, 60(2), 215-224.
19. Vladareanu, V., Munteanu, R. I., Mumtaz, A., Smarandache, F., & Vladareanu, L. (2015).
The optimization of intelligent control interfaces using Versatile Intelligent Portable Robot
Platform. Procedia Computer Science, 65, 225-232.
20. Melinte, O., Vladareanu, L., Munteanu, R. A., & Yu, H. (2015). Haptic interfaces for compensating dynamics of rescue walking robots. Procedia Computer Science, 65, 218-224. DOI:
10.1016/j.procs.2015.09.114
191
Dac-Nhuong Le et al. (eds.), Emerging Technologies for Health and Medicine, (191–284) 
© 2018 Scrivener Publishing LLC
CHAPTER 15
DEVELOPMENT OF A ROBOTIC
TEACHING AID FOR DISABLED
CHILDREN IN MALAYSIA
N.Zamin1, N.I. Arshad2, N. Rafiey2 and A.S. Hashim2
1 University Malaysia of Computer Science and Engineering, Putrajaya, Malaysia
2 Universiti Teknologi PETRONAS, Seri Iskandar, Perak, Malaysia
Emails:
Abstract
Many special needs children suffer from a common characteristics impairment which
appear as disability to interpret social cues, fail to use joint-attention tasks as well as a failure in social gaze when communicating. This what makes them different than the normal
children. The results of this difficulty are the special needs children often get frustrated
when they are unable to expressively share their feeling and socially interact with the community. This research is investigating the problems faced by autistic, down syndrome and
slow learner children to respond and communicate appropriately with the people around
them and to propose an efficient approach to improve their social interaction. Malaysian
education policy is to integrate students with learning difficulties or special educational
needs. Thus, the development of a robotic approach using LEGO Mindstorms EV3 to aid
the teaching and learning of special needs children especially autism in in Malaysia in
introduced in this paper. Robotic approach in special education provides changes, inclusive and sustainable development of the disabled community towards supporting Industrial
Revolution 4.0.
Keywords: Robotic, Special Education, Social Interactions, Developmental Disabilities,
Autism, Down Syndrome, Slow Learner
192 Emerging Technologies for Health and Medicine
15.1 Introduction
Creating the abilities of the special needs children’s day by day life is truly a test as each
of them has diverse symptoms and is remarkable in their own particular ways. To help
improve their life’s quality, there are few real zones of education that must be thought to.
Those ranges are communication, social and independence. Special needs children have
an alternate route in learning and tolerating data that is different from normal children.
The primary goal of this research is to enhance the teaching and learning experience of the
special needs children from the fundamental methodologies and therapies. Many studies
have previously proposed robotic approach as alternative therapy tool to improve social
interaction skills and as well as reducing the emotional problem among the special needs
children [2-4]. This is because robot has no feeling and can perform repetitive actions
without getting bored or stressed. The proposed solution is to develop an LEGO robot
to assist teachers, therapists as well as parents to improve social interaction skills among
special needs children. This tool is not intended to replace the teachers and therapist but
rather as an assistive tool.
15.2 Case Study - Autism
Autism is a complex neurobehavioral disorder that includes impairments in social interaction and developmental language and communication skills combined with rigid and
repetitive behaviours [5]. Autism Spectrum Disorder (ASD) refers to the wide range of
symptoms, skills and level of impairment or disability which include Asperger’s and Kanner’s Syndrome [6]. Among early signs of ASD is persistent deficit in social communication and interaction, repetitive patterns of behaviour, interests or activities and low ability
in understanding multiple instructions. Typically, symptoms are presence in the first two
years of a child’s life [7]. Until today, the medical society is unable to confirm that the
genetic factor is the main cause of ASD [8]. There is no specific medical treatment to
cure autism, but many strategies and treatment options are available for autistic children
[9]. Early diagnosis and correct therapy would help young children with autism to develop
their full potential. Most current therapy methods aimed to improve the overall ability of
the autistic children. As the number of children with autism has risen dramatically over the
past couple of decades, experts have discovered that the earlier specialized therapy can be
initiated; the outcome can be significantly improved [10]. The proposed approach is tested
on autistic children at selected special schools and centers in Malaysia [11].
15.3 Movitations
Inspired by the difficulties of the observed current therapy methods and from the literature
studies, a new and sustainable approach using robotic technology is proposed. The robotic
intervention in nurturing autistic children has been very helpful in enhancing reading skills
and generalizing knowledge for young pupils with autism. The sequence, progressive development is well defined and simple for therapists and parents to amend and keep track of
the child’s improvement. A rising comprehension of the robotic learning practice of autistic children is getting more attention from academic society. Autistic children go through
their day by day activities by their weak senses that can be further enhanced with the aid
of robotics [4]. It reduces the tension for them to rekindle what happens later, give a con-
Development of A Robotic Teaching Aid for Disabled Children 193
cise and clear path between actions, and aid them to be independent. The nonverbal signs
shown by the robots to them can last a long time since they have a habit of repeating on
every action they learnt [12].
Thus, robotic engagement has causes the evolution of education practices amongst autistic children. LEGO therapy is one of the current treatments for learning among disabled
group of children including the autistic children. The LEGO therapy can improve cognitive development, creativity and hand-eye coordination while improving social skills when
played together in a team [13, 14]. In this traditional LEGO therapy, children are normally
supervised by assigned therapists. Our proposed method is to automate the LEGO therapy
by using the LEGO Mindstorms, a programmable LEGO toolkit as a teaching and learning
aid for the autism therapist. Our method is referred as the RoboTherapist that will adapt
the ability to teach the basic foundation of knowledge through observation and hand-eye
coordination with the supportive function from their attracted repetitive behaviors.
15.4 Proposed Approach
As the fourth industrial revolution (IR 4.0) and its embedded technology diffusion progress
is expected to grow exponentially in terms of technical change and socioeconomic impact,
we introduce a holistic approach that encompasses innovative and sustainable system solutions for special needs children [15]. In this article, a robot known as RoboTherapist using
LEGO Mindstorms EV3 to teach autistic kids to differentiate shapes and encourage the
kids to draw basic shapes correctly. It is a new approach and never been applied in special
educations in Malaysia.
The RoboTherapist will be placed on a flat whiteboard and detect color by using color
sensor that has been programmed in the LEGO Mindstorms EV3 Software. When the
RoboTherapist detects the color on the whiteboard, it will start to draw shapes as preprogrammed. It will keep on looping until the user click end program. The association of
colors and shapes are programmed as follows:
Figure 15.1 The shapes and colors
The mechanism used by the RoboTherapist is the fixed rotation of the motor steering to
draw each shape shown in Figure 15.1. The following figures illustrate the movement of
RoboTherapist:
Figure 15.2 The fixed motor directions
194 Emerging Technologies for Health and Medicine
The flow chart in Figure 15.2 shows the flow of the overall program. RoboTherapist initiates by detecting the color read by the color sensor and draw the shapes as preprogrammed in Figure 15.2.
Figure 15.3 RoboTherapist flowchart
Figure 15.4 RoboTherapist
Development of A Robotic Teaching Aid for Disabled Children 195
Then, it will keep on looping until the stop button is pressed. The special needs children
will observe the teaching from the RoboTherapist guided by the teachers. Their understanding is tested by a manual test designed to evaluate the effectiveness of the robotic
approach.
15.5 Results and Discussions
Initially, before the test was carried out, a pre-selection test was done to make sure whether
the test candidates are fit for the test or otherwise. In the pre-selection, the potential candidates are asked to tinker with the RoboTherapist and their responses are recorded. If they
can handle the robots well, then they are selected. This effort is highly crucial to avoid
unnecessary damage on the robot by highly uncontrolled kids (a normal behaviour for
some autistic children). Once selected, they will seat for the actual test where the therapist
will assist the children to RoboTherapist. The comparative results between the traditional
learning method and the robotic approach are presented. From the survey conducted we
can see that most of the children with autism will get easily distracted, need to repeat several times in making them understand. It is very challenging in attracting and retaining the
autistic children attention, especially in learning.
Figure 15.5 Survey on students’ attentiveness
We can also conclude that most of the major challenge in teaching the autistic children
falls under ”Social Communication” where the children find it hard in letting people to
control their emotion and also behavior. This happens because for them it is difficult to understand and follow the instructions given by the teachers. Then, the survey continues with
the benefits of implementing or introducing the learning method with robot as a medium
in teaching the autistic children. As we can see and observe from the results below, most
of the respondents agree with the implementation of Robot in teaching basic shapes to the
autistic children.
Most of the respondents agreed that the teaching approach using robot is the best assistive tool for teaching the autistic children. In addition, below are the opinions shared by the
adult respondents (teachers, parents and caretakers) throughout the survey. As the autistic
196 Emerging Technologies for Health and Medicine
Figure 15.6 Survey on the effectiveness of robotic approach
children can easily get distracted therefore more attention are needed when teaching them.
Thus, with the new teaching and therapy method by introducing the robot to the autistic
children, attracts the children’s attention and making learning basic shapes fun and easy.
Figure 15.7 Opinions on robotic approach
Following this, observation and assessment was conducted in a selected school. Five
selected respondents participated voluntarily (refer to Table 15.1) with the assistance of a
well-trained teacher. The results gathered from the observation and assessment were then
analyzed and discuss in the following paragraphs.
Development of A Robotic Teaching Aid for Disabled Children 197
Table 15.1 Participants Details
Participants Background Age
RN An autistic student 10
DH An autistic student 11
KMH An autistic student 10
CWG An autistic student 14
CSN A trained teacher teaching autistic students N/A
15.6 Robotic Intervention Enhance Autistic Students’ Engagement, Interaction and Focus
It was observed that the traditional method that have been used in teaching the autistic
children in learning basic shapes, which is by using shape cards and whiteboard creates a
monotonous and mundane learning environment. All students have to sit and listen to the
teacher and focus on the drawn shapes on whiteboard or the shapes being shown on the
cards. Students were quiet and seems not interested after 5 minutes, as shown in Figure
15.8.
Figure 15.8 Traditional method in teaching basic shapes using cardboard and whiteboard
Throughout the observation, the traditional learning could only sustain the concentration
of autistic children in learning within 10 minutes. After that, the autistic children start to
lose their interest in learning. This is due to the fact that, the autistic children have the
tendency to engage in repetitive behavior and attention (Autism Speacks Inc., 2017). As
shown in Figure 15.9, the children tend to lost interest when they did not get attention from
the teacher.
Contrary to the traditional learning, learning using Roboshapes creates a different and
more positive atmosphere. From the observation throughout the learning process, the autistic children seem more attracted to learn with the robot as all of them can maintain to learn
basic shapes with the robot for more than 20 minutes. From the Figure 15.10, we can see
that all the autistic children are excited and attracted to learn with the Robot.
Moreover, by implementing robot in assisting the teacher in teaching, the learning process held in the classroom seems more active. The students were pro-active in asking
questions, suggesting new things and idea. This is because they are adopting a different
style of learning basic shapes by learning-by-doing thus, the children are the one who are
really eager to learn and want to see the action done by the robot. The teacher as well as
198 Emerging Technologies for Health and Medicine
Figure 15.9 After 10 minutes learning autistic children started to lose their interest
Figure 15.10 Autistic children still attracted to learn even after 20 minutes
some parents did gave a positive feedback from the robotic intervention. It was highlighted
that the implementation of EV3 Robot in teaching basic shapes to the autistic children is
much more beneficial and gives such positive feedback from the children themselves. For
example, from the Figure 15.11 it was shown that the autistic children took a pro-active
step to command the robot to draw shapes by putting the color sensor of the robot at the
starting point without being asked to. This shows that students were more engaged in
learning.
Figure 15.11 Hands-on learning
Development of A Robotic Teaching Aid for Disabled Children 199
After both learning process (i.e. traditional and robotic intervention) were completed,
the students were given two assessment tests to see the impact of learning basic shapes, as
shown in Figure 15.12. The first test is relating to the content of the traditional module,
while the second test is relating to the robotic intervention learning content. The results
were then collected and analyzed to see the differences and impact of both methods. The
results were presented and discussed in the next paragraphs.
Figure 15.12 Test after learning process with Robot
Table 15.2 shows the results of assessment conducted after the students completed both
the traditional learning and with the assistance of Roboshapes. The ’traditional method’
column presents the results of the assessment (i.e. Test 1) that was conducted based on
the modules taught by the teacher using cards and whiteboard. Following this, the column
’robot method’ presents two assessment results (i.e. Test 2 & Test 3) based on the modules
taught by the Roboshapes. Referring to Table 15.2, the average score for Test 1 (i.e. traditional learning) was 90%, while the average score for Test 2 and Test 3 (i.e. learning with
the assistance of Roboshapes) were 100%. This shows that students learn better when the
teaching and learning was assisted by the Robot.
Table 15.2 Test Assessment Results (traditional vs robotic intervention)
200 Emerging Technologies for Health and Medicine
As mentioned by the teacher, the autistic students learn better by learning through
robotic intervention. This is due to the fact that they find it interesting were attracted
to the learning approach. For example, to make the autistic children more attracted in
learning, it is found that they like to receive compliments. Those compliments will make
them feel more excited and motivated to learn. Since Roboshapes never fail to give them
compliments such as ”Well done!”, ”Good!” and ”Congratulations!”, the students feel engaged and attracted to learn more. However, it is noted that it is important to ensure that
the learning process of the autistic children to be conducted in a conducive environment
(e.g. not hot or noisy, morning time etc.). This is to ensure that learning process could be
run smoothly.
15.7 Conclusion
There are numerous approaches to teach autistic children in a more engaging manner and
this study has shown that robotic intervention seems to be very promising. From the observation and results of the test assessments, it shows that the implementation of robot in
assisting the teacher in teaching leads to more effective learning experience. This could be
seen from students’ behavior whom are more engaged, interested and focused. Further, the
sustenance in learning and focus-learning time span is longer with robotic intervention.
On the other note, having robots as a teaching and learning tool opens up many other
opportunities. This include skills to build and construct robots based on creativity to teachers and the interested autistic children. They could also use it for playing and distressing
themselves. This definitely creates a better teaching and learning in class experiences for
the special needs children.
In conclusion, an extension to the current way of teaching and learning for the special
needs students should not be left unexplored. Although the schools and parents had to take
risks in exploring the best support for this group of children, it is important to note that
this group of students deserve relevant and quality experience in the endeavor of learning.
Therefore, it is hoped that more future studies will be conducted towards exploring better opportunities in improving this group of students in learning so that they could also
embrace the wave of IR 4.0.
REFERENCES
1. Ghani, M. Z., Ahmad, A. C., & Ibrahim, S. (2014). Stress among special education teachers in Malaysia. Procedia-Social and Behavioral Sciences, 114, 4-13. DOI:
10.1016/j.sbspro.2013.12.648
2. Huijnen, C. A., Lexis, M. A., Jansens, R., & de Witte, L. P. (2017). How to Implement
Robots in Interventions for Children with Autism? A Co-creation Study Involving People with
Autism, Parents and Professionals. Journal of autism and developmental disorders, 47(10),
3079-3096. DOI: 10.1007/s10803-017-3235-9
3. SPARTANBURG, R. I., & CAROLINA, S. (2016), IESD Case Study: Children on the Autism
Spectrum Show Improvement with ROBOTS4AUTISM in Spartanburg, South Carolina, Interactive Educational Systems Design Inc., South Carolina 2016.
4. Martelo, A. B. (2017). Social Robots to enhance therapy and interaction for children: From
the design to the implementation in the wild (Doctoral dissertation, Universitat Ramon Llull).
Development of A Robotic Teaching Aid for Disabled Children 201
5. Lauritsen, M. B. (2013). Autism spectrum disorders. European child & adolescent psychiatry,
22(1), 37-42.
6. Murphy, C. M., Wilson, C. E., Robertson, D. M., Ecker, C., Daly, E. M., Hammond, N., ...
& McAlonan, G. M. (2016). Autism spectrum disorder in adults: diagnosis, management,
and health services development. Neuropsychiatric disease and treatment, 12, 1669. DOI:
10.2147/ndt.s65455
7. S. Ozonoff, A. M. Iosif, F. Baguio, I. C. Cook, M. M. Hill, T. Hutman, et al., A prospective
study of the emergence of early behavioral signs of autism, Journal of the American Academy
of Child & Adolescent Psychiatry, vol. 49, pp. 256-266, 2010.
8. Joshi, I., Percy, M., & Brown, I. (2002). Advances in understanding causes of autism and
effective interventions. Journal on developmental disabilities, 9(2), 1-27.
9. DeFilippis, M., & Wagner, K. D. (2016). Treatment of autism spectrum disorder in children
and adolescents. Psychopharmacology bulletin, 46(2), 18.
10. M. o. E. MoE, Data Pendidikan Khas 2016, Ministry of Education Malaysia, Kuala Lumpur
2017.
11. A. Mulligan, R. J. Anney, M. O’Regan, W. Chen, L. Butler, M. Fitzgerald, et al. (2009), Autism
symptoms in attention-deficit/hyperactivity disorder: a familial trait which correlates with conduct, oppositional defiant, language and motor disorders, Journal of autism and developmental
disorders, vol. 39, pp. 197-209.
12. Michaud, F., & Thberge-Turmel, C. (2002). Mobile robotic toys and autism. In Socially Intelligent Agents (pp. 125-132). Springer, Boston, MA. DOI: 10.1007/0-306-47373-9 15
13. LeGoff, D. B. (2004). Use of LEGO as a therapeutic medium for improving social competence.
Journal of autism and developmental disorders, 34(5), 557-571. DOI: 10.1007/s10803-004-
2550-0
14. G. Owens, Y. Granader, A. Humphrey, and S. Baron-Cohen, LEGO therapy and the social use
of language programme: An evaluation of two social skills interventions for children with high
functioning autism and Asperger syndrome, Journal of autism and developmental disorders,
vol. 38, p. 1944, 2008.
15. R. Morrar, H. Arman, and S. Mousa, The Fourth Industrial Revolution (Industry 4.0): A Social
Innovation Perspective, Technology Innovation Management Review, vol. 7, pp. 12-20, 2017.
203
Dac-Nhuong Le et al. (eds.), Emerging Technologies for Health and Medicine, (203–284) 
© 2018 Scrivener Publishing LLC
CHAPTER 16
TRAINING SYSTEM DESIGN OF LOWER
LIMB REHABILITATION ROBOT BASED ON
VIRTUAL REALITY
H. Wang1, M. Lin1, Z. Jin1, X. Wang1, J. Niu1, H. Yu1, L. Zhang1, L.
Vladareanu2
1 Parallel Robot and Mechatronic System Laboratory of Hebei Province and Key Laboratory of Advanced Forging & Stamping Technology and Science of Ministry of Education, Yanshan University,
Qinhuangdao, 066004, China
2 Romanian Academy, Institute of Solid Mechanics, Bucharest, Romanian
Emails: uige.vladareanu@vipro.edu.ro; luigiv2007@yahoo.com.sg
Abstract
This chapter introduces a training system for the lower limb rehabilitation robot based
on virtual reality (VR), mainly including trajectory planning and VR control strategy. It can
simulate the bike riding and encourage patients to join in the recovery training through the
built-in competitive game. The robot could achieve the linear trajectory, circle trajectory
and arbitrary trajectory based on speed control, the training velocity and acceleration in
the planning trajectory have been simulated. The human-machine dynamics equation was
built which is used for judge the patient’s movement intention. The VR training mode is
a variable speed active training under the constraint trajectory, and it has adapting training
posture function which can provide individual riding training track according to the legs
length of patients. The movement synchronization between the robot and virtual model is
achieved by interaction control strategy, and robot can change the training velocity based
on the signal from feedback terrains in game. A serious game about bike match in forest was designed, and the user can select the training level as well as change perspective
through the user interface.
Keywords: Rehabilitation Robot; Trajectory Planning; Virtual Reality; Interact Strategy; Serious Game.
204 Emerging Technologies for Health and Medicine
16.1 Introduction
As aging society comes to many counties in the world, the health of elderly has become a
focus problem [1-2]. Stroke is a common disease in the elderly which has a high morbidity
and disability [3], and the rehabilitation training based on neural plasticity is regarded as
an effective method to stroke sequel [4-6]. Traditional rehabilitation need a long term
one-on-one treatment which costs too much human resources, and it cannot maintain a
stable intensive. Since the training process is too boring and simple, it is hard to attract
patients and receive active cooperation. However, the combination of robotics and VR
could properly solve these problems.
Serious game is a kind of application designed for a primary purpose other than pure
entertainment, and it is generally referred to video games based on VR technology which
are commonly used in defense, education, and scientific exploration, health care. As the
positive effects of VR games in rehabilitation are shown by various scientific studies [7-
11], the VR applications in rehabilitation robots have been the focus of researchers in
various countries [13-15]. MIT-Manus is the first widely known limb rehabilitation robot
with simple VR system, and a robot named GENTLE/s with virtual interaction scenes was
designed by University of Reading [16-17]. A 6-DOF (Degree Of Freedom) rehabilitation
robot with VR function was developed by Osaka University [18]. A VR-based rehabilitation robot was developed by Tianjin University of Technology, which could make the
training process visual and interactive [19].
This chapter presents a VR training system based on a lower limb rehabilitation robot,
and it is shown as follow: section 16.2 is the introduction of the rehabilitation robot and
the sensors. In section 16.3, the design of training trajectory and simulation are presented.
In section 16.4, the design of VR training system is presented. Section 16.5 is the build
process of VR game scenes and game functions. The experiment data of VR training is
shown in the last section 16.6.
16.2 Application Device
16.2.1 Lower Limb Rehabilitation Robot
Figure 16.1 Lower limb rehabilitation robot
Training System Design of Lower Limb Rehabilitation Robot 205
The LLRR (Lower Limb Rehabilitation Robot), presented in Figure 16.1, was designed
as a modular structure, and it consist of the left mechanical leg, the right mechanical leg,
the separable chair and the electric box. User could control the robot through the touchscreen equipped on the right mechanical leg.
Each mechanical leg owns 3-DOF and contains hip joint, knee joint and ankle joint
which are same as human joints shown in Figure 16.2. The mechanical leg could be divided into the thigh part and the shank part, and the length of each part could be changed
electronically to meet the various legs length of patients from 1.5m to 1.9m.
Figure 16.2 Left mechanical leg
To satisfy the different shapes of patients, the width between two legs could be adjusted
automatically. A separable chair with four universal wheels used for sitting/lying training
and patients transfer was designed.
16.2.2 Necessary Sensor Element
The torque and pressure sensors equipped on LLRR are shown in Figure 16.3. Four torque
sensors are installed in hip and knee joints which could receive torque data constantly from
the joints. The joint torque data is the necessary judgment of the active training and the
VR training.
Figure 16.3 Necessary sensor element
206 Emerging Technologies for Health and Medicine
Foot pressure data, which is collected by sensors equipped in the foot pedal, could
transformed into the acceleration factor used for controlling training velocity of mechanical
legs terminal.
16.3 Trajectory Planning and Smooth Motion
In order to obtain LLRR training trajectories smooth and flexibility, the velocity and the
acceleration of the endpoint of the mechanical leg should be continuous. However, the
realization of the endpoint motion is through the control of the LLRR mechanism leg
joints. It is necessary to map movement of the end point in the Cartesian coordinate into
joints space to get each joint angular velocity, angular position and angular acceleration.
The linkage model of LLR-Ro mechanism leg is built as shown in Figure 16.4.
Figure 16.4 Linkage model of LLRR mechanical leg
Hip joint axis, knee joint axis and ankle joint axis are placed at point O, A and B, respectively. Besides, P represents end point of mechanical leg; li (i = 1, 2, 3) expresses length
of thigh, calf and foot; θi (i = 1, 2, 3) represents the angular position of three joints; the
joint axis of hip joint is located at the base coordinate system. x0 represents the horizontal
direction. y0 represents the vertical direction. In the below trajectory planning, as the coordinate of the end point P is almost same with the point B. So, the movement of ankle will
be planned separately. Then target path is the position of the point B and the coordinate of
point B can be calculated easily as below:

xB = l2 cos(θ1 + θ2) + l1 cos θ1
yB = l2 sin(θ1 + θ2) + l1 sin θ1
(16.1)
16.3.1 Design of Training Velocity and Acceleration with Linear Path
The displacement of the end point in the direction of the line path is designed to meet a
quintic polynomial. It describes the relationship between the displacement in the direction
of the line and the time in the equation 16.2.
l(t) = a0 + a1t + a2t
2 + a3t
3 + a4t
4 + a5t
5 (16.2)
Also, constrains are given: When the time is zero, the displacement of the end point
is zero. When the time is tend, the displacement of the end point is l(tend). To make the
motion smooth, the velocity of origin and end points must be zero. To meet the continuous
Training System Design of Lower Limb Rehabilitation Robot 207
acceleration, the acceleration of origin and end points must be zero.
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
l(0) = 0
l(tend) = lend
˙
l(0) = 0
˙
l(tend)=0
¨l(0) = 0
¨l(tend)=0
(16.3)
The polynomials of the displacement, velocity, acceleration in X axis and Y axis are
obtained,
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
x(t) = l · cos(θl) + x0
y(t) = lsin(θl) + yq
x˙(t) = ˙
l · cos(θl)
y˙(t) = ˙
lsin(θl)
x¨(t) = ¨l · cos(θl)
y¨(t) = ¨lsin(θl)
(16.4)
where, θl represents angular position between line-trajectory and X-axis. From the
forward kinematics equations (16.1), we could obtain,

x˙
y˙

= J(q)
 ˙
θ1
˙
θ2

(16.5)
where,
J(q) = 
−l2 sin(θ1 + θ2) − l1 sin θ1 −l2 sin(θ1 + θ2)
l2cos(θ1 + θ2)+l1 cos θ1 l2 cos(θ1 + θ2)

The velocity of joints can be calculated,
 ˙
θ1
˙
θ2

= J−1(q)

x˙
y˙

(16.6)
The acceleration of the joint is calculated:

¨θ1
¨θ2

= J−1(q)
x¨
y¨

− J˙(q)
 ˙
θ1
˙
θ2
 (16.7)
where,
J˙(q) = 
−l1 ˙
θ1 cos θ1 − l2( ˙
θ1 + ˙
θ2)cos(θ1 + θ2) −l2( ˙
θ1 + ˙
θ2)cos(θ1 + θ2)
−l1 ˙
θ1 sin θ1 − l2( ˙
θ1 + ˙
θ2)sin(θ1 + θ2) −l2(˙
θ1 + ˙
θ2) sin(θ1 + θ2)

(16.8)
The angle of the ankle joint is changed according to the position in the training track. It
is defined by the equation below.
θ3 = lBO(t) − lBO min
lBO max − lBO min
× (θ3 max − θ3 min) + θ3 min (16.9)
208 Emerging Technologies for Health and Medicine
where, θ3 represents the ankle angle and initial position is at where the footboard is
perpendicular to the calf. Anticlockwise is the ankle joint motion as positive direction.
lBO represents the distance between ankle joint center (same with point B) and the original
point (the point O).
Then displacement of the ankle joint could be designed as below:
θ3 =
l2 + 2lx0cos(θl)+2ly0sin(θl) + x2
0 + y2
0 − lBO min
lBO max − lBO min
×(θ3 max −θ3 min)+θ3 min
(16.10)
The expression of the ankle velocity can be obtained by taking differential of equation
(16.10) above, and the ankle joint acceleration can be obtained by taking the derivative of
velocity equation.
16.3.2 Design of Training Velocity and Acceleration with Circle Path
The displacement of the end point in circle path is also designed to meet a quantic polynomial.
α(t) = a0 + a1t + a2t
2 + a3t
3 + a4t
4 + a5t
5 (16.11)
The end point need to satisfy with the constrains,
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
α(0) = 2π
α(tend) = αend
α˙(0) = 0
α˙(tend)=0
α¨(0) = 0
α¨(tend)=0
(16.12)
The polynomials of the displacement, velocity, acceleration following the X axis and Y
axis are obtained,
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
x(t) = rcos(α(t)) + x0
x˙(t) = −rα˙(t) sin(α(t))
x¨(t) = −rα˙(t) ˙α(t) cos(α(t)) − rα¨(t)sin(α(t))
y(t) = r sin(α(t)) + y0
y˙(t) = rα˙(t)cos(α(t))
y¨(t) = −rα˙(t) ˙α(t)sin(α(t)) + rα¨(t)cos(α(t))
(16.13)
The joints angular position, angular velocity and angular acceleration of the knee and
hip could be solved by inverse kinematics relationship which is same with the solution
method at straight-line trajectory. The ankle joint will be defined as below:
θ3 = θ3 max−
r2 + x2
0 + y2
0 + 2rx0cos(α(t)) + 2ry0sin(α(t)) − lBO min
lBO max − lBO min
(θ3 max−θ3 min)
(16.14)
Training System Design of Lower Limb Rehabilitation Robot 209
16.3.3 Design of Training Velocity and Acceleration with Arbitrary Trajectory
The arbitrary trajectory is made by connecting the points with lines. The time is defined
between the two adjacent points in the arbitrary path,
tn = ln
m
k=1
lk
T (16.15)
where, ln represents the distance of the two adjacent points; tn represents the motion
time between the two adjacent points; T represents the whole motion time of the arbitrary
trajectory.
The displacement, velocity and acceleration are defined at X axis and Y axis, and that
curve will be divided into many small curves by the intermediate points. Taking the X axis
as an example, the points’ displacement xn in the direction of X axis is combined with its
corresponding time tn,
(x1, t1),(x2, t2), ...,(xn, tn), ...,(xm−1, tm−1),(xm, tm)
Displacement function between two adjacent points is a polynomial. The polynomials
of first small curve and the last small curve are quartic polynomial, and the rest polynomials
are defined cubic polynomial. To make the velocities are smooth and continuous, the
velocities at the beginning point and end point are required to be zero, and the velocity of
the intermediate adjacent points is required to be equal to its previous adjacent point. Also,
in order to make the acceleration smooth and continuous, the acceleration at the beginning
point and end point are required to be zero and the acceleration of the intermediate adjacent
points is required to be equal to its previous adjacent point. These constraints can be used
to obtain the expressions of the small curves.
The expression of ankle joints displacement could be obtained separately as below,
θ3 =
x2(t) + y2(t) − lBO min
lBO max − lBO min
× (θ3 max − θ3 min) + θ3 min (16.16)
16.3.4 The Analysis of Ambiguous Points
The motion range of knee joint’s angle is from −1200 to 00. When the θ2 approaches to
00, the robot is close to its singularity, as the velocity calculated by J−1(q) of knee joint
is infinite. Thus, the constrain θ2 ≤ −100 is added in the path planning, considering the
actual position of calf will become collinear with the thigh when θ2 is close to 00.
When θ2 is under the value −100, the knee’s velocity is calculated by J−1(q). When θ2
is −100 ≤ θ2 ≤ 00 , we can plan the displacement, velocity and acceleration of the knee
joint directly, instead of J−1(q), to train the calf. However, it is not necessary for knee
joint in the motion of the circle path.
16.3.5 The Simulation of Training Velocity and Acceleration in the Planning
Trajectory
As paper space is limited, this paper displaced the simulations of the linear trajectory and
the arbitrary trajectory. l1 equals 390mm, l2 equals 295mm and the whole time cost 5s.
1) The planning of the arbitrary trajectory: The initial point, the intermediate points
and the end point is shown in Figure 16.5.
210 Emerging Technologies for Health and Medicine
Figure 16.5 Comparison between the original path and the new path planned
Applying the design of training velocity and acceleration with arbitrary trajectory, the
displacement, velocity and acceleration of the new path in the direction of X axis and Y
axis are displayed in the Figure 16.6, Figure 16.7 and Figure 16.8. Obviously, we can find
that the displacement, velocity and acceleration are continuous.
Figure 16.6 The angular position of the end point at X axis and Y axis
Figure 16.7 The velocity in the direction of X axis and Y axis
Training System Design of Lower Limb Rehabilitation Robot 211
Figure 16.8 The acceleration in the direction of X axis and Y axis
The displacement, velocity and acceleration of each joint are calculated as shown in
Figure 16.9, Figure 16.10 and Figure 16.11. The results show that the angular position and
velocity of joints can be smooth and the acceleration is continuous after interpolation for
arbitrary trajectory.
Figure 16.9 The angular position of three joints
Figure 16.10 The angular velocity of three joints
212 Emerging Technologies for Health and Medicine
Figure 16.11 The acceleration of three joints
2) The planning of the linear trajectory: Through workspace analysis of the linkage model, a linear trajectory is designed from coordinate (362.16, 131.36) to coordinate
(758.69, -30.66). Applying the design of training velocity and acceleration with linear trajectory, the displacement, velocity and acceleration of the new path in X axis and Y axis
are displayed in Figure 16.12, Figure 16.13 and Figure 16.14. Obviously, we can find that
the displacement, velocity and acceleration are continuous.
Figure 16.12 The angular position curves at X axis and Y axis
Displacement, velocity and acceleration of each joint are obtained as shown in Figure
16.15, Figure 16.16 and Figure 16.17. Based on analysis of the above results, angular
position and velocity of each joint are smooth and the acceleration is continuous after
interpolation for arbitrary trajectory.
16.4 Virtual Reality Training System
To cooperate with the VR software and simulate the riding body feeling, a VR training
system was designed. VR training is an improved kind of active training, it includes intention judgment, adapting training posture and interaction control strategy. The intention
judgment is similar to the normal active training, and more details are shown in paper [20].
Training System Design of Lower Limb Rehabilitation Robot 213
Figure 16.13 Angular velocity curves at the line, X axis and Y axis
Figure 16.14 The acceleration in the direction of the line, X axis and Y axis
Figure 16.15 The displacement of three joints
16.4.1 Design of Intention Judgment of Patients
Lagrange dynamics method was used to solve the inverse problem of the mechanical leg,
and the joint torque was obtained in real time to judge the patient’s movement intention.
214 Emerging Technologies for Health and Medicine
Figure 16.16 The velocity of three joints
Figure 16.17 The acceleration of three joints
The general equation of dynamics was obtained,
H(θ)θ
¨+ C(θ, ˙
θ)˙
θ + G(θ) = τ (16.17)
θ represents the angular position of the joints; τ represents the joint torque; H(θ) represents the inertia matrix; C(θ, ˙
θ) represents the centrifugal force and Coriolis force related
term matrix; G(θ) represents the gravity terms matrix.
To achieve active rehabilitation training for patients, it must be considered that the impact of lower limb gravity on the mechanical leg joint torque. In this chapter, referring to
the study of robotic statics, the patient’s lower limb is reduced to a two-bar linkage model.
The gravity of foots is concentrated at the ankle joint, and the direction of force is vertical. Refer to linkage model of the mechanical legs in Figure 16.4, the equation could be
obtained according to the principle of leverage,
m1gR1 cos θ1+m2g [l1 cos θ1 + R2 cos(θ1 + θ2)] = (F0−m3g) [l1 cos θ1 + l2 cos(θ1 + θ2)]
(16.18)
mi represents the quality of the patient’s leg; li represents the length of the patient’s leg;
θi represents the angular position of the joints; Ri represents the distance from the center
of the patient’s leg to the joint; F0 represents the end force when the patient relaxes.
Training System Design of Lower Limb Rehabilitation Robot 215
And the force vector F of lower limb to the leg end could be obtained:
F =
 0
m3g + m1gR1 cos θ1+m2g[l1 cos θ1+R2 cos(θ1+θ2)]
[l1 cos θ1+l2 cos(θ1+θ2)] 
(16.19)
The joint torque τ0 from terminal force be calculated easily as below,
τ0 = JT (θ)F (16.20)
JT (θ) is the force Jacobian matrix of the mechanical leg model.
Finally, the human-machine dynamics equation, when patient’s lower limb freely placed
on LLRR, are obtained,
H(θ)θ
¨+ C(θ, ˙
θ)˙
θ + G(θ) = τ − JT (θ)F (16.21)
According to the equation above, the real-time torque of the mechanical leg and the
patient’s lower acting on each joint can be determined while the patient does not have an
active exercise intention. The real-time torque of the joint and the measured data of the
torque sensor can be used to complete the active training control.
16.4.2 Design of Adapting Training Posture Function
Based on the research of bike mechanism and riding body posture, as shown in Figure
16.18, adapting posture function was built.
Figure 16.18 Riding body posture
The function can provide individual terminal trajectory according to the different legs
length of patients. Based on alternative tracks in workspace, it could select suitable track
for patients which could make training closer to real bike riding (Figure 16.19).
16.4.3 Interaction Control Strategy
Interaction control strategy is a necessary link between the robot and the VR software, and
it mainly contains the model synchronization and feedback terrains. The strategy block
diagram of interaction control is shown in Figure 16.20.
216 Emerging Technologies for Health and Medicine
Figure 16.19 Calculated circular trajectory
Figure 16.20 Interaction control strategy
Comparing the sensors data with calculated data, the patient movement intention data
could be determined. The torque intention is defined as the judge factor, and the robot will
begin to run with default terminal velocity when judge factor exceeds the preset threshold.
The pressure intention works as an acceleration factor when it exceeds the threshold, and
there is a linear relationship between the factor and the value added on default velocity.
Final terminal velocity is used for LLRR control, and it is sent to VR software for model
action synchronization. Meanwhile, there are some different feedback terrains set up in the
VR riding game, such as hill and obstacle road. When the virtual character model in those
terrains upon, the robot will receive the feedback signal from the VR software and then
change the mechanical legs running speed depending on different conditions.
16.5 Virtual Reality Software Design
16.5.1 Virtual Scene Build
Based on the game development engine Unity3D, the VR riding game was built. The
planning of the virtual scene not only meets the requirements of exercise intensity for
rehabilitation training, but also could stimulate the nervous system of the patients and has
Training System Design of Lower Limb Rehabilitation Robot 217
a good influence on psychological of patients. This scene properly meets the outdoors
walking desire of the patient with walking problems.
An outdoor riding match scene with green background tone and plenty sunlight is shown
in Figure 16.21, and it could provide a relaxing virtual environment for patients.
Figure 16.21 Match scene in game
Riding game has 4 character models, except one controlled by the patient, 3 models
are NPC (Non-Player Character) with different actions controlled by the computer. Multicharacters could avoid loneliness, while match training has properly entertainment and
competitiveness.
The whole road is about 600 meters, a single match will take 2 or 3 minutes. There
are 2 obstacle areas and a hill on the road, and the stimulus strength of training could be
changed when the model goes through these areas.
16.5.2 Game Function Design
VR game software cannot run without scripts, and each component or model in the game
need a relative script at least. The main scripts are shown as follows.
1. User Interface: Used for game start, level select, software close and other buttons.
2. Model synchronization: Control the virtual legs move same as the real legs based on
robot terminal velocity.
3. Model movement: According to terminal velocity, calculate speed and control model
move forward along straight road.
4. NPC action: Control models run in preset parameters when game start, and NPC
could run in 4 levels speed based on the difficulty choice in the title screen.
5. Feedback trigger: Constantly monitor positions of 4 models, if a model enter the
feedback areas it will send signal to robot until the model leave areas.
6. Signal I/O: Build temp files used for writing and loading signal by robot and software.
7. Pause and timer: Game and robot could be paused at any time when training start, it
could also record the time since the game start.
218 Emerging Technologies for Health and Medicine
Figure 16.22 First-person perspective
8. Camera: Game screen could be switched between the first-person perspective and the
third-person perspective when the game start (Figure 16.22).
The connections between scripts and models were built, and scripts working condition
in the simple scene was tested as shown in Figure 16.23. After debugging, all components
were imported into the completed scene.
Figure 16.23 Function test
Training System Design of Lower Limb Rehabilitation Robot 219
16.6 Virtual Reality Training Experiment
16.6.1 Model Synchronization Test
The movements of mechanical legs and virtual legs were recorded through the video. Due
to the deflection between the training posture and the real riding posture, the movements
are not exactly same as shown in Figure 16.24. But the time costs of both legs reaching the
lowest point in their own circle track are same, the synchronization between the robot and
the model is properly achieved.
Figure 16.24 Screenshot of synchronization test
16.6.2 Feedback Terrains Test
The sensors data is set as a fixed value, and the robot terminal velocity is recorded when the
model goes through the feedback terrains in the game. The recording data was transformed
into a graph shown in Figure 16.25.
Figure 16.25 Feedback terrains test
220 Emerging Technologies for Health and Medicine
The result shows that the patient need to pay more efforts or reduce the training speed
when the character model goes into the difficult feedback areas, and the opposite effect
will occur in easy areas.
16.7 Conclusion
Based on the lower limb rehabilitation robot, a virtual reality training system with competitive game was designed, which could simulate the bike riding and encourage patients to
join in the recovery training. LLRR could achieve three types of trajectories and each one
is smooth and continuous. It can realize movement synchronization between the robot legs
and the virtual model, and the robot can vary terminal velocity according to the signal of
feedback terrains in the game. The system can select the suitable training trajectory based
on the legs length of patients before training, and the training can be paused by patients or
doctors at any time while training. Doctors could switch VR training difficulty according
to the recovery of patients, and the recovery could be reflected through the timer function.
Contributions
Applying virtual reality technology in active training can greatly mobilize the enthusiasm
of patient training, and doctors can regard the virtual training situation as a basis to evaluate
the rehabilitation situation. This study provides a template for the follow VR technology
research in the field of rehabilitation robots. The team may later upgrade the virtual training
experience and will introduce VR glasses to enhance the visual experience, and add virtual
training scene such as high jumps or pedal boats. It will bring changes in the field of
rehabilitation, if the idea above could be achieved, and the study of this article is necessary.
ACKNOWLEDGEMENTS
This work was developed with the support of ”Joint Laboratory of Intelligent Rehabilitation Robot” collaborative research agreement between Yanshan University, China and
Romanian Academy by IMSAR, RO by China Science and Technical Assistance Project
for Developing Countries (KY201501009).
REFERENCES
1. Feng Y.F., Wang H.B., Lu T.T., et al, Teaching training method of a lower limb rehabilitation
robot, Int. J. Adv. Robot Syst., vol. 13, pp. 1-11, February 2016.
2. Ozkul F., Barkana D.E., Upper-extremity rehabilitation robot RehabRoby: methodology, design, usability and validation, Int. J. Adv. Robot Syst., vol. 10, pp. 1-13, October 2013.
3. Wei W.Q., Hou Z.G., Cheng L., et al, Toward patients’ motion intention recognition: dynamics
modeling and identification of iLegan LLRR under Motion constraints, IEEE Transactions on
Systems Man & Cybernetics Systems, vol. 46, no. 7, pp. 980-992, Jul. 2016.
4. Fouad K., Krajacic A., Tetzlaff W, Spinal cord injury and plasticity: opportunities and challenges, Brain Research Bulletin, vol. 84, no. 4, pp. 337-342.
Training System Design of Lower Limb Rehabilitation Robot 221
5. Behrman A.L., Harkema S.J., Locomotor training after human spinal cord injury: a series of
case studies, Physical therapy, vol. 80, no. 7, pp. 688-700.
6. Fouad K., Tetzlaff W., Rehabilitative training and plasticity following spinal cord injury, Experimental Neurology, vol. 235, no. 1, pp. 91-99.
7. Saini S., Rambli D.R.A., Sulaiman S., et al, A low-cost game framework for a home-based
stroke rehabilitation system, International Conference on Computer & Information Science,
vol. 1, pp. 55-60.
8. Holden M.K., Virtual environments for motor rehabilitation: review, Cyberpsycholo. Behav.,
vol. 8, pp. 187-211.
9. Schnauer C., Pintaric T., Kaufmann H., Full body interaction for serious games in motor rehabilitation, Augm. Human Int. Conf., Ah 2011, Tokyo, Japan, pp. 1-8.
10. Vladareanu L., Velea L.M., Munteanu R.I, Curaj A., Cononovici S., Sireteanu T., Capitanu L.,
Munteanu M.S., Real time control method and device for robots in Virtual Projection, patent
EPO-09464001, 18.05.2009, EP2105263. Patent OSIM 123527/30.04.2013
11. Victor Vladareanu, R.I. Munteanu, Ali Mumtaz, F. Smarandache and L. Vladareanu, The optimization of intelligent control interfaces using Versatile Intelligent Portable Robot Platform,
Procedia Computer Science 65 (2015): 225-232, ELSEVIER.
12. Wang H.B., Zhang D., Lu H., Feng Y.F, Xu P., Mihai R.V., Vladareanu L. Active training
research of a lower limb rehabilitation robot based on constrained trajectory, ICAMechS, Beijing, China, pp. 24-29, August, 22-24, 2015.
13. Melinte O., L. Vladareanu, R.A. Munteanu, and Hongnian Yu. ”Haptic Interfaces for Compensating Dynamics of Rescue Walking Robots”, Procedia Computer Science 65 (2015): 218-224.
ELSEVIER.
14. L. Vladareanu, D. Mitroi, R. I. Munteanu, Shuang Chang, Hongnian Yu, Hongbo Wang, V.
Vladareanu, R.A. Munteanu. , O.n Melinte, , Zeng-Guang Hou , X. Wang , G. Bia , Yongfei
Feng and E. Albu, Improved Performance of Haptic Robot Through the VIPRO Platform, Acta
Electrotehnica, vol. 57, 2016, ISSN 2344-5637
15. Vladareanu V., I. Dumitrache, L. Vladareanu, I. S. Sacala, G. Ton, M. A. Moisescu Versatile
Intelligent Portable Robot Platform applied to dynamic control of the walking robots, Studies
in Informatics and Control 24(4):409-418. December 2015, ISSN: 1220 1766, 2015
16. Krebs H., Celestino J., Williams D., et al, 24 A Wrist Extension for MITMANUS, Advances
in Rehabilitation Robotics, pp. 377-390.
17. Loureiro R., Amirabdollahian F., Topping M., et al, Upper Limb Robot Mediated Stroke Therapy GENTLE/s Approach, Reading: Autonomous Robots, vol. 15, no. 1, pp. 35-51.
18. Furusho J., Li C.Q., Yamaguchi Y., A 6-DOF rehabilitation mechanical for upper limbs including wrists using ER actuators, Mechatronics and Automation, 2005 IEEE International
Conference IEEE, vol. 2, pp. 1033-1038.
19. Wei W., Guo S., Zhang W., et al, A novel VR-based upper limb rehabilitation robot system,
ICME International Conference on Complex Medical Engineering IEEE, pp. 302-306.
20. Yongfei Feng, Hongbo Wang, Tingting Lu, Victor Vladareanu, Qi Li and Chaosheng Zhao.
Teaching Training Method of a Lower Limb Rehabilitation Robot. Int J Adv Robot Syst, 2016.
vol. 13, no. 2, pp. 1-11.
Part IV
INTERNET OF THINGS
TECHNOLOGIES AND
APPLICATIONS FOR HEALTH
AND MEDICINE
Dac-Nhuong Le et al. (eds.), Emerging Technologies for Health and Medicine, (223–284) 
© 2018 Scrivener Publishing LLC
225
Dac-Nhuong Le et al. (eds.), Emerging Technologies for Health and Medicine, (225–284) 
© 2018 Scrivener Publishing LLC
CHAPTER 17
AUTOMATION OF APPLIANCES USING
ELECTROENCEPHALOGRAPHY
Shivam Kolhe1, Dhaval Khemani1, Chintan Bhatt1, and Nilesh Dubey1
1 Charotar University of Science And Technology, Changa, Gujarat
Emails: 15ce054@charusat.edu.in, 15ce051@charusat.edu.in, chintanbhatt.ce@charusat.ac.in, nileshdubey.ce@charusat.ac.in
Abstract
Brain Computer Interface (BCI) is one of the new emerging field in which a direct
communication pathway is established between a human or animal brain and any outside
or external device. The two way BCI’s allow the brain and the external devices to exchange
signals in both the directions. But until today we have been successful in establishing one
way BCI’s. In future, we will be able to use two way BCI’s effortlessly. The best is yet
to come. In this chapter, an introduction to the BCI technology is given, the different
signals generated by the brain are stated, also brain anatomy is explained. In addition,
how are brain signals generated by the brain, how does BCI system work, a method to
perform Electroencephalogram, how are those brain signals detected is explained and also
BCI classes are stated and introduced.
Keywords: Internet of Things (IoT), Brain Computer Interface (BCI), Electroencephalography / Electroencephalogram (EEG)
226 Emerging Technologies for Health and Medicine
17.1 Introduction
There may not be a drastic increase in the population but there is surely massive change in
the number of devices people are using. The Internet is also evolving and so is its usage.
So with the increase in both electronic devices and the internet, a new field is born and
that is ”Internet of Things”. The Internet of Things is a newly emerging and we can say
evolving field. Internet of Things is a system of interrelated computing devices and objects
which we use in our daily lives and when connected within a network gain the ability to
communicate with each other without requiring human to human or human to computer
interaction.
Till now we were living in the Information Era, but now we have left it far behind and we
are now living in the Technological Era. Computers are becoming smarter, powerful and
cheaper in cost. Now the molecular computer is expected to accelerate this trend. One time
will come when these computing machines and sensors will move to each and every object
we have or use in our daily lives. Even our bodies will also be connected to the internet.
Just imagine the world where every object will interact with other and with humans. This
time is coming soon. By 2020, we will surely be living in technology. We will have
to harness the power of Internet of Things. So we can technically say that the Internet
of Things Era has begun. A new massive wave is coming and going towards connected
cars, smart houses, health monitors, wearable, smart cities. Basically a connected life.
According to the report, by 2025 connected devices count will reach to 1 trillion. In IoT the
unstructured machine generated data is being collected by the sensors, it is then properly
analyzed and then used for the desired purpose. A thing in the Internet of Things can be
anything like it can be an implant in the heart of the person for heart rate monitoring, it can
be a be tracker implanted in the belt of a pet or it can be a coffee maker machine which is
smart enough to automatically works as human wants without his/her efforts.
Applications of Internet of Things are: Smart Homes, Wearable, Connected Cars, Industrial Internet, Smart Cities, in Agriculture, Smart Retail, Energy Engagement, IoT in
Poultry, IoT in HealthCare.
17.2 Background, History and Future Aspects
The Brain Computer Interface is a direct communication between the brain and any external device. The brain signals travel from brain to the computer directly instead of traveling
through the neuromuscular system to the body parts. In earlier days the brain computer
interface devices or the electrodes were implanted in the brain but nowadays non-invasive
techniques are used which are directly placed on the scalp to control the external devices.
The Brain Computer Interface devices nowadays require effort but in future, these devices
are expected to work effortlessly. This field is the combination of the fields like Electrical
engineering, computer engineering, biomedical engineering and neuroscience or neurology.
Hans Berger worked in the human brain research field and its electrical activity and by
his innovation Brain Computer Interface was discovered. Hans Berger thus developed the
new field called Electroencephalography. Thus he is known as the father of Electroencephalography. His research made it possible to detect brain diseases. He was inspired by
Richard Canton’s discovery of electrical signals in brains of animals in the year 1875. In
the year 1998, Philip Kennedy implanted the first brain computer interface device into the
human brain. In 2003, a first BCI game called BrainGate was designed John Donoghue
Automation of Appliances Using Electroencephalography 227
and his team. In June 2004, Matthew Nagle became the first human to be implanted with
BCI devices (BrainGate BCI devices) to restore functionality he lost because of paralysis.
John Wolpaw demonstrated the ability to control a computer using a BCI in December
2004. In his study electrode cap was placed on the scalp to capture EEG signals This field
will be greatly developed in the future. Many developments have been taking place in this
field nowadays. Future of BCI will be like a man will be able to control and manipulate
the outside objects by their mind. Man will be able to control natural as well as complex
motions of everyday life. The mobility functions lost during paralysis or any accidents will
be restored perfectly. Mental health problems will be instinct.
In this chapter main focus is on a new life changing technology called ”Brain Computer
Interface (BCI)”. It is based on a test called ”Electroencephalography” (EEG) that meters
and maps the brain’s electrical activity. The electrodes are connected to your scalp and
the system is wired to a computer, the signals are analyzed and translated into actions
and instructions that are used to drive the computer and this data can be used to perform
different tasks. The first EEG was recorded by Hans Berger in 1929 on animals. Instead
of using keyboard and mouse as the input methods, by using BCI we will be able to give
input using our brain. There are many applications of BCI depending on your thinking.
The signals are processed and integrated and then the impulses are given back to actuators.
This works just like your body e.g. when we touch anything the fingers work as sensors
which send the data to the brain to process it and then brain resends the processed data to
fingers.
The BCI was also not only studied on humans but on animals also. A monkey was able
to control a robotic hand using this technology. BCI will help us understand how and what
animals think and also how their brain performs. A time will come when animals will be
able to interact with humans. BCI includes the study of brain wave patterns of various
people. When that will be done perfectly humans will be able to communicate with their
brains. The BCI can lead to various applications. This technology is still in progress and
BCI tools are also limited.
17.3 Brain with Its Main Parts and Their Functions
The brain is the most complex and important organ of the human body. No other thing on
this planet can be compared with the human brain. The brain performs the physiological
tasks like receiving information from the rest of the body, interpreting the information and
then assisting the body to work according to that information. Our body is embedded
with natural sensors like eyes, ears, nose, skin, and tongue which give inputs to the brain
like light, sounds, odors, pain, and taste. The brain interprets these inputs. The brain
also helps perform operations like breathing, releasing hormones, maintaining balance and
blood pressure, thoughts, movement of the body (arms and legs), memory and speech. The
brain controls all the functions of the body and works like a network that transfers messages
to different parts of the body. An average brain weighs approximately 3 pounds. The brain
is protected by the bones called a skull. Meninges are the cushion layered membrane which
along with the cerebrospinal fluid protects the brain.
The nervous system in the human body is divided into two main parts: Central Nervous
System, and
228 Emerging Technologies for Health and Medicine
17.3.1 Central Nervous System
The Central Nervous System consists of two main parts that are brain and spinal cord. The
human brain is divided into 3 parts:
Figure 17.1 Brain Anatomy
Fore Brain: is the largest part of the brain, most of which is made up of Cerebrum also
called Telencephalon and Diencephalon. Fore brain contains information related to human
intelligence, memory, personality, emotions, speech, ability to feel the mood.
The cerebrum is divided into two parts/hemisphere:
Left Hemisphere: is considered to be logical, analytical and objective. It controls
voluntary limb movements on the right side of the body.
Right Hemisphere: is thought to be more intuitive, creative and subjective. It controls limb movements on the left side of the body.
The cerebral hemispheres are hollow from inner side. Walls of cerebral hemisphere
have two regions; the outer cortex is the ’Gray Matter’ and the inner White Matter. Gray
matter is folded to form the coil like structure and the folds are called gyri and the grooves
or canal like structure are called sulci. These sulci and gyri increase the surface area to
accommodate more neurons. Thus, it is believed that large number of convolutions in the
human brain indicate greater intelligence.
Each hemisphere is divided into 4 lobes which are interconnected:
Frontal Lobes: The frontal lobes are located in the front side of the brain. They control
the organizing the memory, movement, processing of speech and mood.
Parietal Lobes: The location of parietal lobes is behind the frontal lobes and above
occipital lobes. These lobes handle the sensory information such as taste, pain, temperature, and touch.
Temporal Lobes: The temporal lobes are situated on each side of the brain. They deal
with processing of memory, speech, hearing information and also language functions.
Automation of Appliances Using Electroencephalography 229
Occipital Lobes: Their location is at the rear side of the brain. They deal with processing of visual information.
The Diencephalon is the posterior part of the forebrain. It contains structures such
as Thalamus, Hypothalamus, and Epithalamus. Thalamus is located at the base of the
hemispheres. It relays sensory impulses such as pain to the cerebrum. Hypothalamus is
located below the thalamus and it regulates autonomic functions such as thirst, appetite
and body temperature. The function of Epithalamus is to connect the limbic system to
other parts of the brain. It also secrets melatonin by the pineal gland and regulates motor
pathways and emotions.
Mid Brain: The mid brain acts as the master coordinator of all the messages going in
and coming out of the brain to the spinal cord. It is located underneath the middle of the
fore brain. It is also known as mesencephalon. It connects the forebrain and hindbrain.
Mid brain consists of cranial nerves which control the reflexes involving eyes and ears.
Hind Brain: The hind brain also called rhombencephalon, is a brain stem connecting
the brain with the spinal cord. It is composed of the metencephalon and the myelencephalon. The metencephalon contains structures such as the pons and cerebellum while
the myelencephalon contains medulla oblongata.
Cerebellum: Cerebellum is located just behind the cerebrum, above the medulla oblongata. It is divided into two hemispheres. Each hemisphere has a central core made up of
white matter and an outer region made up of gray matter. The main function of the cerebellum is to maintain the balance of body, coordinate muscular activity, motion, learning
new things. Thus we can walk without falling.
Pons: is the broad horse shoe shaped mass of nerve fibers. It is located below the
cerebellum and it serves as a bridge between mid-brain and the medulla oblongata. It is
also the point of origin or termination for four of the cranial nerves that transfer sensory
information and motor impulses to and from the facial region and the brain.
Medulla Oblongata: Myelencephalon or the medulla oblongata is the lowest part of the
brain stem and is continuous posteriorly with the spinal cord. It has a central core made up
of gray matter. It contains several functional centers that control autonomic nervous activity regulates respiration, heart rate, and digestive processes. Other activities of the medulla
include control of movement, relaying of somatic sensory information from internal organs
and control of arousal and sleep.
17.3.2 Peripheral Nervous System
This consists of many nerves which are spread throughout the body. There are two types
of nerves and they are:
Sensory Nerves: The sensory nerves carry messages from sensors to the brain.
Motor Nerves: The motor nerves carry messages from brain to the body. They carry
instructions from the brain to the body and what action to take.
An example for this is when you eat a chilly your sensory organ tongue carries the taste
data to the brain and then brain resends the processed data to the body that spit out the chili
to avoid any further damage. And this process is very fast.
The nerves are not directly connected to the brain but they are connected to the spinal
cord which indirectly connects the nerves with the brain.
230 Emerging Technologies for Health and Medicine
Figure 17.2 Nervous System
17.3.3 How are The Brain Signals Generated
In the term Electroncephalogram; Electro stands for electrical, Encephalon stands for brain
and Gram or Graphy stands for a picture. Neuron communicate electrically or using neurotransmitters. EEG measures the summation of electrical activity on the scalp primarily
derived from post-synaptic activity round the dendrites of pyramidal neurons in the cerebral cortex. Neurons communicate by passing an electrical signal by the movement of ions
flowing in or out of the cell.
Figure 17.3 Neuron
Automation of Appliances Using Electroencephalography 231
Parts of Neuron are: Nucleus, Dendrites, Myelin Sheath, Axon, Axon terminals. First,
the electrical signals are transmitted from the dendrites to the cell membrane where they
meet axon hillock. Axon Hillock is the gate keeper through which the signal can pass to the
axon. Here the summation of all the charges is used by axon hillock to decide whether or
not the signal should be passed to the axon terminal. The axon hillock is the part where the
summation of the excitatory post-synaptic potential and inhibitory post-synaptic potentials
meet.
If the summation of these potentials reaches the threshold voltage the signal passes.
A neuron is like a battery with its own separate charges. Outside there are positive
sodium ions lingering outside the membrane. And inside there are positive potassium ions,
they are also positive but are mingled with the negatively charged protein. So the cell interior has overall negative charge. This state is called Polarized state. This is the resting state
of the neuron. A neuron has the resting membrane potential of about -70mV. Cell membrane contains voltage gated channels. They allow either sodium or potassium ions to pass
through. When any message arrives a neuron the voltage gated sodium channels open and
the sodium ions enter the cell membrane which decreases the negative voltage. Because
of this, more voltage gated sodium channels will open causing more sodium ions to enter
the cell membrane and the membrane potential becomes more positive or depolarizes and
reaches up to +40mV. This occurrence is called Action Potential. A signal electrical event
isn’t big enough to be detected by EEG and the action potentials can cancel each other out.
So the Pyramidal Neurons come in to picture.
Figure 17.4 Pyramidal Neuron
They are found within the most superficial layers of the brain and they are spatially
aligned. Therefore, their activity is synchronous that produces a larger signal which will
be measured superficially from the scalp. Axons from neighboring neurons synapse with
the pyramidal neurons.
Figure 17.5 Pyramidal Neuron Chain
232 Emerging Technologies for Health and Medicine
Figure 17.6 Synapse with Pyramidal Neuron
17.3.4 What is Neuron Synapse?
Synapse: Synapse is the meeting point between two neurons. A neuron is of no use if
nothing is connected to it. So the communication among neurons is taken care by Synapse.
In Greek, Synapse means to join. An action potential transmits an electrical message to
the end of an axon. The electrical message then strikes a synapse that then converts it into
another type of signal and transfers it to the neighbouring or other neuron.
Figure 17.7 Neurotransmitters
Each synapse acts like a minute computer. It is able to change and adapt in response to
the neuron firing patterns. Synapses are what allow you to learn and remember. They are
the reasons why psychiatric disorders arise like drug addiction. Synapses have two modes
of communication, electrical and chemical. Electrical synapse works like broadcasting
the signals and one synapse can activate thousands of different other cells such that all
cells can act in synchrony. While chemical synapse is slower but more precise and more
selective. They use neurotransmitters or chemical signals. Chemical signals can convert
electrical to chemical and chemical to an electrical signal which allows different ways to
control that impulse. The cell that sends a signal is called pre-synaptic neuron. The presynaptic terminal is filled with thousands of neurotransmitters. Receiving cell is called
post-synaptic neuron. They are present on the body of the cell. Its function is to accept
neurotransmitters. Chemically gated ions on the post-synaptic membrane open in response
to increasing of neurotransmitters that bind to the proteins.
When the depolarization begins at one end of the neuron the other end repolarises back
to -70mV thus creating a dipole of the neuron and conducting a current.
Automation of Appliances Using Electroencephalography 233
Figure 17.8 Neuron Dipole
Dipole: A dipole is a part of equal and oppositely charged or magnetized poles separated
by a distance.
In Neurology: EEG signals are derived from the net effect of ionic currents flowing in
the dendrites of the neurons during synaptic transmitters. Any electric field produces a
magnetic field which can be measured. The net current can be thought as current dipole
i.e. currents with a position, orientation, and magnitude. All post-synaptic potential will
contribute to the EEG signals. Every post-synaptic potential causes the charge inside the
neuron to change and the charge outside the neuron to change in opposition. Electrical
dipole from a single cell is undetectable because of thick skull and brain protective layers.
Thus, the summation of the dipoles created by hundreds to thousands of neurons is what is
detected by the EEG.
17.4 Working of BCI
1) Signal Acquisition and Pre-processing: First of all the EEG electrodes are implanted
in the brain either by invasive or by non-invasive techniques. The brain waves electrical
impulses are detected by the electrodes. The signals that we get have actually low signal
strength so they need to be amplified to be of use. The computer understands digital information so they need to be digitized. In pre-processing the electrical signals are recorded
and filtering is used so that the signals are properly and clearly detected.
2) Signature Extraction: Whenever the electrical signals are recorded they are not alone
but noise is also detected. Our aim is to extract some specific brain signals which can be
useful. So we need to remove that unwanted noise or signals. Because of these unwanted
signals, we may get incorrect results. So the signals are separated through the signature
extraction process. This process is also called Feature Extraction.
3) Signal Amplification: The signals that we get have the low signal strength and signals
with that much low signal strength cannot be useful. So these extracted signals are then
amplified. And then these amplified signals are used for specific purposes.
4) Signal Translation and Signal Classification: The extracted signals are then translated into their corresponding frequencies so that the user can use them directly for their
specific purposes. These waves are then classified according to their frequencies like Alpha
waves, Beta waves, Gamma waves, Delta waves and theta waves. After these processes,
the output is then shown on computer screen.
234 Emerging Technologies for Health and Medicine
Figure 17.9 Working of BCI
17.4.1 Types of Waves Generated and Detected by Brain
There are four types of brain wave namely Delta, Alpha, Beta, and Theta. For example,
when we sleep a vast number of neurons activate and all together work in synchrony and
produce Delta waves high amplitude. The brain transmits the waves in the form of electrical signals. These signals are generated when the neurons fire messages to one another.
Frequency and amplitude of the waves are directly proportional to the rate of neurons working in synchrony and transmitting the signals all together at same time. The different types
of waves are classified according to their specific frequencies and their specific activities.
These waves are listed below with their corresponding frequencies:
1. Delta Waves: 0 Hz to 4 Hz
2. Theta Waves: 4 Hz to 8 Hz
3. Alpha Waves: 8 Hz to 12 Hz
4. Beta Waves: 12 Hz to 40 Hz
5. Gamma Waves: 40 Hz to 100 Hz
Delta Waves: Delta waves are the waves having the least frequency among the five
waves and also they are the slowest recorded brain waves. These waves are mainly detected
from young kids or infants. These waves are related with the deep sleep and relaxation.
When we grow up these waves tend to decrease. People having learning disabilities and
those who cannot control their consciousness have less delta wave brain activity. Abnormal
delta wave activity is generated during brain injuries, severe ADHD, poor sleep, problems
in thinking and learning problems.
Automation of Appliances Using Electroencephalography 235
Figure 17.10 Delta Waves
Increased amount of delta waves is found during deep sleep.
Theta Waves: Theta waves are generally connected with sleep, daydreaming and with
deep emotions. They help in improving creativity and helps us feel natural. The number
of theta waves decrease during ADHD, depression, stress, poor emotional awareness, inattentiveness, etc. Theta waves are not produced an excess in waking state. Theta waves are
related to the subconscious brain state.
Figure 17.11 Theta Waves
Alpha Waves: Alpha waves lie between the conscious and subconscious state of mind.
These waves are related to deep relaxation and calm state of the brain. If we are experiencing any stress then the alpha waves are blocked. Alpha waves generation can be increased
by alcohol, drugs, antidepressants.
Figure 17.12 Alpha Waves
Beta Waves: Beta waves are commonly detected when awake. They are the highfrequency waves. These waves are associated with consciousness. They are observed
when focussed on something or during logical thinking. These waves are associated with
daily common tasks like reading, writing, focusing, critical thinking, etc. Energy drinks,
coffee, etc. can increase a number of beta waves.
Figure 17.13 Beta Waves
236 Emerging Technologies for Health and Medicine
Gamma Waves: Whenever brain performs tasks that have higher processing then these
waves are generated. These waves are important during learning and memory functions.
During learning of new things, gamma waves are involved. People having low memory
power, or if they are mentally challenged have low gamma wave brain activity. During
high anxiety, stress, depression a number of gamma waves generated are less. These waves
are known for higher alertness.
Increased amount of gamma waves are found during meditation.
Figure 17.14 Gamma Waves
17.4.2 How to Perform Electroencephalogram
Using EEG is not that simple just put on the device and read the data. Some steps are
needed to get the accurate EEG data.
Let’s talk about how to collect EEG data:
Phase A: Prepare the solution
– Step 1: Fill the vessel or a bucket with some distilled water.
– Step 2: Mix in Potassium Chloride to increase the electrical conductance. One
can also mix shampoo to soften the scalp and to decrease the electrical impedance.
Now mix it well.
Phase B: Measure the head
– Step 1: Measure the diameter and then the circumference of the head of the user
to correctly determine the EEG cap size which the user will be wearing.
– Step 2: Once the measurement is taken, select the right EEG cap and soak the cap
in the solution that is already prepared. Soak it for approximately 10 minutes.
– Step 3: Now other measurements are needed to be taken to find the exact center
of the user’s head. First measure from jawline to jawline. We may need to ask
the user to open and close the mouth to get the correct measurements. Once the
correct center is found take a note of it.
– Step 4: Now measure from Nasion (the point between the eyebrows of the user)
to the Inion. Inion is the bony projection on the back of the skull which you can
feel it with your fingers. Now once the measurement is obtained, divide it by 2 to
get the center and note that center point. The two points that are noted as above
should form an ’X’ on the user’s scalp.
Phase C: Lowering the impedance
– Step 1: Connect the cap with the electrical recording equipment. Make sure that
the user is comfortable.
Automation of Appliances Using Electroencephalography 237
– Step 2: Last step is to place the EEG cap on the participant’s head. To do so spread
your fingers on either side of the inside of the cap. There is a reference electrode
present on the center of the cap which is called ’CZ. Place this directly on the ’X’
mark that was obtained from the measurements. As you bring the cap around the
user’s head, bring down the straps onto the chin.
– Step 3: Adjust the cap so that it is properly fit using the openings that are present
at the end of the cap. More tightly the cap fits, more will be the conductance for
the electrodes and more accurate will be the results.
– Step 4: Still there can be impedance seen in the software we are using for capturing EEG signals. So, for that wiggle the electrodes. This will bring the electrode closer to the scalp. And if there are still some electrodes that are causing
impedance, then add some saline solution and then again wiggle the electrodes.
Now you are ready to ask the user to start the test and thus we can measure the
EEG data.
17.4.3 How to Take Measurements of the Head
First of all, measurement is taken from Nasion to Inion. And then this measurement is
then divided into 10 or 20% parts and this way this system gets its name. Now marks are
placed. Now measurements are taken from one pre auricular point to the other. Again
this is divided into 10-20% parts and additional marks are made. Further marks are made
around the circumference of the head and separated by 10% distance. Then para-sagittal
measurements are made separated by 25%. And then transverse measurements are made
and then at the intersection of these two lines marks are made.
Figure 17.15 Steps
When all electrodes position is marked, they are named with letters and numbers.
Figure 17.16 Electrode position
238 Emerging Technologies for Health and Medicine
Side of the scalp (head portion) is represented by the numbers. Odd numbers represent
left side and even numbers represent the right side. As the numbers get closer it mean that
the electrodes are closer to the midline. E.g. here C4 is closer to midline as compared to
T8. ”z” i.e. Zero represents the midline. Positions on the scalp are indicated by letters. In
the middle chain of electrodes, it can be seen that F means Frontal, C means Central and P
means Parietal.
Figure 17.17 Electrode Cap side view
17.4.4 How are EEG Signals Recorded
EEG is recorded using the technology called Differential Amplifier.
Figure 17.18 Differential Amplifier
It accepts two inputs and we get the output in the form of difference between the two
inputs. This is particularly useful for recording and showing very little amount of electrical
signals like of EEG. For example, consider two signals as input to Differential Amplifier.
So the output is the difference between the inputs and the remaining common portion (part)
is ignored.
Figure 17.19 Differential Amplifier Working
Automation of Appliances Using Electroencephalography 239
So, it can be said that EEG is always relative.
17.4.5 Methods to Display EEG on Screen
EEG can be displayed in different ways which are called Montages. Different types of
Montages are:
1. Bipolar Montage
2. Common Electrode reference
3. Average reference
4. Weighted average reference
5. Laplacian Montage
But in this document, most commonly used montage i.e. Bipolar Montage will be
explained. Consider a picture of the head from the top down.
If we display the difference between Fp2 and F8 as a single tracing, it would look like:
Figure 17.20 Fp2-F8 Single Tracing
And we call this single tracing a Channel or Derivation. Next, we move downwards
and take the difference between F8 and T8 and we can display this as a different channel.
Similarly, as we move downwards we get a string of recording from the front of the head
to the back.
Figure 17.21 Chain
This string of recordings is called a ”Chain”
The above chain is Right Temporal Chain.
Several chains can be put on a display together as shown below:
These above recordings are displayed as if our head position is pointing towards the
right side. So in above figure, we first see the Left Temporal Chain, then the left ParaSagittal chain, then the Midline, then the Right Para-Sagittal Chain and at last the Right
Temporal Chain.
240 Emerging Technologies for Health and Medicine
Figure 17.22 Bipolar Montage Electrodes
Figure 17.23 Anterior-Posterior Montage
17.4.6 Eye Blink EEG Patterns
One of the important parts of EEG signal recordings is the Eye blink patterns. EEG also
detects eye patterns. Each EEG has special types of electrodes to detect eye muscle movements and this is called ’Electrooculogram’(EOG). So to understand this, think about an
eye as a dipole with polarity. Our eye has a Retina and Cornea. The Retina is negatively
charged and the Cornea is positively charged. Whenever user blinks an eye the eyeball
moves upward into the head. This phenomenon is known as Bell’s Phenomenon. If we
think about the EEG electrode that is closest to the eye, the upward movement of an eye
will cause large positive signals at the frontal electrodes Fp1 and Fp2. On EEG this results
in a very large deflection on the Frontal electrodes on both sides as indicated in the snap
below:
Automation of Appliances Using Electroencephalography 241
Figure 17.24 Eye Deflection Readings
17.5 BCI Classes
There are three classes of BCI:
1) Invasive Brain Computer Interface: In the Invasive BCI technique, electrodes or
Brain Computer Interface devices are implanted onto the brain directly by performing some
surgical operations. They are meant to provide highest quality brain signals. The predominant platform used here is Cortical multi-electrode array.
Invasive BCI is again divided into two parts: Single Unit Invasive BCI and Multi unit
Invasive BCI. When signals from a single side of the brain are to be detected then these
devices are called Single Unit Invasive BCI devices. If signal detection from multiple areas
of the brain is needed, then it is called multi-unit Invasive BCI. Invasive BCI’s can be used
to restore hearing by implanting the hearing device directly onto the brain and connected
with the ear. Also, eye vision can be restored and limb movement can be restored by brain
controlled robotic arms and legs. It directly records the cortical neuron potential.
Figure 17.25 BCI methods Implantation
242 Emerging Technologies for Health and Medicine
Invasive BCI can cause neuronal damage. Neurosurgical implantation is required which
is risky if not handled with proper care. As the electrodes and BCI devices are directly in
contact with the brain, they can form scar tissue on the brain which can weaken the signals.
2) Partially Invasive Brain Computer Interface: The predominant platform used in
partially invasive BCI is ECoG (ElectroCorticoGram). Here electrodes are implanted inside the skull, above the brain and beneath the Dura Mater rather than within cerebral
cortex or within gray matter. Signals obtained from partially invasive BCI are weaker than
Invasive BCI. The advantage of [partially invasive BCI over Invasive BCI is that it has less
risk of forming scar tissue. It causes no cortical damage but surgery should be performed
with proper care. In this technique, the electrodes are covered within the thin plastic pad.
3) Non Invasive Brain Computer Interface: Among the three BCI classes, the noninvasive class gives the least quality of signals. The electrical impulses or signals coming
out from the neurons are scattered and distorted by the skull. The advantage of non-invasive
BCI over other two classes is that it is the safest option to use non-invasive BCI methods.
In this technique, the sensors or the electrodes are placed on the cap and then placed on
the head such that the electrodes touch the scalp completely to read the brain signals. The
most popular method under this category is Electroencephalography (EEG) and is cheap,
easy to use and portable. Other than EEG, other noninvasive methods Magneto-Resonance
Imaging, Single Photon Emission Computed Tomography, magnetoencephalography and
Positron Electron Tomography. Many consumer EEG BCI interfaces are available for sale.
Some of the leading hardware companies manufacturing these EEG BCI interface devices
are NeuroScan, Brain Products, BioSemi, EGI, EMOTIV, NeuroSky, Advance Brain Monitoring, AntNeuro, Neuroelectrics, MUSE, OpenBCI, Cognionics, g tec, mBrainTrain.
17.5.1 Applications of BCI
There are various applications of BCI. Some of them are as below:
1. Games and Entertainment
2. Rehabilitation and Movement Control
3. Neuroprosthetics
4. Medical Field.
5. Neuromarketing and Neuroadvertisement.
6. Communication.
7. Neuroergonomics and Smart Environment control
8. Security
9. Education
10. Self-regulation
17.5.2 Challenges BCI is facing
The major challenges that BCI is facing are:
1. Data Transmission Rate.
Automation of Appliances Using Electroencephalography 243
2. Lower Signal Strength.
3. High Error Rate.
4. Inaccuracy in Signal Classification.
5. Understanding the functions of brain areas.
6. Robust machine learning algorithms.
7. Effect of feedback.
8. Interaction of the electrode and cortical tissue or scalp.
17.6 Conclusion
With the discovery of Electroencephalography, the newly developed branch Brain Computer Interface captures the brain signals from different positions of the brain. These received signals are then translated and this processed data can be used to control anything.
In fact, the computer itself can be controlled using brain waves. BCI is the future. It will
become the new mode of communication and by this people will be able to control almost
everything. Different methods are used to detect the EEG signals from the brain but each
of these methods has their own pros and cons. This technology almost seemed impossible
before it was developed. And still, also it seems impossible to researchers because Brain
Computer Interface is capable of solving verities of issues that seem impossible. Still,
researchers are working on development in this field.
REFERENCES
1. Bhatt, C., Dey, N., & Ashour, A. S. (Eds.). (2017). Internet of things and big data technologies
for next generation healthcare.
2. Bhatt, Y., & Bhatt, C. (2017). Internet of things in healthcare. In Internet of things and big data
technologies for next generation HealthCare (pp. 13-33). Springer, Cham.
3. En.wikipedia.org. (2018). Human brain. [Online] Available at:
https://en.wikipedia.org/wiki/Human brain [Accessed 4 Apr. 2018].
4. En.wikipedia.org. (2018). Action potential. [Online] Available at:
https://en.wikipedia.org/wiki/Action potential [Accessed 4 Apr. 2018].
5. FREUDENRICH, C., & BOYD, R. (n.d.). How Your Brain Works. Retrieved April 04, 2018,
from https://science.howstuffworks.com/life/inside-the-mind/human-brain/brain.htm
6. EEG (Electroencephalogram): Purpose, Procedure, and Risks. (2018). Healthline. Retrieved 4
April 2018, from http://www.healthline.com/health/eeg#overview1
7. (2018). Retrieved 4 April 2018, from http://www.skatefins.com/wpcontent/uploads/2016/03/cerebellum-cerebrum-frontal-lobe- occipital-temporal-temporalbrain-anatomy-pons-medulla-oblongata-spinal-cord-blue-pink-green-purple-red.bmp
8. (2018). Health.ucsd.edu. Retrieved 4 April 2018, from
https://health.ucsd.edu/specialties/neuro/specialty-programs/peripheral-nervedisorders/PublishingImages/nervous-system.jpg
245
Dac-Nhuong Le et al. (eds.), Emerging Technologies for Health and Medicine, (245–284) 
© 2018 Scrivener Publishing LLC
CHAPTER 18
DESIGNING A BEAUTIFUL LIFE FOR
INDIAN BLIND PEOPLES: A SMART STICK
Aatrey Vyas, Dhaval Bhimani, Smit Patel, Hardik Mandora, Chintan
Bhatt
U & P U. Patel Department of Computer Engineering, CSPIT, CHARUSAT, India
Emails: aatrey.vyas61@gmail.com, dhavalbhimani206@gmail.com, smitnakrani60@gmail.com, hardikmandora.ce@charusat.ac.in, chintanbhatt.ce@charusat.ac.in
Abstract
The target of the proposed chapter is to fill in as a concise groundwork to make daze
individuals life more intelligent and more solid utilizing the shrewd sensors. The section
begins with an outline of the IoT and unavoidable frameworks and proceeds in talking
about the nuts and bolts of what are genuine issues looked by daze people groups to the
what are diverse arrangements. The part explains the issues looked by a white visually
impaired stick and how a smart stick can tackle that issues. Lastly, the section closes with
how brilliant sensors can make life of visually impaired individuals more agreeable and
how a visually impaired individual can live with no help by lying on the savvy frameworks.
Keywords: IoT, Unavoidable Frameworks, Blindness, Smart system, Shrewd sensors,
Visually impaired stick, Savvy frameworks
246 Emerging Technologies for Health and Medicine
18.1 Introduction
Internet of Things (IoT) is the buzz word now a day roaming here and there. But what
does it actually mean? Let’s Imagine that things around you begin to talk to you and start
providing information in a smatter way. What if your alarm clock, knows your college
location and path, knows traffic conditions, weather conditions and is learned enough to
create an estimation of your arrival time and wake you up accordingly. Wouldn’t it be
great that your coffee machine does know your arrival time and when you enter to your
home sweet home and you will be amazed by the warm smell of coffee. Yes, you are
nearer to what is Internet of things (IoT). The formal definition of Internet of Things can
be: Embedded System + Cloud. Internet of Things has a broader area of its definition.
One can notice that main aim of the Internet of Things is towards how to make human life
better and easier. As one can notice that it is being applied to all parts of life. Hospitality
to Medical. Manufacturing to Logistics. Now a day health is main sector where more and
more researches are going on. And when one talk about making life better of a human first
thing come in the mind will be What about disabled peoples? What kind of problems they
are facing? How to make their life more beautiful? How Internet of Things will be helpful?
Yes, you are thinking right. The whole chapter will talk about what is Internet of things
and how one can make the life of disabled people (Blind people) more beautiful with the
help of Internet of Things. And one simple example will be given as a small implemented
project. That is our small contribution to the field of IoT: ”Smart stick for Blind people”.
18.2 Internet of Things
Definition: Devices with sensors and that all sensors are connected with each other and
shares the information to archive certain result. If one explores the definition then connecting sensors and sharing the information doesn’t mean that it should be connected using
internet only it can be connected via any transmission media. Here the device can be any
device that you will observe from waking up to sleeping down.
The IoT (Internet of Things) is growing at a pace as consumer, businesses, and governments are noticing the benefits of connecting impotent devices to the internet. Since there
are already billions of IoT devices existing athwart industries, Integration of Artificial Intelligence is that what will bring about the real revolution, which won’t just gather the
information for investigation and improvement, yet in addition prepare those IoT gadget
to know the conduct of their client and alter itself in like manner to best sit the reason and
needs, consequently influencing them to brilliant.
Smart Refrigerator System: Smart refrigerator at your home connected to the internet.
Now imagine that your child has opened the refrigerator door and he forgets to close the
door. And you at your office get a notification that the refrigerator door is opened please
close it. By following few simple steps in mobile application you are able to close the
refrigerator by sitting at your office miles away from the home sweet home!
Face Recognition System: Face recognition system installed at your door step which
it connected to the internet. Now imagine that you are busy with your work at office and
your parents come to your home. And keys are with you but there will be no problem as
you are having the Face Recognition System. When someone will be at your door step it
will capture small video and send it to your mobile application. If you want to open the
door then you can open it while seating at the office.
Designing a Beautiful Life for Indian Blind Peoples 247
Smart Home: Wouldn’t you revere if you could switch on cooling before accomplishing home or kill lights even after you have left home? Or then again open the approaches
to sidekicks for passing access despite when you are not at home. Do whatever it takes not
to be astounded with IoT happening as intended associations are building things to make
your life less demanding and supportive. Savvy Home has transformed into the dynamic
venturing stool of accomplishment in the private spaces and it is foreseen Smart homes
will push toward getting to be as typical as phones.
Nest-Thermostat [10]: One of the principal IoT based gadgets, Nest appeared as a
savvy indoor regulator that projects itself as per your timetable and necessities utilizing its
imaginative temperature, mugginess, climate and action sensors. Other than preserving vitality and diminishing bills, Nest has now developed into a thorough home computerization
gadget with ’works with Nest’ designer program.
Activity Tracker [9]: Jawbone Up is an entire wellness following unit moved into a
wristband. Stacked with a wide range of sensors, it causes you measure steps, separate,
calories, rest, heart wellbeing, and sustenance and drink logging. It accompanies ’savvy
mentor’ aide to help survey execution. To top everything, it hosts unending availability
with third get-together applications and furthermore gives group-based information sharing.
Body Analyzer [8]: You can without much of a stretch confuse this gadget for a consistent advanced measuring scale, yet it does substantially more than that. It quantifies
weight, body structure (fat mass and BMI), heart rate, and air quality. It likewise does
program individual profiling for up to 8 people and offers information (as diagrams, charts
and so forth.) finished applications and cloud. The innovation takes a shot at 4 weight
sensors and a body-situating locator.
MakerBot Replicator Mini [11]: 3D printing is ostensibly the greatest assembling
upheaval holding up to happen. Makerbot has not just contracted the size to make it home
amicable, the forward-looking group has additionally coordinated Internet of things at the
center of this model. You can cooperate with the printer from different gadgets and make
and offer outlines autonomously or utilizing Markebot’s Printshop application.
18.3 Background
Smart Cane (Developed by IIT Delhi) [1]: For almost 60 outwardly disabled undergrads in Mumbai, identifying hindrances before them will never again require physical
contact. ’Smart canes’ created by a group from Indian Institute of Technology Delhi (IITD) group will empower the understudies to recognize open windows, electric posts, hanging branches and projecting AC units, open auto entryway at a separation of three meters
utilizing ultrasonic running sensors appended to their standard sticks.
The device gets vibrated with three different frequencies when hindrances is one, two
or three meters away. There is also one special alarm (buzzer) facility provided for fast
moving objects like car, truck etc. The product is tested live with 150 blind peoples in
India. The device is made to find hindrances from knee to head height up to 3 meters.
Device will get recharged using USB port. Device is made in such a way that it can also
identify the low occurring objects such as potholes.
Oh, it is just beginning! If we talk about the non-technical part and user experience
then the stick(cane) is facilitated by: Elegant Design, Ergonomic grip. As per IIT Delhi
site 20000+ smart cane devices are distributed, 40+ channel partners are there and in more
than 12+ countries have facility to order this.
248 Emerging Technologies for Health and Medicine
Smart stick for Blind (Developed by Shruti Dambhare and Prof. A.Sakhare) [2]: This
framework works by utilizing GPS, simulated form framework, impediment location and
voice circuit. This framework works by fitting a camera on the people head, the camera
will be use and algorithm to distinguish the highs and obstructions in front the visually
impaired individual. This framework likewise contains ultra-sonic sensors to distinguish
the obstructions, Furthermore, this framework incorporates GPS framework is to achieve
the required goal. The precision of the fake vision unit gives a high exactness yield to the
client. Be that as it may, the planning multifaceted nature of the framework makes it hard
to outline and get it.
Ultrasound running from a long stick (Directed by Professor Robert X. Gao) [3]:
Another examination in similar field to help dazzle individuals utilizes the beat resound
system keeping in mind the end goal to give notice sound when recognizing the obstructions. This method is utilized by the United States military for finding the submarines.
They utilized beat of ultrasound run from 21KHz to 50KHz which hit the hard surface to
produce reverberate beats. By figuring the distinction between signals transmit time and
flag getting time we can foresee the separation between the client and the obstructions. This
framework is extremely delicate as far as recognizing the impediments. It has a location
extend up to 3 meters and a discovery point 0 to 45 degrees. Nonetheless, this framework
should be re-intended to work with less control utilization.
18.4 Purpose Approach
This system purposes the stick which uses the ultrasonic sensors for detection of obstacles,
NodeMCU that controls the system, mobile phone GPS sensor for sensing the live location,
buzzer for alert, cloud for storing the buzzer data and location. Mobile phone GPS sensor
is used because of the cost factor.
18.4.1 Ultrasonic Sensor
Ultrasonic sensors [7] are regularly utilized as a part of robotization assignments to gauge
remove, position changes, level estimation, for example, nearness finders or in exceptional
applications, for instance, when estimating the immaculateness of straightforward material. They depend on the standard of estimating the spread time of ultrasonic waves. This
guideline guarantees dependable discovery is free of the shading rendering of the question
or to the plan and the sort of its surface. It is conceivable to dependably distinguish even
such materials as fluids, mass materials, straightforward articles, glass and so on. Another
contention for their utilization is them utilizing as a part of forceful situations, not exceptionally extraordinary affect-ability to earth and furthermore the likelihood of estimating
a separation. Ultrasonic sensors are made in numerous mechanical outlines. For research
facility utilize, the basic lodging utilized for transmitter and beneficiary independently or
in a solitary lodging, for modern utilize are frequently built vigorous metal lodging. A few
sorts enable you to alter the affect ability utilizing a potentiometer or carefully. Additionally, the yield might be in the bound together form or the simple flag straightforwardly in
advanced shape. On account of sensors that can be associated by means of the correspondence interface to the PC, it is conceivable to set itemized parameters of all the sensor’s
working extent and estimated separations.
Designing a Beautiful Life for Indian Blind Peoples 249
Figure 18.1 Ultrasonic Sensor Working
18.4.2 NodeMCU
Advancement sheets, for example, Arduino and Raspberry Pi, are regular decisions while
prototyping new IoT gadgets. Those advancement sheets are basically smaller than normal
PCs that can interface with and be modified by a standard PC or Mac. After it has been
modified, the improvement sheets would then be able to associate with and control sensors
in the field. Since the ”I” in IoT remains for web, the advancement sheets require an
approach to interface with the web. In the field, the most ideal approach to associate
with the web is by utilizing remote systems. Engineers should include a Wi-Fi or cell
module to the board and compose code to get to the remote module. The NodeMCU
(Node Micro-controller Unit) is an open source programming and equipment advancement
condition that is worked around an exceptionally cheap System-on-a-Chip (SoC) called
the ESP8266. The ESP8266, planned and fabricated by Espressif Systems, contains every
vital component of the cutting-edge PC: CPU, RAM, organizing (Wi-Fi), and even an
advanced working framework and SDK. At the point when obtained at mass, the ESP8266
chip costs just $2 USD a piece. That settles on it an amazing decision for IoT undertakings
of numerous types.
Figure 18.2 NodeMCU Board [4]
18.4.3 Global positioning system (GPS)
An ordinary GPS recipient tunes in to a specific recurrence for radio signs. Satellites send
time coded messages at this recurrence. Each satellite has a nuclear clock, and sends the
current correct time also. The GPS collector makes sense of which satellites it can hear,
and afterward begins assembling those messages. The messages incorporate time, current
satellite positions, and a couple of different bits of data. The message stream is moderate
- this is to spare power, and furthermore in light of the fact that every one of the satellites
250 Emerging Technologies for Health and Medicine
Figure 18.3 NodeMCU pin diagram [5]
transmit on a similar recurrence and they’re simpler to choose in the event that they go
moderate. Along these lines, and the measure of data expected to work well, it can take
30-60 seconds to get an area on a general GPS. When it knows the position and time code
of no less than 3 satellites, a GPS beneficiary can accept it’s on the world’s surface and get
a decent perusing. 4 satellites are required in the event that you aren’t on the ground and
you need elevation also.
18.4.4 Buzzer
A buzzer is a contraption which makes a murmuring or beeping tumult. There are a couple
of sorts; the most central is a piezoelectric buzzer, which is just a level piece of piezoelectric
material with two anodes. This kind of buzzer requires a type of oscillator (or something
more jumbled like a micro-controller) to drive itif you apply a DC voltage you will just get
a tick. They are used as a piece of spots where you require something that transmits a fit for
being heard tone, however couldn’t think less about high-steadiness sound multiplication,
like microwave grills, smoke alarms, and electronic toys. They are decrepit and can be
rambunctious without using particularly control. They are moreover thin, so they can be
used as a piece of level things like ”singing” welcome cards [6].
Figure 18.4 Simple Buzzer
Designing a Beautiful Life for Indian Blind Peoples 251
18.4.5 Flow Diagram
Actors in the system: Care taker of blind people, Blind people who will use the stick.
Device is composed of an ultrasonic sensor and buzzer. Mobile is taken as an external
peripheral because if one attaches the mobile on the stick it will be an overhead. Mobile
GPS sensor is used because of the project cost problem. One can also use GPS module
rather than mobile sensor. When ever an obstacle is caught by the ultrasonic sensor the
buzzer will automatically cause a beep sound. And this will cause an event trigger in the
mobile application. And application will send current location and buzzer count to the
cloud. Cloud will store the data for the future use. Another use of the mobile application
is it provides the route direction to the user by using Google Map API. When ever buzzer
counter is above 5 that means there is a permanent obstacle and mobile application will
show another path to reach the destination. Care taker can track the location of blind
people. Power Bank is used for the power supply which easily rechargeable.
Figure 18.5 Basic Flow Diagram
18.5 Implementation
In this section one will get to know about how to interface ultrasonic sensors and buzzer
with NodeMCU. For that 1 NodeMCU, 3 Ultrasonic sensors and one buzzer are needed.
Other things which are implemented are using programming not by hardware devices.
Power bank is used for power supply purpose. One 9V battery is used to supply power to
the Ultrasonic sensors. One IC is used to convert 9V to 5V and that IC is 7805.
Figure 18.6 7805 IC
Let’s begin with the implementation steps with good description: Firstly, one need to
get a 9V battery to supply power to the 3 ultrasonic sensors. But as we know one ultrasonic
sensor works on the 5V power supply. Now to solve this problem we need to take one IC
named as 7805 IC. This IC is used to convert power supply to a constant 5V. Now our one
problem is solved. Now our ultrasonic sensors will work properly using 5V power supply
to each.
252 Emerging Technologies for Health and Medicine
Now take one breadboard. Which are a solder-less board. Used to make temporary
circuits and prototypes. And the magic is we don’t have to do any soldering. And as we
are using breadboard this will not make a permanent device but for learning purpose one
can use breadboard because it is easy and effortless.
Figure 18.7 Breadboard
Now take one NodeMCU. And connect it to your laptop or desktop using USB cable.
We are using NodeMCU here because it has inbuilt Wi-Fi and Bluetooth facility. And
another reason is if in future if we want to send data directly from the device only then it
is. Other some benefits we get by using NodeMCU are:
Low cost (No Budget Problem).
Integrated support for Wi-Fi (No external device needed).
Small Size (It can fit anywhere).
Work on low power supply .
We have to associate 9V battery to the breadboard. To connect 9V battery to the breadboard we need SNAP connector. Look something like this
Figure 18.8 SNAP Connector
We need now three kinds of connecting wires named as Female to Female wire, Male
to Male wire, Male to Female wire.
Figure 18.9 Female to Female wire, Male to Male wire, Male to Female wire
Designing a Beautiful Life for Indian Blind Peoples 253
Figure 18.10 All Components
Now we almost have all the required devices to do actual implementation.
Let’s look at the circuit diagram so one will get more idea.
Figure 18.11 Circuit Diagram
Note: Circuit diagram is showing interfacing of three ultrasonic sensors but as per convenience and to make it easier here we will implement only one ultrasonic sensor.
Battery and conversion from the 9V to 5V is done by performing a simple breadboard
connection so we will not talk it in more detail.
As mentioned above that 7805 will convert 9V to 5V now we have powers supply of
5V. Now take 5V power supply from the output of 7805 and give it as a VCC in HC-SR04
(This is the device number of ultrasonic sensor which is shown in the circuit diagram).
Then take ground from the battery and give that to the ground of HC-SR04.
To trigger HC-SR04. Connect Trigger pin to the D0 pin of NodeMCU.
254 Emerging Technologies for Health and Medicine
Figure 18.12 Connection of 7805 IC and Battery with Bread board
Trigger pin is responsible for sending the signal in form of high frequency pulse of
10μs. It will send 40KHz ultrasound in 8 cycles and make its echo line high at that time
period.
Connect echo pin to the D1 of NodeMCU.
When signal finds any object, the transmitted signal will be reflected and echo pin will
take the input as a reflected signal.
Echo pin takes the input in form of pulse timing.
Object found by the trigger pin and reflected signal came to echo pin now what?
We have to measure the distance between the object and device. For that we use one
equation: PULSE_TIME*0.034/2.
Output of this equation will be in centimeters.
Figure 18.13 NodeMCU connections
Designing a Beautiful Life for Indian Blind Peoples 255
If one want to implement three ultrasonic he/she can do it by following same procedure
of connecting trigger and echo pins as shown in above circuit diagram.
Sample code to measure the distance:
1 void cal_distance(int tri, int echo)
2 {
3 digitalWrite(tri, LOW);
4 delayMicroseconds(2);
5 digitalWrite(tri, HIGH);
6 delayMicroseconds(10);
7 digitalWrite(trigger, LOW);
8
9 time_taken = pulseIn(echo, HIGH);
10 dist= time_taken*0.034/2;
11 if (dist>300)
12 dist=300;
13 }
Now we got the distance. Now we have to find whether the object is at 3, 2, or 1 meter
far. That is again programming stuff.
First, let us talk about Piezo buzzer. Because based on the output we have to trigger
Piezo buzzer. So that it can play a tone when object is certain nearer.
Integration of buzzer with NodeMCU is very easy. We have to connect negative pin of
buzzer to the ground and positive pin of buzzer to the D8 pin of the NodeMCU.
Just for information piezo buzzer is used in cars/trucks as a reverse indicator so that
driver can assume some obstacle is there.
Piezo buffer works at DC power supply. Buzzer is encapsulated by a coating of plastic
in a rounded fashion. At the top of the buzzer there is one hole to propagate sound from it.
If we talk inside structure then there is metallic disc that is used for producing buzz
sound through top hole.
Figure 18.14 Buzzer Implementation
Let’s see the sample code by which buzzer will buzz.
1 void loop()
2 {
3 calculate_distance(trigger,echo);
4
5 similar_count=0;
256 Emerging Technologies for Health and Medicine
6
7 //if (dist<70)
8 {
9 Serial.println("Object detected at :");
10 Serial.print(dist);
11 //dist<70 means IF distance is less than 70 cm at that time buzzer will start
12
13 if (distanceLenght<70)
14 {
15 digitalWrite(Buzz,HIGH);
16 digitalWrite(LED, HIGH);
17 for (int increment=distanceLenght; increment>0; increment--)
18 delay(10);
19
20 digitalWrite(Buzz,LOW);
21 digitalWrite(LED, LOW);
22 for (int i=distanceLenght; i>0; i--)
23 delay(10);
24 }
25 }
26 }
By doing this, when ultrasonic finds any object it will make D8 pin high. Buzzer will
produce sound. Such that blind people will get notified when an obstacle is there.
Now main part comes mobile GPS sensor interfacing. For that we have to make one
mobile application which implements the Google Map API.
Google map API provides different functionalities like [12]:
Google Maps Direction.
Google Maps Distance Matrix.
Google Maps Elevation.
Google Maps Geocoding.
Google Maps Roads.
Google Maps Time Zone.
Google Places.
By using this API, one can easily find the Location. It also provides functionality of
Direction.
When buzzer rings the Mobile Application will store the location of blind people and
the Buzzer count at that location to the cloud.
Here cloud can be any one for an example Google Fire base, IBM Cloud etc.
By programming it is set when ever buzzer count at same location is more then 10. It
assumes that obstacle is permanent. And it will direct blind people to another path.
18.6 Advantages and Disadvantages
Advantages
Cost Saver as we are using mobile device as a one of the sensor. And mostly all are
having mobile devices with them.
Designing a Beautiful Life for Indian Blind Peoples 257
Cloud is used so care taker can watch blind people live location whenever and anywhere he/she wants.
Blind people will be directed to the destination with voice assistance provided by
Google API.
As we are using Ultrasonic sensors which are also called as a basic sensor so implantation is also easy.
The adjustment of Ultrasonic is in such a way that it can detect up to 45 degrees for
the obstacle.
As all the free and open sources are used there is no much cost of handling of the
system. Because all the security and privacy functions are provided at the cloud level
only.
If there is any permanent obstacle then the system it self choose another path to reach
the destination.
As power bank is used to supply power to NodeMCU it is rechargeable. It may run
1-2 days without recharging based on the mAh that power bank provides.
Disadvantages
As mobile device is used for cost saving purpose but it is an extra overhead to the
system.
System is still in developing state so testing is not applied.
It cannot detect running obstacles.
It cannot still detect holes.
No speech recognition facility is available.
Care taker can not call or send message to the blind people in emergency.
System is depended on the external systems.
18.7 Conclusion
Mostly all are trying to make human life better by using IoT. And we can’t forget physically
disabled peoples. They are also a living being and they also need some facility to live there
life easily. By thinking and noticing some live example like Blind people are crossing road
and because they can’t see car is coming and they loss their life because they can’t see.
Some times because they can’t see dog is sleeping and they walk over dog. Dog bites them
and they injured. Because they can’t see holes and open gutters they fall into that. By
seeing such problems, we got inspired to make a smart stick by which there all problems
get solved. For that we tried to use our little knowledge of IoT to provide solution.
Let’s talk about the future expansion of the system. We already have discussed what is
the system how it works, how to implement it and what are disadvantages of the system.
Now the future plan is to solve that all disadvantages. And add some new features like
Voice Assistance through Artificial Intelligence.
258 Emerging Technologies for Health and Medicine
REFERENCES
1. http://assistech.iitd.ernet.in/smartcane.php
2. Dambhre, S. (2011). Smart stick for blind: Obstacle Detection. In Artificial vision and Realtime assistance via GPS, journal for 2nd national conference on information and communication Technology (NCICT).
3. Frenkel, R. S. (2008). Coded Pulse Transmission and Correlation for Robust Ultrasound Ranging from a Long-Cane Platform. Masters Theses, 104.
4. https://www.ibm.com/developerworks/library/iot-nodemcu-open-why-use/Picture1.png
5. https://www.ibm.com/developerworks/library/iot-nodemcu-open-why-use/Picture2.png
6. https://www.robomart.com/image/cache/catalog/RM0338/piezo-buzzer-b-10n-piezo-electricbuzzers-rm0338-by-robomart-399-500x500.jpg
7. Koval, L., Vanus, J., & Bilik, P. (2016). Distance Measuring by Ultrasonic Sensor. IFACPapersOnLine, 49(25), 153-158.
8. https://health.nokia.com/us/en/scales
9. https://jawbone.com/up
10. https://nest.com/thermostats/nest-learning-thermostat/overview/
11. https://www.makerbot.com/learn/
12. https://developers.google.com/maps/web-services/overview
13. Bhatt, C., Dey, N., & Ashour, A. S. (Eds.). (2017). Internet of things and big data technologies
for next generation healthcare.
14. Bhatt, Y., & Bhatt, C. (2017). Internet of things in healthcare. In Internet of things and big data
technologies for next generation HealthCare (pp. 13-33). Springer, Cham.
259
Dac-Nhuong Le et al. (eds.), Emerging Technologies for Health and Medicine, (259–284) 
© 2018 Scrivener Publishing LLC
CHAPTER 19
SMART HOME: PERSONAL ASSISTANT
AND BABY MONITORING SYSTEM
Shivam Kolhe, Sonia Nagpal, Priya Makwana, Chintan Bhatt
Charotar Institute Of Science And Technology, Changa, Gujarat, India
Emails: 15ce054@charusat.edu.in, sonia.sn13@yahoo.com, 15ce058@charusat.edu.in,
chintanbhatt.ce@charusat.ac.in
Abstract
In this era of internet and technology, we want every device to be connected with each
other. Meaning of Internet of Things is that each and every device should talk to each other.
The proposed system exemplifies a new class of home automation and Baby Monitoring
platforms that provide intuitive, cloud-based speech interfaces. This system is a combination of 3 systems; Smart Home Personal Assistant, Online Energy Meter and Advanced
Baby Monitoring System. Main feature is that all these three systems talk to each other. In
this chapter a general introduction about Internet of Things is given and description about
these three systems is provided, detailed information about sensors used in this system is
given. Technologies used in this system like Raspberry pi 3, Arduino, sensors, Firebase
real time database cloud platform, data analytics, Android, speech recognition (STT, TTS),
Image and Video Processing are also made familiar in this chapter.
Keywords: Baby Monitor, Internet of Things (IoT), Speech To Text (STT), Text to
Speech (TTS), Data Analytics, Online Energy Meter, Smart Home Personal Assistant.
260 Emerging Technologies for Health and Medicine
19.1 Introduction
The smart voice assistant has every information about the whole module. The given module
listens to you, translating your voice into commands so it can work as an entertainment
suite, Baby training guide as well as a caretaker, automates your home, or orders stuff
online from the module. The device uses speech recognition to perform an ever-growing
range of tasks on given command. The device connects to the voice-controlled intelligent
personal assistant module service which responds to the name ”ABC”.
The device is capable of voice interaction, music playback, turns off home appliances
which are not in use, playing audiobooks, login, and logout from popular shopping sites
and also social media sites on your voice commands, suggests you the bestseller items,
news, providing weather information, real-time energy meter value, current status of baby,
and much more you can’t imagine. It can also control various smart devices by itself as a
smart home automation hub. Smart Personal Assistant device that is present in the user’s
home is triggered using voice commands. Based on the request made, a response will be
returned to the user. This module works as an intelligent and digital personal assistant. It
analyses electric current used by the appliances and learns the usage pattern. It alerts the
user about the appliances which are using more/less power. And it also helps user minimize
the electricity bill. The device uses speech recognition to perform an ever-growing range
of tasks on given command. It can also recognize the ABC name: when you say the word
”ABC”, it recognizes the module calls or the wake word and starts recording your voice.
When you have finished speaking, the speech is converted to text and then according to
given command, it will perform tasks. This module converts the recording into commands
that it interprets. It is more than a simple voice-to-text service. It is a fully programmable
service that can work with various online services to do a surprising range of things. This
module utilizes speech recognition and language processing to facilitate interactions with
devices. Although voice-enabled interfaces are still in nascent, they have been used to
interact with televisions, air-conditioners, speakers, smartphones and other electronic devices. We focus on how to build a cost-effective module that can be widely used. We
use this module to develop an application that will communicate with our raspberry pi to
control our devices. After smartphones, the voice-controlled module is the next big step
for voice assistants. This product or module built-in voice-activated assistants arrived on
the scene in the last few years, but the difference between the existing devices and our
device is that our device ignores the unnecessary words or not related to the task (active
noise cancellation concept), it will cancel out the normal communications and focus on to
the specific given command. It consumes very less time to implement our task after the
wakeup word ”ABC”. With the aim of making people’s lives easier They allow us to access
the perfect tune with ease and change our music selection without leaving our place. Some
kind of devices also come equipped with cameras that can be operated remotely, while
others allow you to order goods online using your voice. The user can trigger a skill by
saying keywords like open, ask, buy, get, launch, tell, play or start, on, off followed by
the name of the module. The range of activities that can be carried out by this kind of
module. According to given data, it analyses and performs the task. For this data, analytics
is used. It has voice control over the home. This module consists of a voice recognition
module that the user can interact with. This module has also smart environmental control in the smart automation system. This Internet of Things application module is created
with a combination of many technologies like Raspberry pi 3, sensors, Firebase real-time
database cloud platform, data analytics, Android, speech recognition (STT, TTS), Image
and Video Processing. It will work as a caretaker for baby as:
Smart Home 261
1. Moisture sensing, cry detection, movement detection
2. Alerts the parents whenever the module detects the baby cry, moisture and if any
changes in vital parameters of health.
3. Works as a health monitor. (Pulse rate, respiration rate, and temperature monitoring)
4. If a baby is crying then it sings a lullaby.
5. Face recognition system (Only limited authorized persons).
This complete module is a combination of fields like health care, energy conservation,
and measurement, environment control, home automation, Education etc.
19.2 Background
There is an abrupt increase in the number of devices and mainly IoT devices. Nowadays we
have IoT devices for teens, adults and old people. But there is a need for such healthcare
modules for infants too. Adults or mature people can express whatever they are feeling,
whatever pain they are suffering through but infants can’t express their pain or their feelings. There is a different way of treating adults and babies. If a baby is having a fever or
any respiratory problem, then he can express it by just crying but parents can’t understand
sometimes what is happening to their baby. So, understanding their vital parameters like
pulse rate, respiration, body temperature and also other parameters like body movement,
moisture detection, cry detection. In this module, pulse rate sensor (Infrared transmitter
and receiver) detects the pulse rate of the baby. A temperature sensor (LM35 or IR Temperature sensor) detects the temperature of the body. Thus, the module is designed in such a
way that each and every information about the baby is available to the parents. The parents
are made aware of the condition and present status of the baby. The author has used not
only SMS service but also Email alerts and Push Notifications from the android app that is
developed for this module. SMS service is provided using GSM module connected with a
micro-controller. For all of this many research papers are studied as well as articles about
healthcare are also studied.
The author has achieved success in connecting this baby monitoring module with the
smart home personal assistant module. Parents can know the details about their baby by
asking the smart assistant about the status of their baby. And if the baby is in any problem
than sensor notify the user, then the smart assistant will immediately alert the parents by
speaking it aloud. So, parents can quickly reach to their babies. The technology used here
is speech recognition and Text To Speech (TTS). If in any emergency, the smart assistant
will alert the trusted or family doctor about the condition of the baby so that the doctor can
quickly reach there. The doctor will get each and every detail about vital parameters like
heart/pulse rate, respiration, and temperature of the baby. So, while on the way a doctor
can look at those readings and when the doctor reaches the baby, the doctor is aware of the
problem baby is suffering through. Thus, the doctor can take proper steps for treating the
baby.
19.3 Proposed Design and Implementation
This chapter will cover the following three regions:
262 Emerging Technologies for Health and Medicine
1. Smart Home Personal Assistant
2. Baby Monitoring System
3. Energy Measurement and Energy Conservation
19.3.1 Smart Home Personal Assistant
Features of Smart Home Personal Assistant:
Figure 19.1 Overview of system
1. Home Automation: Different modules of home are controlled by android app and
data will be stored on firebase cloud. The medium between the appliance and the
application will be wireless and it will use the internet.
Smart Auto Home Control (Turns Off Appliances not in use).
Voice Control Over Home Appliances.
Remotely Control Appliances using Android app and Voice Control.
2. Weather Guide:
Smart Home 263
It will show the current location weather data, and weather data of any region,
Humidity, and UV Index.
The module acts as a personal caretaker. It alerts the user about UV index and
tells the user to take precautions according to UV range.
3. Music Player:
The module plays music in the user’s playlist.
The music player can be controlled using voice commands.
The Main feature of the module is that although the music is playing, the module
will ignore the music and will listen to your voice only.
4. Send an Email:
The module will send an email to anyone using just voice commands.
Similarly other features of Smart home personal assistant are:
1. The module will provide information about the current condition of the baby to the
parents.
2. You can surf the web (Google, YouTube) using voice commands.
3. You can find any location on Google maps using voice commands.
4. The module will provide the user update about the date, time and day.
5. You can change the name of an assistant. You can call it with whatever name you
want.
6. Main Feature of this module is Active Noise Cancellation.
7. This module is also connected with Online Energy Meter module. So, whenever any
appliance crosses the threshold then the smart personal assistant alerts the user using
TTS service and
8. The system will also keep the user updated about the power consumption and cost
of the appliances. This module will also alert the module if any appliance is running
uselessly.
The effective feature of this module is Noise-Cancellation. This intelligent module
will not only take care of our babies but will also take care of us. It is a kind of module
that is fully voice-enabled that gets queries from the user and it gives output accordingly.
We have various components in this module and it has one effective feature that is noise
cancellation which works when one or two voices merged it and identify automatically
which is a proper command for given module and it is fully customized module according
to given commands it serves properly, we named our module ”ABC”. Whenever we want to
turn on or turn off our appliances, the module will automatically turn the appliance on or off
accordingly. The core of this project is Raspberry Pi 3. Home automation here is achieved
by connecting the relay with Arduino which is connected with Raspberry Pi. Appliances
are connected with both relay and ACS712 current Sensor. The user can control appliances
in the home using either android app, web portal or using voice commands.
264 Emerging Technologies for Health and Medicine
Figure 19.2 Connection of Arduino with ACS712 sensor and Relay
Any of the appliances can be controlled. Here in using this app user can control lights,
fans, water motor, Air Conditioners, Televisions and Smart Plug. But this module can be
programmed to control any appliance and hardware connections are to be changed accordingly. The serial data of Arduino is fetched by Raspberry Pi. For programming purpose,
python and Arduino language are used here. For speech recognition, python library used is
Speech Recognition and for face recognition library used is OpenCV. Some of the libraries
used in this module are shown in Table 19.1 below:
Table 19.1 Few libraries used in this system
Feature Library For Python
Speech Recognition Speech Recognition
Face Recognition OpenCV
Text To Speech (TTS) gtts
Email Notifications Smtplib
Music Playback Pygame, piglet
Serial Communication with Arduino Pyserial
Cloud Firebase
Android App for Everything:
Home Automation
Energy conservation Online Energy Meter
Baby Monitoring
Smart Home 265
Figure 19.3 Raspberry Pi and connected different modules
19.3.2 Baby Monitoring System
Nowadays, the workload is increasing and people are getting very busy. Here the main
problem arises when they are parents and they have to take care of their child or infant as
well as their work. Mother is always worried about her child. So, during office hours she
can’t focus on her work. Thus, this module is developed in such a way that the parents
can get each and every vital information as well as current status of their baby. Parents are
notified using either SMS, Email alerts or push up notifications from the application that is
developed with this module.
Figure 19.4 Baby Monitoring System and connected sub modules
266 Emerging Technologies for Health and Medicine
Here the main feature is that the database used here is Firebase real-time database by
Google. This is one of the fastest and free databases available today. The status of the
baby is updated in the app at the same time the sensor senses the condition. So, without
delay, the parents can get up to date information about their baby. The data includes pulse
rate data, movement data, temperature data, diaper moisture data. The Baby Monitoring
System also consists of Baby Training or Teaching Module which will teach baby as per
his/her age group. This module also sings a lullaby. In baby monitoring system we can get
a notification or current status of the baby. In this we have a various parameter in which
it will show the values, all data collected from Arduino and from Arduino to python then
python to firebase cloud and finally from cloud to the application. The application gets
data from firebase cloud. It does not allow any unauthorized persons in baby’s room with
the help of face recognition system. Parents will be able to monitor their babies (live feed,
moisture detection, sleep detection, movement tracking). Assistant will act as a caretaker
as well as a teacher. So, parents will be tension free.
The graphical presentation of temperature and pulse rate is shown below. Whenever the
reading crosses a certain limit, an alert is sent to parents and the doctor immediately.
Figure 19.5 Graph of Body Temperature of a baby
Figure 19.6 Graph of Pulse-Rate of a baby
Smart Home 267
Face Recognition: A face recognition is a system which is used to identify/detect or
verify a person from a digital image or a video frame from a source. There are various
methods by which face recognition systems work, generally, it works by comparing applied
facial features from given image with faces within an original database. It recognizes and
detects faces with the help of Python or by the command line.
Use of face recognition in this system:
For security purpose.
Tracks movement of baby’s body,
Facial movement tracking,
Parents tracking,
Security for newborn babies in the hospital.
Intruder and owner detection,
Notify the user about who is at on the door,
It will provide protection from criminals and it will alert the user about this situation.
In facial recognition system, the module will find the nodal points on the obtained sample face like facial area, chin, eyes, jawline, the width of nose, cheekbones, length of the
face, the width of the face, the area covered by mouth. The first step is the face detection it
will look at the person and find a face in it. The second step is the data collecting, in this,
it will extract unique characteristics of the person. After that data comparison, despite of
variations in light or expression, it will compare those unique features to all the features of
the database. In face recognition, it will determine that, that is the authenticated person.
In Face Recognition System, the module will recognize an analog image and extract it.
The module will find nodal points on the obtained sample face like eyes, etc. Then it will
compare it with the database of images that is already present in the system. In this module,
OpenCV module is used and Python used for face detection, face identification, and face
recognition. Python module is also used to read systems database, training directories, and
file names. This system converts Python lists to numpy arrays as OpenCV face recognizer
needs them for the face recognition and identification.
Figure 19.7 Steps of face detection and Recognition
268 Emerging Technologies for Health and Medicine
This block diagram shows the flow of the face recognition process, in this module scans
and captures an analog or a digital image of a person. The second interface of a module
collects the data and extracts the data of the captured image. Another block of the module
compares the image with the database. If that image is matched with the database then it
will allow the person to enter the room otherwise it will notify the user. In baby monitoring
system it will allow an only specific person to come, it is specifically used for baby’s
security, it will check whether it is an authentic person or not if that image is getting
matched with a database then it will allow the person otherwise it will notify the user about
incorrect identification.
19.4 Online Energy Meter
Energy Measurement and Conservation Module:
1. The user will know the power consumption of each appliance.
2. If the appliance crosses the threshold then it alerts the user.
3. If the appliances are running uselessly or if no one is in the room then the module
will. automatically detect that and will turn off appliances running in that room.
4. The user will be able to control the appliances using the android app as well as by just
issuing voice commands.
Figure 19.8 Energy Measurement and Conservation Module
Smart Home 269
The Energy measurement module measures the power consumption of each appliance
and displays it to the user. The Energy conservation module tries to minimize the electricity
bill of the user by various methods. ACS712 current sensor is also known as Hall Effect
sensor by which AC current of any appliance that is present in our home can be measured,
in home automation we can get data from this sensor and the data will be stored in the
cloud service. It gives analog voltage out proportional to the amount of current which
flows through the circuit and it basically works on the principles of magnetism or the
relationship between magnetism and electricity. In this module cloud used is Firebase.
Firebase provides real-time database functionality. It is one of the fastest and the best
cloud platform which provides faster results and faster updating.
The flow diagram represents the connection of appliances with ACS712, PIR motion
sensor, relay module, Arduino and raspberry pi. The first current measured by the ACS712
current sensor and after that, it will goes into Arduino and then in relay module and the
electrical appliance is already connected with the relay board. This module gets the threephase RMS voltage and current data from the database. PIR motion sensor is also connected by the module to check human presence. This module is named as energy meter.
It shows how much power is consumed by any appliances. The module is able to notify
the user about power consumption before power usage exceeds a certain limit. It will also
notify the user about the power usage and cost for each and every appliance in the house.
This module is fully voice-enabled which executes commands for switching on/off the
appliances.
In energy conservation we can measure the power consumption by any appliances and
total cost according to the used power consumed by electrical appliances and the energy
meter will show energy conservation parameters and alert about the energy consumed by
electrical appliances from that we can save our power and lessen the cost or electricity
bill. From this module, the user will be able to monitor the usage of each appliance in his
home. This module is designed in such a way that it will try to minimize the electricity
bill and will alert the module about electricity usage, It shows Energy Efficiency Ratio
(EER) of any appliance or any circuit which is present in our home. The app will display
used amount of electricity and the units per hour and will notify the user about energy
conservation in which it will minimize the electricity bill.
19.5 Sensors used and Their Working
The detailed information about all the components used in this module is shown below:
19.5.1 Temperature Sensor
Two types of temperature sensors can be used in this module:
1. Semiconductor Sensors: LM35 is a semiconductor sensor that is used in this project.
They are available in the form of IC’s. They are known as IC Temperature Sensor.
To detect the temperature, these sensors are to be kept in contact with the body of the
baby. These temperature sensors provide high accuracy over an operating range of
nearly 550C to 1500C. LM35 has 3 pins. Looking from the flat surface, the first pin
is voltage pin (+5v), the middle pin is analog output pin and the third pin is a ground
pin.
270 Emerging Technologies for Health and Medicine
Figure 19.9 LM35 Temperature Sensor
2. IR Temperature Sensor: IR Temperature Sensors are also known as non-contacting
sensors. They work by transmitting and detecting Infrared signals. If the sensor is
held towards the body, it detects the temperature of the body depending on the level
of radiation the body is emitting.
Figure 19.10 IR Temperature Sensor
19.5.2 Soil Moisture Sensor
The soil moisture sensor is used to measure the moisture content in the soil. It is ideal for
applications in soil science, environmental science, agriculture science, botany, Gardening
etc. The moisture in the soil is measured indirectly by measuring the conductivity of the
soil. The sensor has two long conductors called electrodes, separated by some distance.
Moisture content in the soil is directly proportional to the conductivity in the soil. Means
more the conductivity the more it is moist.
There are three pins on the sensor.
1. Vcc
2. Ground
3. Signal
Along with it, it also has one digital pin which provides high or low signal directly.
Signal pin gives the analog value proportional to the amount of moisture in the soil. The
two electrodes are inserted in the soil to measure the moisture content. The voltage value
Smart Home 271
is the potential drop at the electrodes. The potential drop changes with the change in
conductance of the soil. It can only detect the change in the moisture content but cannot
directly measure the standard value. For measuring the standard value, more accurate
calibration is needed.
Figure 19.11 Soil Moisture Sensor
Apart from its usual applications, the sensor can also be used in various other applications too. The module has been developed which uses soil moisture sensor to check
whether the baby has urinated or not. The sensor is in the interface with LM393 which
helps to obtain analog as well digital output of the sensor. The output of the sensor is then
given to the microcontroller which is connected to the sensor. The microcontroller used
here in an Arduino. The circuit is placed under the cloth on which the baby is sleeping.
Whenever the moisture is detected, it gives the output between 300 and 950 depending
upon the moisture content. Moreover, there are few modules which have been used to send
the data to parents so the parents can know that their baby needs a diaper change.
Table 19.2 Minimum and maximum value obtained from soil moisture sensor
Minimum Value Maximum Value
Voltage Range(V) 3.3 5.0
Current Range(MA) 0.0 35
Output Value
The sensor in dry soil 0.0 300
The sensor in humid soil 300 700
Sensor in water 700 950
The modules used here are:
1. GSM module.
2. Email module (MIME library) in python.
3. Push notification (using Firebase).
This way the parents do not need to worry about their child all the time.
272 Emerging Technologies for Health and Medicine
19.5.3 PIR (Passive Infra-Red) Sensor
It is an electronic device which helps to detect the presence of a human being or any
animal. It also senses the motion and hence can detect whether anyone has moved out or
in of the sensor range. Human beings/Animals emit heat energy in the form of infrared
radiation. The fact that human beings and animals emit infrared radiation helps this device
to detect them. PIR sensor consists of a Pyroelectric sensor which generates energy when
exposed to heat. The Pyroelectric sensor detects the level of infrared radiation. The sensor
is encapsulated with a metal plate which consists of a rectangular glass crystal on top of it.
The detection range of the sensor is between 5m and 12m. It is an inexpensive, small and
an easy to use the device.
Figure 19.12 PIR Motion Sensor
The sensor is actually divided into two halves. When any human or animal comes in the
range of a sensor, the radiation is detected by the first half, due to which a positive differential change is caused between two halves of the Pyroelectric sensor. When human/animal
leaves the area in range, the situation reverses, causing a negative differential change between two halves. This change in temperature is what is detected to detect the presence of
any warm body in the sensor range.
The sensor is encapsulated with a lens called Fresnel lens which focuses the infrared
radiation which comes from human/animal body on the Pyroelectric sensor. With the help
of this component(lens), it becomes easy to detect the radiation. There are three pins on
the PIR sensor.
1. Vcc
2. Signal
3. Ground
Generally, power is 3-5 VDC but it can go up to 12V. The output signal is high when
any warm body is detected. We can also adjust the sensitivity and delay time of the sensor.
The sensitivity of a sensor can be ranged up to 7m and delay time can be adjusted between
3s-5mins. The sensor also consists of two triggers.
19.5.3.1 Non-repeatable trigger When sensor output is high and delay-time is over, it
automatically changes the output from high to low.
Smart Home 273
19.5.3.2 Repeatable trigger This trigger keeps the output high all the time until the
warm body is present in the sensor’s range.
This sensor can be used in various applications. One of them is Smart Home Automation Module. Being a human-being it is natural to forget sometimes. It might happen
that the person in the room forgets to switch off the device before leaving. This results in
wastage of electricity and ultimately saves the extra cost on the electricity bill.
19.5.3.3 Sound Sensor Sound sensor detects the presence of any kind of sound in the
environment. It provides both digital and an analog output signal which represents its
amplitude. Its one of the main components is its microphone. The sensor detects the sound
through a microphone and converts it into the electrical signal to amplifier part. It is an
operational amplifier that increases the signal from the microphone.
Figure 19.13 Sound Sensor
There are four pins on the sound sensor.
1. Vcc
2. Ground
3. An out (Analog output)
4. D out (Digital output)
The sound sensor has been used to amplify sound, to detect sound level etc. There are
many applications in which sound sensor can be used.
A module has been developed which sends the data through GSM module or activates
the alarm to alert the parents as soon as it detects the crying sound of an ina fant. A
lullaby player is connected to the module. As soon as the infant starts crying, the sound
is converted into an electrical signal which is transmitted to ta he lullaby player by the
Arduino system. As the player gets the signal, it starts playing the preloaded songs through
the speaker to calm down the baby. The module is programmed to wait until 15 secs since
the baby started crying. If the crying doesn’t stop after 15 secs, the data is sent to the
parents through GSM module and it also activates the alarm to alert the parents.
19.5.3.4 Pulse Rate Monitor Pulse rate sensors work on the basis of differential absorption characteristics of oxygenated and deoxygenated hemoglobin. Oxygenated hemoglobin
absorbs more infrared light while deoxygenated hemoglobin absorbs more red light. This
274 Emerging Technologies for Health and Medicine
pulse rate sensor is very easy to use. Just by putting finger on the top of the sensor, it
detects a pulse by measuring the change in the light as per expansion of the capillary blood
vessels. This sensor has two portions. LED that is present in the center of the sensor. Below LED, a noise Cancellation module is present which cancels noise that can affect the
readings. This sensor has 3 pins. First one is Ground (GND) pin, the middle one is Vcc
and the last one is analog output pin (A0). The sensor consists of a bright Light Emitting
Diode (LED) and a Light Detector (LDR). The bright light is passed from one side of the
finger and the intensity of the reflected light is measured by LDR. The volume of blood
inside the blood capillary changes the amount of light reflected. During a heartbeat the
heart pumps the blood resulting in the absorption of light and thus there is a decrease in the
intensity of the light received by LDR. This increases the resistance value of LDR. This
resistance variation is converted into a variation of voltage using a circuit called OP-AMP.
Before passing the signal to microcontroller the signal is amplified. The microcontroller
is then made to count a number of an interrupt or count the pulse every minute. Thus, the
value of pulse per minute will give the heart rate in bpm i.e. Beats Per Minute. This sensor
can be attached to the wrist of the baby. And the microcontroller is programmed in such
a way that if the pulse rate value exceeds the limit according to the age of the baby then
quickly alert is sent to the parents as well as to the doctor.
The pulse rate sensor has 3 pins:
1. Ground
2. Vcc
3. Analog (A0)
Figure 19.14 Pulse Rate Sensor
19.5.3.5 ADXL335 (Accelerometer) An Accelerometer is capable of measuring acceleration in all the three orthogonal axes which are X, Y, and Z respectively. Acceleration
is the measurement of change in speed or velocity with respect to time. ADXL335 is
one type of accelerometer sensor. The sensor works by sensing the static acceleration of
gravity. The measuring range of the sensor is 3 g in all the three directions. It gives output in analog representation. The output is basically in the form of a voltage which is in
proportion to acceleration.
Smart Home 275
An Accelerometer can measure the following entities when it is set in different modes:
1. Velocity and Position
2. Orientation and Inclination
3. Vibration (Shock)
This sensor has 5 pins on it out of which three are output pins for X, Y, and Z axis as
mentioned in the Figure 19.15. All these pins give an analog output which ranges from 0
to 1023.
Figure 19.15 Accelerometer ADXL335 Module
Have you ever wondered how the compass app in the smartphone works? Or how an
app for finding the constellation in the sky works? Smartphones have accelerometer IC
(Integrated Circuit) installed in them and hence these apps work with the help of an IC.
The accelerometer can also be used to detect the earthquake.
Acceleration is the function of displacement represented as a = f(x) where a is the acceleration and x is the displacement, so acceleration can be measured by measuring the displacement. The methods which are used to sense the displacement are Resistive technique,
Capacitive technique, and Inductive technique. As these techniques are all mechanical in
nature and are not eligible to be used in smartphones, developers came up with a single
structure called MEMS (Micro Electro Mechanical System). This is inside accelerometer
IC.
Figure 19.16 Accelerometer sensor MEM mechanism
There are two structures inside the IC, the outer assembly having fixed plates and the
internal movable assembly. It has a small mass and is connected to the outer assembly
276 Emerging Technologies for Health and Medicine
using spring contacts. The movable assembly also has plates which form a capacitor.
As the module moves due to acceleration, the internal assembly moves which cause the
change in displacement and ultimately change in the value of the capacitor. By measuring
the change in capacitance, the value of acceleration acting on the body is calculated. An
accelerometer gives the reading of acceleration in a different direction as seen by the body.
In the above section, the common applications of the sensor are mentioned. To extract
the most out of it, one can implement this sensor in various other applications. One such
module has been developed for working parents to monitor their baby. ADXL335 is used in
this module to monitor the orientation and movement of the baby. The sensor is connected
to a microcontroller which is Arduino to send that data. The data then is sent to that parents
to let them know whether their child is awake or sleeping. If no movement is recorded
within the given time, the module sends the message to the parents which say ”Your child
is sleeping peacefully”. The module is programmed in such a way that it waits for 10sec
before sending the data to parents through GSM module. Because it might happen that
the baby is just changing gesture in sleep. This way, this module is helpful in giving live
updates to parents about their child.
19.5.3.6 Respiration Monitor System Respiration monitoring module analyses a person’s respiration patterns on the basis of breathing rate. The module uses Piezoelectric
Film sensor to track the breathing rate. The word Piezo is a Greek word which means
”Press” or ”Squeeze”. It is highly sensitive and gives analog output. The sensor measures
displacement variation induced by inhaling or exhaling. The sensor is placed in a wearable elastic belt, the length of which can be adjusted. This is usually placed slightly above
the belly so that the breathing rate can be tracked effectively. This sensor also monitors
respiratory rate, respiratory cycle regularity, the relative amplitude of the cycle, and others.
The Piezoelectric film forms a circuit with a voltmeter which measures the voltage
produced by the Piezoelectric film sensor.
Figure 19.17 Sensor with neutral position
Figure 19.18 The sensor in a flexed position
When the sensor is in the neutral position or at rest, there’s nothing special observed.
The voltage produced by the sensor will be equal to zero. But when you bend or flex the
Smart Home 277
sensor, that mechanical work translates into a charge displacement. Non-zero voltage is
observed. Positive charge accumulates on one side and negative on the other. The structure
becomes like a charged capacitor. Like any other charged capacitor, the charge ultimately
gets combined after some time and becomes normal again. This change in voltage is
recorded and is given as output.
Given below is a table of the respiratory rate for almost all age group which helps to
analyze whether a person’s respiratory rate is normal or not.
RESPIRATORY RATE: The module has already been developed to monitor the baby
which uses the respiratory monitoring system to monitor the breathing activity of a child.
The baby is made to wear this soft elastic belt. The belt is placed slightly above the belly.
The sensor is connected to an Arduino micro-controller board which collects the data given
by the sensor and send it to the parents when an unusual activity is recorded. The normal
respiratory rate has been specified in the above given table. If the breathing rate increases
or decreases with respect to the normal respiratory rate, then an alert message is sent to the
parents through the GSM module. The message will also be sent to the doctor if the details
are added to the database so that the condition of the baby can be more precisely analyzed.
This way baby’s health can be monitored using a Piezoelectric film sensor.
Table 19.3 Respiration rate as per the age group
Age group Respiratory rate
Adults 12-20 breathes/minute
Infants (¡12 months) 30-60 breathes/minute
Toddler (1-3 years) 24-40 breathes/minute
Pre-school (4-5 years) 22-34 breathes/minute
School age (6-12 years) 18-30 breathes/minute
Adolescence (13-16 years) 12-16 breathes/minute
19.5.3.7 ACS712 Current Sensor ACS712 is made by a company called Allegro, it is
an American company. In this ACS712 we have 3 pins:
1. Power supply (Vcc)
2. Output
3. Ground
And we have a couple of clusters in this sensor. We have little LED and resistor in it.
The ACS712 is called Hall Effect sensor because the person who measuring current this
way was Edwin Hall. This is ”ACS712 30A” because it is capable of measuring up to
30Amperes of current. This is an analog device. It is to be read in an analog pin in the
Arduino, we power this up with 5 volts, another one connects with the ground and output
pin goes to an analog input pin on the Arduino. It follows low noise and analog signal
path is really a good thing about the sensor. We have 66 to 185 millivolts per ampere
output sensitivity range. We get this range of sensitivity just because there is a different
type of submodules are there, and the submodules are like 5 amperes, 20 amperes and
30-ampere version are there. So, for this module the sensitivity we taking in a count is 66
mV per ampere, if we want to pull 1 Ampere of current through this chip then the output
of the chip to the Arduino would be 66mV. If we pull 2 Amperes through this chip then
278 Emerging Technologies for Health and Medicine
Figure 19.19 ACS712 Current Sensor
the output would be about 132 or 130 mV. He discovered this in the late 1800s when an
amount of current passes through somewhere it gives of a degree of magnetism, it is linear
with the amount of current that passes through a circuit or conductor or any appliance.
This chip basically uses that effect in order to measure the current. When current passes
through these two plates here underneath the current goes through the two pins and then
it comes back out through another two pins and then back out this plate and through the
screw terminal adapter but another little chip measure the hall effect, when it is measured
the hall effect there is some bits of processing and then it gets sent out there at where three
pins are situated.
From the Output voltage vs sampled current graph, we can conclude its just a margin
of error there in the graph. It seems to suggest that it is mainly based on the temperature,
there as long as you keep it at 25 degrees Celsius or under then we get a pretty good rate of
accuracy. So, how do we measure the AC current using ACS712 current sensing sensor?
Basically, in-home we got 200 or 250 volts and 50 Hertz, 50 Hertz means 50 waves per
second. Duration of that wave which shown in the figure is 20 milliseconds (1000/50
Hertz = 20). There is wave every 20ms (50 waves per second i.e. 50 Hertz). In DC we
took samples everywhere and if we take some sample ones a second because in DC 12
volts are a consistent amount of amperage, voltage won’t change in the amperage, it would
be fairly accurate.
Figure 19.20 Graph 1
But in AC if we have 50 of these waves in one second, now we have to find where
will our sample fall at the point shown in Figure 21. We will take some sample ones per
second. In our case, if this sensor gives 66mV per ampere then how many millivolts will
return, at 0V point, we get 0V because at that point the 0 ampere is flowing through it. If
we were to take samples we could fall on that point and it would be a complete waste of
Smart Home 279
Figure 19.21 Graph 2
time. We can only get current when there is a voltage, if there is no voltage then there is no
current. The more voltage there is potentially more current that we would be able to show.
So, that really matters exactly where we take a sample. Basically, what we can conclude
from this is that unlike DC where everything is stable, linear and does not actually move.
We could take some sample ones a second but in AC we have got absolutely no chance.
In AC, it will totally have no meaning. So, with AC current we start with taking the 50
waves per second. There is a wave every 20ms. It would not even be sufficient to take one
sample every 20ms, it is very short time but for electronic appliance specifically to be able
to measure wave which is 20ms long.
Figure 19.22 Graph 3
It is just not On/Off circuit. We were to take 10 samples every 20ms or every 2ms. Then
we would get 5 samples of positive part and 5 samples of negative part of the voltage. After
taking 10 samples we miss the peak and the trough then we are not going to get an accurate
answer. Even taking 10 samples over one is not enough. Now we take 20 samples over one
wave. From this, we get 10 samples per positive half and 10 samples per negative half. It
can be sufficient. We have got 1000 samples which we have taken in a second. We get 0mV
which is 0 ampere, after that, we get negative 660mV which shows -10 amperes value. If
we were to get all those samples and find the mean, we would add them all together and
divide by the total amount of sample. We would get 0mV from this calculation. ACS712
current sensor is also known as Hall Effect sensor by which we can measure AC current of
any appliance that is present in our home, in home automation we can get data from this
sensor and the data will store in the cloud service. It gives analog voltage out proportional
to the amount of current which flows through the circuit and it basically works on the
principles of magnetism or the relationship between magnetism and electricity.
280 Emerging Technologies for Health and Medicine
Figure 19.23 Flow of process for Online Energy Meter module
This block diagram shows connectivity of ACS712 Current sensor with Microcontroller
having Wi-Fi capability. Input from AC source is given and it goes to the load through
theACS712 Hall effect sensor. Input current goes through the load and it gets sensed by
the sensor and sensor is connected with an analog pin (A0) of Arduino. Power supply gives
power to the microcontroller. The total power and the energy rate is displayed on a LED
display. This chip is connected with the database or web application. It will show various
parameters of Energy meter.
Real Power (P): It is also known as Watt full power. The actual power which is used to
produce electric medium in the circuit. The power which is directly transferred to the load.
For DC real power is
P = I ∗ V (19.1)
For AC real power is
I ∗ V ∗ cos(θ). (19.2)
Reactive Power (Q): The power which oscillates between load and source. It continuously returns back between load and source. For inductor it is positive, for a capacitor it is
negative.
Q = I ∗ V ∗ sin(θ) (19.3)
Apparent Power (S): In circuit when there is no phase angle difference then will be
counted. It is a product of I and V.
S = I ∗ V. (19.4)
Smart Home 281
Figure 19.24 Real Power
Figure 19.25 Reactive Power
S = ReactiveP ower + RealP ower (19.5)
kvA = (kW + kvAR) (19.6)
Figure 19.26 Apparent Power
Power Factor (Pf ): It is a ratio of true power to the apparent power.
This triangle is known as a power triangle.
To get various parameters there are some formulas. Take the value of Vpp, after that
divide Vpp by 2 to get Vp.
Vrms = Vp ∗ 0.707 (19.7)
282 Emerging Technologies for Health and Medicine
Figure 19.27 Power Factor
Vpp
2 = Vp (19.8)
Vpp = Peak to peak voltage (19.9)
To find Vrms, take Vp and multiply it by 0.707. After that multiply sensitivity of
ACS712 with Vrms, to get Irms.
Vrms ∗ sensitivity = Irms (19.10)
For final answer real power is used because it is the main or actual power which is
taking an account.
F inalanswer = Vrms ∗ Irms ∗ Pf (19.11)
Vrms = 230(i.e.standardvalue) (19.12)
Irms = Reading of sensor at Pf = 0.85 (19.13)
There are two types of different methods for AC measurement.
1. RMS: Root Mean Square: It is a calculation that we do with all of these given values
including negative. We do its calculation after performance calculation on them and
it gives an amount of average voltage as a positive number, not as a negative number. The peak voltage is about 300 and the trough is -300 but the RMS voltage is
approximately 250. By RMS method we can get value or output of ACS712.
2. Multiply with 0.707: This method is a lot easier but not as accurate. We get 1000
samples in a second in order to detect the wave, now we are going to eliminate all the
negative ones straightaway, any value lower than 0 will automatically eliminate. Then
we have to go through all of these numbers and we have to find the peak. We eliminate
all the positive numbers too. The only number we are after is the peak. We get the
peak which is responsible in this case and the peak value is 660mV. We multiply peak
value by 0.707. This value is equivalent to RMS value.
Smart Home 283
Figure 19.28 Root Mean Square
19.6 Conclusion
Voice Controlled Home Automation is a very different concept than what is presently available in the market. This would make automation more easy and intuitive. The people will
be able to interact with the module. It also is an important aspect in the present world where
people are so busy, this would help them in easing the basic functionality of their life. The
world around us is going digital in every aspect we can imagine and it is happening fast,
we also need to move forward with it. This module is a great initial step in automation,
it would also provide with security. As it is based on voice recognition we can assign a
particular password to each user and the automation will respond to the correct Passwords
only. The following are the features of this module:
Easy to use.
Saves unnecessary power consumption.
Low cost compared to other automation systems.
Easy to implement.
Could also be used to provide security measures.
Has good processing power and can handle multiple functions at the same time.
Uses reliable wireless connection.
Provides security and personal customization.
The module also has integrated with different modules which are home automation,
user control smart home appliances with their voice. Multiple modules can be placed in
different rooms in a home for synchronized playback of music. This module will provide
a various number of services, both in-house and third-party, are integrated, allowing users
to listen to music, control playback of videos or photos, or receive news updates, provide
information about power usage by electrical appliances according to that information it
will automatically control the usage of appliances.
The output of another module is that it will monitor the activities like crying, sleeping,
playing etc. of a baby within the house and shows the pulse rate of the baby. The module
will provide a training-suite for baby. It also restricts the entry of unauthorized persons in
284 Emerging Technologies for Health and Medicine
baby’s room with the help of face recognition module. The main thing is that everything
will be connected with each other and every module will talk to each other. So, this module
is a bundle of everything you need.